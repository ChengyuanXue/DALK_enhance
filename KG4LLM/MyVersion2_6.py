from langchain.chat_models import ChatOpenAI
from langchain import PromptTemplate,LLMChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage,
)
import numpy as np
import re
import string
from neo4j import GraphDatabase, basic_auth
import pandas as pd
from collections import deque, Counter, defaultdict
import itertools
from typing import Dict, List, Tuple, Optional
import pickle
import json
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize 
import openai
from langchain.llms import OpenAI
import os
from time import sleep
import logging
from functools import wraps
from datetime import datetime
import gc
import requests

from dataset_utils import *
from tqdm import tqdm
from sentence_transformers import SentenceTransformer


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ========================= ç»Ÿä¸€é˜ˆå€¼é…ç½®ç®¡ç† =========================


class ThresholdConfig:
    """ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼çš„é…ç½®ç±»"""
    
    def __init__(self, config_name='default'):
        self.config_name = config_name
        self._load_config(config_name)
    
    def _load_config(self, config_name):
        """åŠ è½½æŒ‡å®šé…ç½®"""
        configs = {
            'default': {
                # å®ä½“åŒ¹é…ç›¸å…³é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'entity_matching': {
                    'basic_similarity': 0.6,           # åŸºç¡€å®ä½“åŒ¹é…é˜ˆå€¼
                    'enhanced_similarity': 0.6,        # å¢å¼ºå®ä½“åŒ¹é…é˜ˆå€¼
                    'confidence_threshold': 0.85,      # å®ä½“ç½®ä¿¡åº¦é˜ˆå€¼
                    'min_similarity': 0.6,             # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
                    'negation_factor': 0.8,            # å¦å®šé—®é¢˜çš„é˜ˆå€¼è°ƒæ•´å› å­
                },
                
                # è¯­ä¹‰åŒ¹é…ç›¸å…³é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'semantic_matching': {
                    'jaccard_similarity': 0.7,         # SemanticMatcherçš„Jaccardç›¸ä¼¼åº¦é˜ˆå€¼
                    'vector_similarity': 0.7,          # å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼
                    'keyword_matching': 0.3,           # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                },
                
                # é—®é¢˜åˆ†ç±»ç›¸å…³é˜ˆå€¼
                'question_classification': {
                    'type_similarity': 0.4,            # é—®é¢˜ç±»å‹åˆ†ç±»é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'secondary_threshold': 0.85,       # ç¬¬äºŒç›¸ä¼¼ç±»å‹çš„é˜ˆå€¼å› å­
                },
                
                # åŒ»å­¦æ¦‚å¿µåˆ†ç±»é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'medical_concept': {
                    'disease': 0.65,                   # ç–¾ç—…æ¦‚å¿µé˜ˆå€¼
                    'symptom': 0.60,                   # ç—‡çŠ¶æ¦‚å¿µé˜ˆå€¼
                    'treatment': 0.58,                 # æ²»ç–—æ¦‚å¿µé˜ˆå€¼
                    'general': 0.55,                   # é€šç”¨åŒ»å­¦æ¦‚å¿µé˜ˆå€¼
                },
                
                # å±‚æ¬¡åŒ–å›¾è°±ç›¸å…³é˜ˆå€¼
                'hierarchical_kg': {
                    'semantic_matching': 0.7,          # å±‚æ¬¡åŒ–å›¾è°±è¯­ä¹‰åŒ¹é…é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'concept_center': 0.65,            # æ¦‚å¿µä¸­å¿ƒç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'hierarchy_weight': 0.75,          # å±‚æ¬¡å…³ç³»æƒé‡é˜ˆå€¼
                },
                
                # çŸ¥è¯†è´¨é‡è¯„ä¼°é˜ˆå€¼
                'knowledge_quality': {
                    'quality_threshold': 0.7,          # çŸ¥è¯†è´¨é‡é˜ˆå€¼
                    'relation_importance': 0.5,        # å…³ç³»é‡è¦æ€§é˜ˆå€¼
                    'path_confidence': 0.1,            # è·¯å¾„ç½®ä¿¡åº¦é˜ˆå€¼
                },
                
                # å¤šè·³æ¨ç†ç›¸å…³é˜ˆå€¼
                'multi_hop': {
                    'path_quality': 0.5,               # è·¯å¾„è´¨é‡é˜ˆå€¼
                    'reasoning_confidence': 0.6,       # æ¨ç†ç½®ä¿¡åº¦é˜ˆå€¼
                    'evidence_weight': 0.4,            # è¯æ®æƒé‡é˜ˆå€¼
                }
            },
            
            'my_settings': {
                # å®ä½“åŒ¹é…ç›¸å…³é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'entity_matching': {
                    'basic_similarity': 0.8,           # åŸºç¡€å®ä½“åŒ¹é…é˜ˆå€¼
                    'enhanced_similarity': 0.8,        # å¢å¼ºå®ä½“åŒ¹é…é˜ˆå€¼
                    'confidence_threshold': 0.9,      # å®ä½“ç½®ä¿¡åº¦é˜ˆå€¼
                    'min_similarity': 0.7,             # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
                    'negation_factor': 0.85,            # å¦å®šé—®é¢˜çš„é˜ˆå€¼è°ƒæ•´å› å­
                },
                
                # è¯­ä¹‰åŒ¹é…ç›¸å…³é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'semantic_matching': {
                    'jaccard_similarity': 0.8,         # SemanticMatcherçš„Jaccardç›¸ä¼¼åº¦é˜ˆå€¼
                    'vector_similarity': 0.8,          # å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼
                    'keyword_matching': 0.4,           # å…³é”®è¯åŒ¹é…é˜ˆå€¼
                },
                
                # é—®é¢˜åˆ†ç±»ç›¸å…³é˜ˆå€¼
                'question_classification': {
                    'type_similarity': 0.4,            # é—®é¢˜ç±»å‹åˆ†ç±»é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'secondary_threshold': 0.85,       # ç¬¬äºŒç›¸ä¼¼ç±»å‹çš„é˜ˆå€¼å› å­
                },
                
                # åŒ»å­¦æ¦‚å¿µåˆ†ç±»é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                'medical_concept': {
                    'disease': 0.8,                   # ç–¾ç—…æ¦‚å¿µé˜ˆå€¼
                    'symptom': 0.8,                   # ç—‡çŠ¶æ¦‚å¿µé˜ˆå€¼
                    'treatment': 0.8,                 # æ²»ç–—æ¦‚å¿µé˜ˆå€¼
                    'general': 0.8,                   # é€šç”¨åŒ»å­¦æ¦‚å¿µé˜ˆå€¼
                },
                
                # å±‚æ¬¡åŒ–å›¾è°±ç›¸å…³é˜ˆå€¼
                'hierarchical_kg': {
                    'semantic_matching': 0.8,          # å±‚æ¬¡åŒ–å›¾è°±è¯­ä¹‰åŒ¹é…é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'concept_center': 0.8,            # æ¦‚å¿µä¸­å¿ƒç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦
                    'hierarchy_weight': 0.75,          # å±‚æ¬¡å…³ç³»æƒé‡é˜ˆå€¼
                },
                
                # çŸ¥è¯†è´¨é‡è¯„ä¼°é˜ˆå€¼
                'knowledge_quality': {
                    'quality_threshold': 0.7,          # çŸ¥è¯†è´¨é‡é˜ˆå€¼
                    'relation_importance': 0.5,        # å…³ç³»é‡è¦æ€§é˜ˆå€¼
                    'path_confidence': 0.1,            # è·¯å¾„ç½®ä¿¡åº¦é˜ˆå€¼
                },
                
                # å¤šè·³æ¨ç†ç›¸å…³é˜ˆå€¼
                'multi_hop': {
                    'path_quality': 0.5,               # è·¯å¾„è´¨é‡é˜ˆå€¼
                    'reasoning_confidence': 0.6,       # æ¨ç†ç½®ä¿¡åº¦é˜ˆå€¼
                    'evidence_weight': 0.4,            # è¯æ®æƒé‡é˜ˆå€¼
                }
            },
            
            'strict': {
                # ä¸¥æ ¼æ¨¡å¼ï¼šæ›´é«˜çš„é˜ˆå€¼
                'entity_matching': {
                    'basic_similarity': 0.75,
                    'enhanced_similarity': 0.75,
                    'confidence_threshold': 0.9,
                    'min_similarity': 0.7,
                    'negation_factor': 0.85,
                },
                'semantic_matching': {
                    'jaccard_similarity': 0.8,
                    'vector_similarity': 0.8,
                    'keyword_matching': 0.4,
                },
                'question_classification': {
                    'type_similarity': 0.5,
                    'secondary_threshold': 0.9,
                },
                'medical_concept': {
                    'disease': 0.75,
                    'symptom': 0.7,
                    'treatment': 0.68,
                    'general': 0.65,
                },
                'hierarchical_kg': {
                    'semantic_matching': 0.8,
                    'concept_center': 0.75,
                    'hierarchy_weight': 0.8,
                },
                'knowledge_quality': {
                    'quality_threshold': 0.8,
                    'relation_importance': 0.6,
                    'path_confidence': 0.2,
                },
                'multi_hop': {
                    'path_quality': 0.6,
                    'reasoning_confidence': 0.7,
                    'evidence_weight': 0.5,
                }
            }
        }

        if config_name not in configs:
            logger.warning(f"Unknown threshold config '{config_name}', using 'default'")
            config_name = 'default'
        
        # åŠ è½½é…ç½®
        config = configs[config_name]
        for category, thresholds in config.items():
            setattr(self, category, thresholds)
        
        logger.info(f"Loaded threshold configuration: {config_name}")
    
    def get_threshold(self, category, key):
        """è·å–æŒ‡å®šç±»åˆ«å’Œé”®çš„é˜ˆå€¼"""
        try:
            category_config = getattr(self, category)
            return category_config.get(key, 0.5)  # é»˜è®¤è¿”å›0.5
        except AttributeError:
            logger.warning(f"Unknown threshold category: {category}")
            return 0.5
    
    def set_threshold(self, category, key, value):
        """åŠ¨æ€è®¾ç½®é˜ˆå€¼"""
        try:
            category_config = getattr(self, category)
            category_config[key] = value
            logger.info(f"Updated threshold {category}.{key} = {value}")
        except AttributeError:
            logger.warning(f"Cannot set threshold for unknown category: {category}")
    
    def get_concept_threshold(self, concept_type):
        """æ ¹æ®æ¦‚å¿µç±»å‹è·å–å¯¹åº”é˜ˆå€¼"""
        concept_type = concept_type.lower()
        if concept_type in self.medical_concept:
            return self.medical_concept[concept_type]
        else:
            return self.medical_concept['general']
    
    def adjust_for_negation(self, base_threshold):
        """ä¸ºå¦å®šé—®é¢˜è°ƒæ•´é˜ˆå€¼"""
        return base_threshold * self.entity_matching['negation_factor']
    
    def print_config(self):
        """æ‰“å°å½“å‰é…ç½®"""
        print(f"\n=== Threshold Configuration: {self.config_name} ===")
        for category in ['entity_matching', 'semantic_matching', 'question_classification', 
                        'medical_concept', 'hierarchical_kg', 'knowledge_quality', 'multi_hop']:
            if hasattr(self, category):
                print(f"\n{category}:")
                config = getattr(self, category)
                for key, value in config.items():
                    print(f"  {key}: {value}")

    
threshold_mode = os.getenv('THRESHOLD_MODE', 'default')
THRESHOLDS = ThresholdConfig(threshold_mode)


# ========================= æ¶ˆèå®éªŒé…ç½® =========================
# ğŸ”¬ æ¶ˆèå®éªŒå¼€å…³é…ç½®
ABLATION_CONFIGS = {
    'baseline': {
        'USE_HIERARCHICAL_KG': False,
        'USE_MULTI_STRATEGY_LINKING': False,
        'USE_ADAPTIVE_UMLS': False,
        'USE_UMLS_NORMALIZATION': False,
        'USE_REASONING_RULES': False,
        'USE_KG_GUIDED_REASONING': False,
        'USE_OPTIMIZED_MULTIHOP': False,
        'USE_ENHANCED_ANSWER_GEN': False
    },
    'full_model': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_hierarchical_kg': {
        'USE_HIERARCHICAL_KG': False,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_multi_strategy': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': False,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_adaptive_umls': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': False,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_umls_normalization': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': False,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_reasoning_rules': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': False,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_kg_guided': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': False,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_multihop': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': False,
        'USE_ENHANCED_ANSWER_GEN': True
    },
    'ablation_enhanced_answer': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': True,
        'USE_KG_GUIDED_REASONING': True,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': False
    },
    'myself_settings': {
        'USE_HIERARCHICAL_KG': True,
        'USE_MULTI_STRATEGY_LINKING': True,
        'USE_ADAPTIVE_UMLS': True,
        'USE_UMLS_NORMALIZATION': True,
        'USE_REASONING_RULES': False,
        'USE_KG_GUIDED_REASONING': False,
        'USE_OPTIMIZED_MULTIHOP': True,
        'USE_ENHANCED_ANSWER_GEN': True
    }
}

# å½“å‰å®éªŒé…ç½® (å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡ä¿®æ”¹)
CURRENT_ABLATION_CONFIG = os.getenv('ABLATION_CONFIG', 'ablation_kg_guided')


def get_ablation_config():
    """è·å–å½“å‰æ¶ˆèå®éªŒé…ç½®"""
    config = ABLATION_CONFIGS.get(CURRENT_ABLATION_CONFIG, ABLATION_CONFIGS['full_model'])
    logger.info(f"ğŸ”¬ Using ablation configuration: {CURRENT_ABLATION_CONFIG}")
    logger.info(f"ğŸ“‹ Configuration details: {config}")
    return config

# è·å–å½“å‰é…ç½®
ABLATION_CONFIG = get_ablation_config()

# ========================= æ€§èƒ½ä¼˜åŒ–é…ç½® =========================
CLEANUP_FREQUENCY = 15
MAX_CACHE_SIZE = 1500
KEEP_CACHE_SIZE = 800
MAX_FAILED_CUIS = 1000

# Enhanced API retry configuration
MAX_RETRIES = 60
RETRY_WAIT_TIME = 60
ENTITY_CONFIDENCE_THRESHOLD = 0.85
KNOWLEDGE_QUALITY_THRESHOLD = 0.7
MIN_SIMILARITY_THRESHOLD = 0.6

# ========================= è¯­ä¹‰é—®é¢˜åˆ†ç±»å™¨ =========================

class SemanticQuestionTypeClassifier:
    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2', similarity_threshold=0.4):
        """
        åˆå§‹åŒ–è¯­ä¹‰é—®é¢˜ç±»å‹åˆ†ç±»å™¨
        
        Args:
            model_name: ä½¿ç”¨çš„sentence transformeræ¨¡å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        try:
            self.model = SentenceTransformer(model_name)
            logger.info(f"âœ… Loaded semantic model: {model_name}")
        except Exception as e:
            logger.error(f"âŒ Failed to load semantic model: {e}")
            raise
            
        # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„é˜ˆå€¼
        self.similarity_threshold = THRESHOLDS.get_threshold('question_classification', 'type_similarity')
        
        # å®šä¹‰æ¯ä¸ªé—®é¢˜ç±»å‹çš„å…¸å‹ä¾‹å¥
        self.type_examples = {
            'definition': [
                "What is Alzheimer's disease?",
                "Define dementia",
                "What does cognitive impairment mean?",
                "Alzheimer's disease is characterized by",
                "The meaning of neurodegeneration"
            ],
            'causation': [
                "What causes Alzheimer's disease?",
                "Why does dementia occur?",
                "What leads to memory loss?",
                "The etiology of cognitive decline",
                "Alzheimer's disease is caused by",
                "Due to what factors does dementia develop?"
            ],
            'treatment': [
                "How to treat Alzheimer's disease?",
                "What medication for dementia?",
                "Treatment options for cognitive decline",
                "Therapy for memory loss",
                "Drugs used to treat Alzheimer's",
                "Management of dementia patients"
            ],
            'symptom': [
                "What are symptoms of Alzheimer's?",
                "Signs of dementia",
                "Clinical manifestations of cognitive decline",
                "How does Alzheimer's present?",
                "Symptoms include memory loss",
                "Patient presents with confusion"
            ],
            'diagnosis': [
                "How to diagnose Alzheimer's disease?",
                "What tests for dementia?",
                "Diagnostic criteria for cognitive impairment",
                "How to identify Alzheimer's?",
                "Assessment of memory problems",
                "Evaluation of cognitive function"
            ],
            'prevention': [
                "How to prevent Alzheimer's disease?",
                "Prevention of dementia",
                "How to avoid cognitive decline?",
                "Reducing risk of memory loss",
                "Protective factors against Alzheimer's",
                "Ways to prevent neurodegeneration"
            ],
            'exception': [
                "All of the following EXCEPT",
                "Which is NOT associated with",
                "All are true EXCEPT",
                "The following are false EXCEPT",
                "Not a characteristic of",
                "All predisposing factors EXCEPT"
            ]
        }
        
        # é¢„è®¡ç®—æ‰€æœ‰ä¾‹å¥çš„åµŒå…¥å‘é‡
        self._precompute_embeddings()
    
    def _precompute_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰ç±»å‹ä¾‹å¥çš„åµŒå…¥å‘é‡"""
        self.type_embeddings = {}
        
        for q_type, examples in self.type_examples.items():
            # è®¡ç®—æ‰€æœ‰ä¾‹å¥çš„åµŒå…¥å‘é‡
            embeddings = self.model.encode(examples, show_progress_bar=False)  # ç¦ç”¨ï¼Œé¿å…7ä¸ªé‡å¤è¿›åº¦æ¡
            # å–å¹³å‡ä½œä¸ºè¯¥ç±»å‹çš„ä»£è¡¨å‘é‡
            self.type_embeddings[q_type] = np.mean(embeddings, axis=0)
            
        logger.info(f"âœ… Precomputed embeddings for {len(self.type_embeddings)} question types")
    
    def identify_question_type(self, question):
        """
        ä½¿ç”¨è¯­ä¹‰åŒ¹é…è¯†åˆ«é—®é¢˜ç±»å‹
        
        Args:
            question: è¾“å…¥é—®é¢˜æ–‡æœ¬
            
        Returns:
            list: è¯†åˆ«çš„é—®é¢˜ç±»å‹åˆ—è¡¨
        """
        try:
            # è®¡ç®—é—®é¢˜çš„åµŒå…¥å‘é‡
            question_embedding = self.model.encode([question], show_progress_bar=False)[0]  # ç¦ç”¨è¿›åº¦æ¡
            
            # è®¡ç®—ä¸å„ä¸ªç±»å‹çš„ç›¸ä¼¼åº¦
            similarities = {}
            for q_type, type_embedding in self.type_embeddings.items():
                similarity = cosine_similarity(
                    question_embedding.reshape(1, -1),
                    type_embedding.reshape(1, -1)
                )[0][0]
                similarities[q_type] = similarity
            
            # ç‰¹æ®Šå¤„ç†å¦å®š/ä¾‹å¤–é—®é¢˜
            if self._is_exception_question(question):
                similarities['exception'] = max(similarities.get('exception', 0), 0.8)
            
            # é€‰æ‹©ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ç±»å‹
            matched_types = []
            for q_type, similarity in similarities.items():
                if similarity >= self.similarity_threshold:
                    matched_types.append((q_type, similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            matched_types.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›æœ€ç›¸ä¼¼çš„ç±»å‹(ä»¬)
            if matched_types:
                result_types = [matched_types[0][0]]  # è‡³å°‘è¿”å›æœ€ç›¸ä¼¼çš„
                
                # å¦‚æœç¬¬äºŒç›¸ä¼¼çš„ç±»å‹åˆ†æ•°æ¥è¿‘ï¼Œä¹ŸåŒ…å«è¿›æ¥
                if (len(matched_types) > 1 and 
                    matched_types[1][1] >= matched_types[0][1] * 0.85):
                    result_types.append(matched_types[1][0])
                
                logger.debug(f"Question: '{question[:50]}...' -> Types: {result_types}")
                return result_types
            else:
                logger.debug(f"Question: '{question[:50]}...' -> Type: ['general'] (no match)")
                return ['general']
                
        except Exception as e:
            logger.error(f"Error in semantic question type identification: {e}")
            return ['general']
    
    def _is_exception_question(self, question):
        """æ£€æµ‹æ˜¯å¦ä¸ºä¾‹å¤–/å¦å®šç±»å‹é—®é¢˜"""
        exception_indicators = [
            'except', 'not', 'false', 'incorrect', 'exclude', 
            'excluding', 'other than', 'rather than', 'not true',
            'not associated', 'not characteristic'
        ]
        
        question_lower = question.lower()
        return any(indicator in question_lower for indicator in exception_indicators)
    
    def get_similarity_scores(self, question):
        """è·å–é—®é¢˜ä¸æ‰€æœ‰ç±»å‹çš„ç›¸ä¼¼åº¦åˆ†æ•°(ç”¨äºè°ƒè¯•)"""
        question_embedding = self.model.encode([question], show_progress_bar=False)[0]  # ç¦ç”¨è¿›åº¦æ¡
        
        similarities = {}
        for q_type, type_embedding in self.type_embeddings.items():
            similarity = cosine_similarity(
                question_embedding.reshape(1, -1),
                type_embedding.reshape(1, -1)
            )[0][0]
            similarities[q_type] = round(similarity, 3)
            
        return similarities

    def batch_identify_question_types(self, questions):
        """æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜"""
        if not questions:
            return []
        
        # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰é—®é¢˜ â†’ åªæœ‰1ä¸ªè¿›åº¦æ¡ï¼ˆæˆ–å…³é—­è¿›åº¦æ¡ï¼‰
        question_embeddings = self.model.encode(questions, show_progress_bar=False)
        
        results = []
        for i, question in enumerate(questions):
            question_embedding = question_embeddings[i]
            result = self._classify_with_embedding(question_embedding, question)
            results.append(result)
        
        return results
    
    def _classify_with_embedding(self, question_embedding, question):
        """ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥å‘é‡è¿›è¡Œåˆ†ç±»"""
        similarities = {}
        for q_type, type_embedding in self.type_embeddings.items():
            similarity = cosine_similarity(
                question_embedding.reshape(1, -1),
                type_embedding.reshape(1, -1)
            )[0][0]
            similarities[q_type] = similarity
        
        # ç‰¹æ®Šå¤„ç†å¦å®š/ä¾‹å¤–é—®é¢˜
        if self._is_exception_question(question):
            similarities['exception'] = max(similarities.get('exception', 0), 0.8)
        
        # é€‰æ‹©ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ç±»å‹
        matched_types = []
        for q_type, similarity in similarities.items():
            if similarity >= self.similarity_threshold:
                matched_types.append((q_type, similarity))
        
        matched_types.sort(key=lambda x: x[1], reverse=True)
        
        if matched_types:
            result_types = [matched_types[0][0]]
            if (len(matched_types) > 1 and 
                matched_types[1][1] >= matched_types[0][1] * 0.85):
                result_types.append(matched_types[1][0])
            return result_types
        else:
            return ['general']

# ========================= å±‚æ¬¡åŒ–å›¾è°±æ„å»º_é¢„è®¡ç®—å‘é‡ç®¡ç†å™¨ =========================
class PrecomputedVectorManager:
    def __init__(self, entity_embeddings, keyword_embeddings):
        """åˆ©ç”¨ç°æœ‰çš„é¢„è®¡ç®—å‘é‡"""
        self.entity_embeddings = entity_embeddings
        self.keyword_embeddings = keyword_embeddings
        
        # å»ºç«‹å¿«é€ŸæŸ¥æ‰¾ç´¢å¼•
        self.entity_to_idx = {entity: idx for idx, entity in enumerate(entity_embeddings['entities'])}
        self.keyword_to_idx = {keyword: idx for idx, keyword in enumerate(keyword_embeddings['keywords'])}
        
        # é¢„è®¡ç®—åŒ»å­¦æ¦‚å¿µä¸­å¿ƒå‘é‡
        self._compute_medical_concept_centers()
    
    def _compute_medical_concept_centers(self):
        """è®¡ç®—å„åŒ»å­¦æ¦‚å¿µç±»å‹çš„ä¸­å¿ƒå‘é‡"""
        concept_keywords = {
            'disease': ['disease', 'syndrome', 'disorder', 'condition', 'is_a', 'subtype'],
            'symptom': ['symptom', 'sign', 'manifestation', 'presents', 'shows'],
            'treatment': ['treat', 'therapy', 'medication', 'drug', 'cure']
        }
        
        self.concept_centers = {}
        
        for concept, keywords in concept_keywords.items():
            vectors = []
            for keyword in keywords:
                if keyword in self.keyword_to_idx:
                    idx = self.keyword_to_idx[keyword]
                    vectors.append(self.keyword_embeddings['embeddings'][idx])
            
            if vectors:
                # è®¡ç®—ä¸­å¿ƒå‘é‡ï¼ˆå¹³å‡å€¼ï¼‰
                center_vector = np.mean(vectors, axis=0)
                self.concept_centers[concept] = center_vector
                
        logger.info(f"Computed concept centers for {len(self.concept_centers)} medical concepts")
    
    def get_entity_vector(self, entity):
        """è·å–å®ä½“å‘é‡"""
        if entity in self.entity_to_idx:
            idx = self.entity_to_idx[entity]
            return self.entity_embeddings['embeddings'][idx]
        return None
    
    def get_keyword_vector(self, keyword):
        """è·å–å…³é”®è¯å‘é‡"""
        if keyword in self.keyword_to_idx:
            idx = self.keyword_to_idx[keyword]
            return self.keyword_embeddings['embeddings'][idx]
        return None
    
    def batch_entity_similarity(self, entities, concept_type):
        """æ‰¹é‡è®¡ç®—å®ä½“ä¸æ¦‚å¿µä¸­å¿ƒçš„ç›¸ä¼¼åº¦"""
        if concept_type not in self.concept_centers:
            return {}
        
        concept_center = self.concept_centers[concept_type]
        similarities = {}
        
        entity_vectors = []
        valid_entities = []
        
        for entity in entities:
            vector = self.get_entity_vector(entity)
            if vector is not None:
                entity_vectors.append(vector)
                valid_entities.append(entity)
        
        if entity_vectors:
            # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆçŸ©é˜µè¿ç®—ï¼‰
            entity_matrix = np.array(entity_vectors)
            concept_center = concept_center.reshape(1, -1)
            
            sims = cosine_similarity(entity_matrix, concept_center).flatten()
            
            for entity, sim in zip(valid_entities, sims):
                similarities[entity] = sim
        
        return similarities

# ========================= å±‚æ¬¡åŒ–å›¾è°±æ„å»º_é‡å†™æ ¸å¿ƒè¯­ä¹‰åŒ¹é…é€»è¾‘ =========================

class OptimizedSemanticMatcher:
    def __init__(self, vector_manager):
        self.vector_manager = vector_manager
        # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„é˜ˆå€¼
        self.thresholds = {
            'disease': THRESHOLDS.get_threshold('medical_concept', 'disease'),
            'symptom': THRESHOLDS.get_threshold('medical_concept', 'symptom'),
            'treatment': THRESHOLDS.get_threshold('medical_concept', 'treatment')
        }
    
    def classify_triple_batch(self, triples, concept_type):
        """æ‰¹é‡åˆ†ç±»ä¸‰å…ƒç»„æ˜¯å¦å±äºæŒ‡å®šæ¦‚å¿µç±»å‹"""
        matched_triples = []
        
        # æå–æ‰€æœ‰å®ä½“
        all_entities = set()
        for triple in triples:
            if len(triple) >= 3:
                all_entities.add(triple[0])  # head entity
                all_entities.add(triple[2])  # tail entity
        
        # æ‰¹é‡è®¡ç®—å®ä½“ç›¸ä¼¼åº¦
        entity_similarities = self.vector_manager.batch_entity_similarity(
            list(all_entities), concept_type
        )
        
        # æ‰¹é‡è®¡ç®—å…³ç³»ç›¸ä¼¼åº¦
        relations = [triple[1] for triple in triples if len(triple) >= 3]
        relation_similarities = self._batch_relation_similarity(relations, concept_type)
        
        threshold = self.thresholds[concept_type]
        
        for triple in triples:
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                # è¯­ä¹‰åŒ¹é…ä¸ºä¸»å¯¼
                head_sim = entity_similarities.get(head, 0)
                tail_sim = entity_similarities.get(tail, 0)
                relation_sim = relation_similarities.get(relation, 0)
                
                # ç»¼åˆç›¸ä¼¼åº¦è¯„åˆ†
                combined_score = max(head_sim, tail_sim) * 0.6 + relation_sim * 0.4
                
                if combined_score >= threshold:
                    matched_triples.append({
                        'triple': triple,
                        'score': combined_score,
                        'head_sim': head_sim,
                        'tail_sim': tail_sim,
                        'relation_sim': relation_sim
                    })
        
        return matched_triples
    
    def _batch_relation_similarity(self, relations, concept_type):
        """æ‰¹é‡è®¡ç®—å…³ç³»ç›¸ä¼¼åº¦"""
        if concept_type not in self.vector_manager.concept_centers:
            return {}
        
        concept_center = self.vector_manager.concept_centers[concept_type]
        similarities = {}
        
        relation_vectors = []
        valid_relations = []
        
        for relation in relations:
            vector = self.vector_manager.get_keyword_vector(relation)
            if vector is not None:
                relation_vectors.append(vector)
                valid_relations.append(relation)
        
        if relation_vectors:
            relation_matrix = np.array(relation_vectors)
            concept_center = concept_center.reshape(1, -1)
            
            sims = cosine_similarity(relation_matrix, concept_center).flatten()
            
            for relation, sim in zip(valid_relations, sims):
                similarities[relation] = sim
        
        return similarities


# ========================= ä¼˜åŒ–çš„å±‚æ¬¡åŒ–å›¾è°±æ„å»ºç±» =========================

class OptimizedHierarchicalKGFramework:
    def __init__(self, entity_embeddings, keyword_embeddings, use_semantic_matching=True):
        # ä¿æŒåŸæœ‰æ•°æ®ç»“æ„
        self.disease_hierarchy = defaultdict(list)
        self.symptom_hierarchy = defaultdict(list)
        self.treatment_hierarchy = defaultdict(list)
        
        self.hierarchy_weights = {
            'is_a': 1.0,
            'part_of': 0.9,
            'subtype_of': 0.95,
            'category_of': 0.8,
            'related_to': 0.6
        }
        
        # æ–°çš„ä¼˜åŒ–ç»„ä»¶
        self.vector_manager = PrecomputedVectorManager(entity_embeddings, keyword_embeddings)
        self.use_semantic_matching = use_semantic_matching

        # ä½¿ç”¨ç»Ÿä¸€é…ç½®çš„é˜ˆå€¼
        self.thresholds = {
            'disease': THRESHOLDS.get_threshold('medical_concept', 'disease'),
            'symptom': THRESHOLDS.get_threshold('medical_concept', 'symptom'),
            'treatment': THRESHOLDS.get_threshold('medical_concept', 'treatment')
        }
    
    def build_hierarchical_structure(self, flat_kg):
        """ä¼˜åŒ–çš„å±‚æ¬¡ç»“æ„æ„å»º - ä¸€æ¬¡éå†åŒæ—¶æ„å»ºæ‰€æœ‰å±‚æ¬¡"""
        if not ABLATION_CONFIG['USE_HIERARCHICAL_KG']:
            logger.info("Hierarchical KG Framework disabled in ablation study")
            return
            
        logger.info("Building optimized hierarchical knowledge structure with single-pass approach...")
        
        # ä¸€æ¬¡æ€§æ„å»ºæ‰€æœ‰å±‚æ¬¡
        self._build_all_hierarchies_single_pass(flat_kg)
        
        logger.info(f"Built optimized hierarchies: diseases={len(self.disease_hierarchy)}, "
                   f"symptoms={len(self.symptom_hierarchy)}, treatments={len(self.treatment_hierarchy)}")
    
    def _build_all_hierarchies_single_pass(self, flat_kg):
        """ä¸€æ¬¡éå†ï¼ŒåŒæ—¶æ„å»ºæ‰€æœ‰æ¦‚å¿µç±»å‹çš„å±‚æ¬¡"""
        
        # é¢„æå–æ‰€æœ‰å”¯ä¸€å®ä½“å’Œå…³ç³»ï¼Œé¿å…é‡å¤æå–
        all_entities = set()
        all_relations = set()
        for triple in flat_kg:
            if len(triple) >= 3:
                all_entities.update([triple[0], triple[2]])
                all_relations.add(triple[1])
        
        logger.info(f"Extracted {len(all_entities)} unique entities and {len(all_relations)} unique relations")
        
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å®ä½“ä¸å„æ¦‚å¿µä¸­å¿ƒçš„ç›¸ä¼¼åº¦
        entity_similarities = {}
        for concept_type in ['disease', 'symptom', 'treatment']:
            entity_similarities[concept_type] = self.vector_manager.batch_entity_similarity(
                list(all_entities), concept_type
            )
        
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å…³ç³»ä¸å„æ¦‚å¿µä¸­å¿ƒçš„ç›¸ä¼¼åº¦
        relation_similarities = {}
        for concept_type in ['disease', 'symptom', 'treatment']:
            relation_similarities[concept_type] = self._batch_relation_similarity(
                list(all_relations), concept_type
            )
        
        logger.info("Completed batch similarity computation for all concept types")
        
        # ä¸€æ¬¡éå†ï¼ŒåŒæ—¶è¯„ä¼°å’Œæ„å»ºæ‰€æœ‰å±‚æ¬¡
        for triple in tqdm(flat_kg, desc="Building all hierarchies simultaneously"):
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                # åŒæ—¶è®¡ç®—ä¸æ‰€æœ‰æ¦‚å¿µç±»å‹çš„ç›¸ä¼¼åº¦
                concept_scores = {}
                for concept_type in ['disease', 'symptom', 'treatment']:
                    head_sim = entity_similarities[concept_type].get(head, 0)
                    tail_sim = entity_similarities[concept_type].get(tail, 0)
                    relation_sim = relation_similarities[concept_type].get(relation, 0)
                    
                    # ç»¼åˆç›¸ä¼¼åº¦è¯„åˆ†
                    combined_score = max(head_sim, tail_sim) * 0.6 + relation_sim * 0.4
                    concept_scores[concept_type] = combined_score
                
                # æ ¹æ®é˜ˆå€¼åˆ¤æ–­å¹¶ç›´æ¥æ·»åŠ åˆ°å¯¹åº”å±‚æ¬¡
                for concept_type, score in concept_scores.items():
                    if score >= self.thresholds[concept_type]:
                        self._add_to_hierarchy(triple, concept_type, score)
        
        # å¯¹æ‰€æœ‰å±‚æ¬¡æŒ‰è¯­ä¹‰å¾—åˆ†æ’åº
        self._sort_all_hierarchies()
    
    def _batch_relation_similarity(self, relations, concept_type):
        """æ‰¹é‡è®¡ç®—å…³ç³»ç›¸ä¼¼åº¦"""
        if concept_type not in self.vector_manager.concept_centers:
            return {}
        
        concept_center = self.vector_manager.concept_centers[concept_type]
        similarities = {}
        
        relation_vectors = []
        valid_relations = []
        
        for relation in relations:
            vector = self.vector_manager.get_keyword_vector(relation)
            if vector is not None:
                relation_vectors.append(vector)
                valid_relations.append(relation)
        
        if relation_vectors:
            relation_matrix = np.array(relation_vectors)
            concept_center = concept_center.reshape(1, -1)
            
            sims = cosine_similarity(relation_matrix, concept_center).flatten()
            
            for relation, sim in zip(valid_relations, sims):
                similarities[relation] = sim
        
        return similarities
    
    def _add_to_hierarchy(self, triple, concept_type, score):
        """å°†ä¸‰å…ƒç»„æ·»åŠ åˆ°æŒ‡å®šæ¦‚å¿µç±»å‹çš„å±‚æ¬¡ä¸­"""
        head, relation, tail = triple[0], triple[1], triple[2]
        
        hierarchy_dict = getattr(self, f"{concept_type}_hierarchy")
        
        # ä¿æŒä¸åŸä»£ç ä¸€è‡´çš„æƒé‡è®¾ç½®
        if concept_type == 'disease':
            default_weight = 0.5
        else:
            default_weight = 0.7  # symptomå’Œtreatmentä½¿ç”¨0.7
        
        hierarchy_item = {
            'entity': None,
            'relation': relation,
            'weight': self.hierarchy_weights.get(relation.lower(), default_weight),
            'semantic_score': score
        }
        
        # æ ¹æ®æ¦‚å¿µç±»å‹ç¡®å®šå±‚æ¬¡ç»“æ„
        if concept_type == 'disease':
            # ç–¾ç—…å±‚æ¬¡ï¼šå­ç±» -> çˆ¶ç±»
            hierarchy_item['entity'] = head
            hierarchy_dict[tail].append(hierarchy_item)
        else:
            # ç—‡çŠ¶/æ²»ç–—å±‚æ¬¡ï¼šå®ä½“ -> ç›¸å…³é¡¹
            hierarchy_item['entity'] = tail
            hierarchy_dict[head].append(hierarchy_item)
    
    def _sort_all_hierarchies(self):
        """å¯¹æ‰€æœ‰å±‚æ¬¡æŒ‰è¯­ä¹‰å¾—åˆ†æ’åº"""
        for hierarchy_name in ['disease_hierarchy', 'symptom_hierarchy', 'treatment_hierarchy']:
            hierarchy_dict = getattr(self, hierarchy_name)
            for entity, items in hierarchy_dict.items():
                items.sort(key=lambda x: x.get('semantic_score', 0), reverse=True)
    
    def get_hierarchical_context(self, entity, context_type='all'):
        """ä¿æŒåŸæœ‰æ¥å£ä¸å˜"""
        if not ABLATION_CONFIG['USE_HIERARCHICAL_KG']:
            return {}
            
        context = {}
        
        if context_type in ['all', 'disease']:
            context['diseases'] = self.disease_hierarchy.get(entity, [])
        
        if context_type in ['all', 'symptom']:
            context['symptoms'] = self.symptom_hierarchy.get(entity, [])
        
        if context_type in ['all', 'treatment']:
            context['treatments'] = self.treatment_hierarchy.get(entity, [])
        
        return context
    
    def print_hierarchy_stats(self):
        """æ‰“å°å±‚æ¬¡å›¾è°±çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        
        total_disease_relations = sum(len(items) for items in self.disease_hierarchy.values())
        total_symptom_relations = sum(len(items) for items in self.symptom_hierarchy.values())
        total_treatment_relations = sum(len(items) for items in self.treatment_hierarchy.values())
        
        logger.info(f"ğŸ“Š Detailed Hierarchy Statistics:")
        logger.info(f"  Diseases: {len(self.disease_hierarchy)} parent nodes, {total_disease_relations} total relations")
        logger.info(f"  Symptoms: {len(self.symptom_hierarchy)} parent nodes, {total_symptom_relations} total relations") 
        logger.info(f"  Treatments: {len(self.treatment_hierarchy)} parent nodes, {total_treatment_relations} total relations")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        if self.disease_hierarchy:
            sample_disease = list(self.disease_hierarchy.keys())[0]
            logger.info(f"  Example - {sample_disease}: {len(self.disease_hierarchy[sample_disease])} sub-items")
        
        total_relations = total_disease_relations + total_symptom_relations + total_treatment_relations
        logger.info(f"  Total hierarchical relations: {total_relations}")


        
# ========================= å¤šç­–ç•¥å®ä½“é“¾æ¥ =========================

class SemanticMatcher:
    # è¯­ä¹‰åŒ¹é…å™¨ç±»ï¼šé€šè¿‡è®¡ç®—å®ä½“é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…
    
    def __init__(self):
        # æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–ç›¸ä¼¼åº¦é˜ˆå€¼
        self.similarity_threshold = THRESHOLDS.get_threshold('semantic_matching', 'jaccard_similarity')
        # ä»å…¨å±€é˜ˆå€¼é…ç½®ä¸­è·å–Jaccardç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰
        # è¿™ä¸ªé˜ˆå€¼ç”¨äºåˆ¤æ–­ä¸¤ä¸ªå®ä½“æ˜¯å¦è¶³å¤Ÿç›¸ä¼¼ä»¥å»ºç«‹é“¾æ¥
        # å…ˆç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€å€™é€‰Jaccardç›¸ä¼¼åº¦
    
    def match(self, entities, umls_kg):
        # ä¸»è¦åŒ¹é…æ–¹æ³•
        # entities: å¾…åŒ¹é…çš„å®ä½“åˆ—è¡¨ï¼ˆå¦‚ï¼š["alzheimer", "dementia"]ï¼‰
        # umls_kg: UMLSçŸ¥è¯†å›¾è°±ä¸­çš„æ ‡å‡†æ¦‚å¿µåˆ—è¡¨
        
        """è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…"""
        
        if not ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
            # æ£€æŸ¥æ¶ˆèå®éªŒé…ç½®ï¼Œå¦‚æœå¤šç­–ç•¥é“¾æ¥è¢«ç¦ç”¨åˆ™ç›´æ¥è¿”å›
            return {}
            
        matches = {}
        # åˆå§‹åŒ–åŒ¹é…ç»“æœå­—å…¸
        
        for entity in entities:
            # éå†æ¯ä¸ªå¾…åŒ¹é…çš„å®ä½“
            best_match = None
            # å­˜å‚¨å½“å‰å®ä½“çš„æœ€ä½³åŒ¹é…ç»“æœ
            best_score = 0
            # å­˜å‚¨å½“å‰å®ä½“çš„æœ€é«˜åŒ¹é…åˆ†æ•°
            
            for kg_entity in umls_kg:
                # éå†çŸ¥è¯†å›¾è°±ä¸­çš„æ¯ä¸ªæ ‡å‡†æ¦‚å¿µ
                score = self._calculate_hybrid_similarity(entity, kg_entity)
                # è®¡ç®—å½“å‰å®ä½“ä¸çŸ¥è¯†å›¾è°±æ¦‚å¿µçš„è¯­ä¹‰ç›¸ä¼¼åº¦
                
                if score > best_score and score > self.similarity_threshold:
                    # å¦‚æœå½“å‰åˆ†æ•°æ—¢è¶…è¿‡å†å²æœ€é«˜åˆ†åˆè¶…è¿‡è®¾å®šé˜ˆå€¼
                    best_score = score
                    # æ›´æ–°æœ€é«˜åˆ†æ•°
                    best_match = kg_entity
                    # æ›´æ–°æœ€ä½³åŒ¹é…æ¦‚å¿µ
            
            if best_match:
                # å¦‚æœæ‰¾åˆ°äº†æ»¡è¶³æ¡ä»¶çš„åŒ¹é…
                matches[entity] = {'match': best_match, 'score': best_score, 'method': 'semantic'}
                # å°†åŒ¹é…ç»“æœå­˜å‚¨åˆ°å­—å…¸ä¸­ï¼ŒåŒ…å«åŒ¹é…çš„æ¦‚å¿µã€åˆ†æ•°å’Œæ–¹æ³•æ ‡è¯†
        
        return matches
        # è¿”å›æ‰€æœ‰å®ä½“çš„è¯­ä¹‰åŒ¹é…ç»“æœ
    
    def _calculate_hybrid_similarity(self, entity1, entity2):
        """æ··åˆç›¸ä¼¼åº¦è®¡ç®—"""
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒå‘é‡
        vector_sim = self._calculate_vector_cosine_similarity(entity1, entity2)
        if vector_sim is not None:
            return vector_sim
        
        # å›é€€åˆ°æ”¹è¿›çš„æ–‡æœ¬ä½™å¼¦ç›¸ä¼¼åº¦
        text_cosine = self._calculate_cosine_similarity(entity1, entity2)
        jaccard = self._calculate_jaccard_similarity(entity1, entity2)
        
        # ç»„åˆä¸¤ç§æ–‡æœ¬ç›¸ä¼¼åº¦
        return 0.7 * text_cosine + 0.3 * jaccard

class ContextAwareLinker:
    # ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥å™¨ï¼šåŸºäºå®ä½“åœ¨é—®é¢˜ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³æ€§è¿›è¡Œé“¾æ¥
    
    def __init__(self):
        # æ„é€ å‡½æ•°
        self.context_weight = 0.3
        # è®¾ç½®ä¸Šä¸‹æ–‡æƒé‡ä¸º0.3ï¼ˆè™½ç„¶åœ¨å½“å‰å®ç°ä¸­æœªç›´æ¥ä½¿ç”¨ï¼‰
    
    def link(self, entities, context):
        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥æ–¹æ³•
        # entities: å¾…é“¾æ¥çš„å®ä½“åˆ—è¡¨
        # context: å®Œæ•´çš„é—®é¢˜æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡
        
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥"""
        
        if not ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
            # æ£€æŸ¥å¤šç­–ç•¥é“¾æ¥æ˜¯å¦å¯ç”¨
            return {}
            
        links = {}
        # åˆå§‹åŒ–é“¾æ¥ç»“æœå­—å…¸
        
        context_words = set(context.lower().split())
        # å°†é—®é¢˜æ–‡æœ¬è½¬ä¸ºå°å†™åˆ†è¯ï¼Œåˆ›å»ºä¸Šä¸‹æ–‡è¯æ±‡é›†åˆ
        # ä¾‹å¦‚ï¼š"What causes Alzheimer disease?" -> {"what", "causes", "alzheimer", "disease"}
        
        for entity in entities:
            # éå†æ¯ä¸ªå¾…é“¾æ¥çš„å®ä½“
            entity_words = set(entity.lower().split())
            # å°†å®ä½“è½¬ä¸ºå°å†™åˆ†è¯ï¼Œåˆ›å»ºå®ä½“è¯æ±‡é›†åˆ
            
            context_overlap = len(entity_words.intersection(context_words))
            # è®¡ç®—å®ä½“è¯æ±‡ä¸ä¸Šä¸‹æ–‡è¯æ±‡çš„é‡å æ•°é‡
            
            context_score = context_overlap / len(entity_words) if entity_words else 0
            # è®¡ç®—ä¸Šä¸‹æ–‡åˆ†æ•°ï¼šé‡å è¯æ±‡æ•° / å®ä½“æ€»è¯æ±‡æ•°
            # è¿™è¡¨ç¤ºå®ä½“æœ‰å¤šå°‘æ¯”ä¾‹çš„è¯æ±‡å‡ºç°åœ¨é—®é¢˜ä¸Šä¸‹æ–‡ä¸­
            
            links[entity] = {
                'context_score': context_score,
                'method': 'context_aware'
            }
            # å­˜å‚¨æ¯ä¸ªå®ä½“çš„ä¸Šä¸‹æ–‡é“¾æ¥ç»“æœ
        
        return links
        # è¿”å›æ‰€æœ‰å®ä½“çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥ç»“æœ
        
class ConfidenceEstimator:
    # ç½®ä¿¡åº¦ä¼°è®¡å™¨ï¼šèåˆå¤šç§åŒ¹é…ç­–ç•¥çš„ç»“æœå¹¶è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
    
    def __init__(self):
        # æ„é€ å‡½æ•°ï¼Œè®¾ç½®èåˆæƒé‡
        self.weight_semantic = 0.6
        # è¯­ä¹‰åŒ¹é…çš„æƒé‡ä¸º0.6ï¼ˆå ä¸»å¯¼åœ°ä½ï¼‰
        self.weight_context = 0.4
        # ä¸Šä¸‹æ–‡åŒ¹é…çš„æƒé‡ä¸º0.4ï¼ˆèµ·è¾…åŠ©ä½œç”¨ï¼‰
    
    def fuse_results(self, semantic_matches, context_matches):
        # ç»“æœèåˆæ–¹æ³•
        # semantic_matches: è¯­ä¹‰åŒ¹é…å™¨çš„è¾“å‡ºç»“æœ
        # context_matches: ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥å™¨çš„è¾“å‡ºç»“æœ
        
        """ç½®ä¿¡åº¦ä¼°è®¡å’Œèåˆ"""
        
        if not ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
            # æ£€æŸ¥å¤šç­–ç•¥é“¾æ¥æ˜¯å¦å¯ç”¨
            return {}
            
        final_links = {}
        # åˆå§‹åŒ–æœ€ç»ˆé“¾æ¥ç»“æœå­—å…¸
        
        all_entities = set(semantic_matches.keys()) | set(context_matches.keys())
        # ä½¿ç”¨é›†åˆå¹¶è¿ç®—è·å–æ‰€æœ‰å‡ºç°è¿‡çš„å®ä½“
        # ç¡®ä¿å³ä½¿æŸä¸ªå®ä½“åªåœ¨ä¸€ç§åŒ¹é…ä¸­å‡ºç°ä¹Ÿä¼šè¢«å¤„ç†
        
        for entity in all_entities:
            # éå†æ‰€æœ‰å®ä½“
            semantic_score = semantic_matches.get(entity, {}).get('score', 0)
            # å®‰å…¨åœ°è·å–è¯­ä¹‰åŒ¹é…åˆ†æ•°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º0
            # ä½¿ç”¨é“¾å¼get()è°ƒç”¨é¿å…KeyErrorå¼‚å¸¸
            
            context_score = context_matches.get(entity, {}).get('context_score', 0)
            # å®‰å…¨åœ°è·å–ä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ•°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º0
            
            combined_score = (self.weight_semantic * semantic_score + 
                            self.weight_context * context_score)
            # è®¡ç®—åŠ æƒç»„åˆåˆ†æ•°ï¼š0.6 * è¯­ä¹‰åˆ†æ•° + 0.4 * ä¸Šä¸‹æ–‡åˆ†æ•°
            # è¿™æ ·è¯­ä¹‰åŒ¹é…çš„å½±å“æ›´å¤§ï¼Œä¸Šä¸‹æ–‡åŒ¹é…èµ·åˆ°è°ƒèŠ‚ä½œç”¨
            
            final_links[entity] = {
                'final_score': combined_score,
                'semantic_score': semantic_score,
                'context_score': context_score,
                'method': 'fused'
            }
            # å­˜å‚¨èåˆåçš„å®Œæ•´ç»“æœï¼ŒåŒ…å«æœ€ç»ˆåˆ†æ•°ã€å„é¡¹åˆ†æ•°å’Œæ–¹æ³•æ ‡è¯†
        
        return final_links
        # è¿”å›æ‰€æœ‰å®ä½“çš„èåˆé“¾æ¥ç»“æœ

class EnhancedEntityLinking:
    # å®šä¹‰å¢å¼ºçš„å®ä½“é“¾æ¥ç±»ï¼Œç”¨äºå°†åŒ»å­¦æ–‡æœ¬ä¸­çš„å®ä½“é“¾æ¥åˆ°æ ‡å‡†åŒ–çš„UMLSæ¦‚å¿µ
    
    def __init__(self):
        # ç±»çš„æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶
        self.semantic_matcher = SemanticMatcher()
        # åˆå§‹åŒ–è¯­ä¹‰åŒ¹é…å™¨ï¼šé€šè¿‡è®¡ç®—å®ä½“é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…
        # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦ç­‰æ–¹æ³•æ¯”è¾ƒå®ä½“çš„è¯­ä¹‰ç‰¹å¾
        
        self.context_aware_linker = ContextAwareLinker()
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥å™¨ï¼šè€ƒè™‘å®ä½“åœ¨é—®é¢˜ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³æ€§
        # åˆ†æå®ä½“ä¸é—®é¢˜ä¸Šä¸‹æ–‡è¯æ±‡çš„é‡å åº¦ï¼Œæä¾›ä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ•°
        
        self.confidence_estimator = ConfidenceEstimator()
        # åˆå§‹åŒ–ç½®ä¿¡åº¦ä¼°è®¡å™¨ï¼šèåˆå¤šç§åŒ¹é…ç­–ç•¥çš„ç»“æœå¹¶è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        # å°†è¯­ä¹‰åŒ¹é…å’Œä¸Šä¸‹æ–‡åŒ¹é…çš„ç»“æœè¿›è¡ŒåŠ æƒèåˆ
    
    def multi_strategy_linking(self, entities, context, umls_kg):
        # å®šä¹‰å¤šç­–ç•¥å®ä½“é“¾æ¥æ–¹æ³•
        # å‚æ•°è¯´æ˜ï¼š
        # - entities: å¾…é“¾æ¥çš„å®ä½“åˆ—è¡¨ï¼ˆä»é—®é¢˜ä¸­æå–çš„åŒ»å­¦æœ¯è¯­ï¼‰
        # - context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå®Œæ•´çš„é—®é¢˜æ–‡æœ¬ï¼‰
        # - umls_kg: UMLSçŸ¥è¯†å›¾è°±ï¼ˆæ ‡å‡†åŒ–çš„åŒ»å­¦æ¦‚å¿µé›†åˆï¼‰
        
        """å¤šç­–ç•¥å®ä½“é“¾æ¥"""
        # æ–¹æ³•çš„ä¸­æ–‡æè¿°æ³¨é‡Š
        
        if not ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
            # æ£€æŸ¥æ¶ˆèå®éªŒé…ç½®ï¼šå¦‚æœå¤šç­–ç•¥é“¾æ¥åŠŸèƒ½è¢«ç¦ç”¨
            # ABLATION_CONFIGæ˜¯ç”¨äºæ§åˆ¶ä¸åŒæ¨¡å—å¼€å…³çš„å®éªŒé…ç½®
            return {}
            # è¿”å›ç©ºå­—å…¸ï¼Œè·³è¿‡å¤šç­–ç•¥é“¾æ¥å¤„ç†
            
        semantic_matches = self.semantic_matcher.match(entities, umls_kg)
        # è°ƒç”¨è¯­ä¹‰åŒ¹é…å™¨è¿›è¡Œç¬¬ä¸€è½®åŒ¹é…
        # è®¡ç®—æ¯ä¸ªå®ä½“ä¸UMLSçŸ¥è¯†å›¾è°±ä¸­æ¦‚å¿µçš„è¯­ä¹‰ç›¸ä¼¼åº¦
        # è¿”å›æ ¼å¼ï¼š{entity: {'match': matched_concept, 'score': similarity_score, 'method': 'semantic'}}
        
        context_matches = self.context_aware_linker.link(entities, context)
        # è°ƒç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é“¾æ¥å™¨è¿›è¡Œç¬¬äºŒè½®åŒ¹é…
        # åŸºäºå®ä½“åœ¨é—®é¢˜ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³æ€§è¿›è¡Œé“¾æ¥
        # è¿”å›æ ¼å¼ï¼š{entity: {'context_score': overlap_score, 'method': 'context_aware'}}
        
        final_links = self.confidence_estimator.fuse_results(
            semantic_matches, context_matches
        )
        # è°ƒç”¨ç½®ä¿¡åº¦ä¼°è®¡å™¨èåˆä¸¤ç§åŒ¹é…ç»“æœ
        # å°†è¯­ä¹‰åŒ¹é…åˆ†æ•°å’Œä¸Šä¸‹æ–‡åŒ¹é…åˆ†æ•°æŒ‰æƒé‡ç»„åˆ
        # è®¡ç®—æœ€ç»ˆçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼šfinal_score = 0.6 * semantic_score + 0.4 * context_score
        
        return final_links
        # è¿”å›èåˆåçš„æœ€ç»ˆé“¾æ¥ç»“æœ
        # æ ¼å¼ï¼š{entity: {'final_score': combined_score, 'semantic_score': score1, 'context_score': score2, 'method': 'fused'}}

# ========================= è‡ªé€‚åº”UMLSçŸ¥è¯†é€‰æ‹© =========================

class AdaptiveUMLSSelector:
    def __init__(self, umls_api):
        self.umls_api = umls_api
        self.task_specific_weights = {
            'treatment': {
                'therapeutic_procedure': 2.0,
                'pharmacologic_substance': 1.8,
                'clinical_drug': 1.6
            },
            'diagnosis': {
                'disease_syndrome': 2.0,
                'sign_symptom': 1.8,
                'finding': 1.6
            },
            'causation': {
                'disease_syndrome': 1.8,
                'pathologic_function': 1.6,
                'injury_poisoning': 1.4
            },
            'prevention': {
                'therapeutic_procedure': 1.8,
                'preventive_procedure': 2.0,
                'health_care_activity': 1.6
            }
        }
    
    def select_relevant_umls_knowledge(self, question_type, entities):
        """æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç›¸å…³UMLSçŸ¥è¯†"""
        if not ABLATION_CONFIG['USE_ADAPTIVE_UMLS']:
            return self.get_general_knowledge(entities)
            
        if question_type == 'treatment':
            return self.get_treatment_focused_knowledge(entities)
        elif question_type == 'diagnosis':
            return self.get_diagnosis_focused_knowledge(entities)
        elif question_type == 'causation':
            return self.get_causation_focused_knowledge(entities)
        elif question_type == 'prevention':
            return self.get_prevention_focused_knowledge(entities)
        else:
            return self.get_general_knowledge(entities)
    
    def get_treatment_focused_knowledge(self, entities):
        """è·å–æ²»ç–—ç›¸å…³çš„çŸ¥è¯†"""
        treatment_knowledge = []
        
        for entity in entities:
            concepts = self.umls_api.search_concepts(entity)
            if concepts and 'results' in concepts:
                for concept in concepts['results'][:5]:
                    cui = concept.get('ui', '')
                    relations = self.umls_api.get_concept_relations(cui)
                    
                    treatment_relations = [
                        rel for rel in relations 
                        if any(keyword in rel.get('relationLabel', '').lower() 
                               for keyword in ['treat', 'therapy', 'medication'])
                    ]
                    
                    if treatment_relations:
                        treatment_knowledge.extend(treatment_relations)
        
        return treatment_knowledge
    
    def get_diagnosis_focused_knowledge(self, entities):
        """è·å–è¯Šæ–­ç›¸å…³çš„çŸ¥è¯†"""
        diagnosis_knowledge = []
        
        for entity in entities:
            concepts = self.umls_api.search_concepts(entity)
            if concepts and 'results' in concepts:
                for concept in concepts['results'][:5]:
                    cui = concept.get('ui', '')
                    relations = self.umls_api.get_concept_relations(cui)
                    
                    diagnosis_relations = [
                        rel for rel in relations 
                        if any(keyword in rel.get('relationLabel', '').lower() 
                               for keyword in ['diagnose', 'symptom', 'sign', 'finding'])
                    ]
                    
                    if diagnosis_relations:
                        diagnosis_knowledge.extend(diagnosis_relations)
        
        return diagnosis_knowledge
    
    def get_causation_focused_knowledge(self, entities):
        """è·å–å› æœå…³ç³»ç›¸å…³çš„çŸ¥è¯†"""
        causation_knowledge = []
        
        for entity in entities:
            concepts = self.umls_api.search_concepts(entity)
            if concepts and 'results' in concepts:
                for concept in concepts['results'][:5]:
                    cui = concept.get('ui', '')
                    relations = self.umls_api.get_concept_relations(cui)
                    
                    causation_relations = [
                        rel for rel in relations 
                        if any(keyword in rel.get('relationLabel', '').lower() 
                               for keyword in ['cause', 'lead', 'result', 'induce'])
                    ]
                    
                    if causation_relations:
                        causation_knowledge.extend(causation_relations)
        
        return causation_knowledge
    
    def get_prevention_focused_knowledge(self, entities):
        """è·å–é¢„é˜²ç›¸å…³çš„çŸ¥è¯†"""
        prevention_knowledge = []
        
        for entity in entities:
            concepts = self.umls_api.search_concepts(entity)
            if concepts and 'results' in concepts:
                for concept in concepts['results'][:5]:
                    cui = concept.get('ui', '')
                    relations = self.umls_api.get_concept_relations(cui)
                    
                    prevention_relations = [
                        rel for rel in relations 
                        if any(keyword in rel.get('relationLabel', '').lower() 
                               for keyword in ['prevent', 'avoid', 'reduce_risk'])
                    ]
                    
                    if prevention_relations:
                        prevention_knowledge.extend(prevention_relations)
        
        return prevention_knowledge
    
    def get_general_knowledge(self, entities):
        """è·å–é€šç”¨çŸ¥è¯†"""
        general_knowledge = []
        
        for entity in entities:
            # 1. å¯¹æ¯ä¸ªå®ä½“æœç´¢UMLSæ¦‚å¿µ
            concepts = self.umls_api.search_concepts(entity)
            if concepts and 'results' in concepts:
                # 2. åªå–å‰3ä¸ªæœ€ç›¸å…³çš„æ¦‚å¿µ
                for concept in concepts['results'][:3]:
                    cui = concept.get('ui', '')
                    # 3. è·å–è¯¥æ¦‚å¿µçš„æ‰€æœ‰å…³ç³»
                    relations = self.umls_api.get_concept_relations(cui)
                    # 4. æ¯ä¸ªæ¦‚å¿µåªå–å‰10ä¸ªå…³ç³»
                    general_knowledge.extend(relations[:10])
        
        return general_knowledge

# ========================= çŸ¥è¯†å›¾è°±å¼•å¯¼çš„æ€ç»´é“¾æ¨ç† =========================

class SchemaReasoner:
    def __init__(self):
        self.medical_schemas = {
            'diagnosis': ['symptom', 'finding', 'test', 'disease'],
            'treatment': ['disease', 'medication', 'procedure', 'outcome'],
            'causation': ['risk_factor', 'cause', 'disease', 'complication'],
            'prevention': ['risk_factor', 'intervention', 'prevention', 'outcome']
        }
    
    def infer_paths(self, question, kg):
        """åŸºäºæ¨¡å¼æ¨ç†è·¯å¾„"""
        if not ABLATION_CONFIG['USE_KG_GUIDED_REASONING']:
            return []
            
        question_type = self._identify_question_schema(question)
        schema = self.medical_schemas.get(question_type, [])
        
        reasoning_paths = []
        for i in range(len(schema) - 1):
            start_type = schema[i]
            end_type = schema[i + 1]
            paths = self._find_schema_paths(kg, start_type, end_type)
            reasoning_paths.extend(paths)
        
        return reasoning_paths
    
    def _identify_question_schema(self, question):
        """è¯†åˆ«é—®é¢˜æ¨¡å¼"""
        question_lower = question.lower()
        
        if any(keyword in question_lower for keyword in ['treat', 'therapy', 'medication']):
            return 'treatment'
        elif any(keyword in question_lower for keyword in ['cause', 'why', 'due to']):
            return 'causation'
        elif any(keyword in question_lower for keyword in ['prevent', 'avoid', 'reduce risk']):
            return 'prevention'
        else:
            return 'diagnosis'
    
    def _find_schema_paths(self, kg, start_type, end_type):
        """æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„è·¯å¾„"""
        paths = []
        for triple in kg:
            if len(triple) >= 3:
                if (start_type in triple[0].lower() or start_type in triple[1].lower()) and \
                   (end_type in triple[2].lower() or end_type in triple[1].lower()):
                    paths.append(triple)
        return paths

class KGGuidedReasoningEngine:
    def __init__(self, kg, llm):
        self.kg = kg
        self.llm = llm
        self.schema_reasoner = SchemaReasoner()
    
    def kg_guided_reasoning(self, question, kg_subgraph):
        """çŸ¥è¯†å›¾è°±å¼•å¯¼çš„æ¨ç†"""
        if not ABLATION_CONFIG['USE_KG_GUIDED_REASONING']:
            return "KG-guided reasoning disabled in ablation study"
            
        schema_paths = self.schema_reasoner.infer_paths(question, self.kg)
        optimal_subgraph = self.generate_optimal_subgraph(
            question, schema_paths, kg_subgraph
        )
        reasoning_result = self.llm_reasoning_with_kg(question, optimal_subgraph)
        
        return reasoning_result
    
    def generate_optimal_subgraph(self, question, schema_paths, kg_subgraph):
        """ç”Ÿæˆæœ€ä¼˜å­å›¾"""
        combined_graph = kg_subgraph + schema_paths
        
        scored_triples = []
        for triple in combined_graph:
            score = self._calculate_relevance_score(question, triple)
            scored_triples.append((triple, score))
        
        scored_triples.sort(key=lambda x: x[1], reverse=True)
        optimal_subgraph = [triple for triple, score in scored_triples[:15]]
        
        return optimal_subgraph
    
    def _calculate_relevance_score(self, question, triple):
        """è®¡ç®—ä¸‰å…ƒç»„ä¸é—®é¢˜çš„ç›¸å…³æ€§åˆ†æ•°"""
        question_words = set(question.lower().split())
        triple_words = set()
        
        for element in triple:
            triple_words.update(element.lower().split())
        
        overlap = len(question_words.intersection(triple_words))
        relevance_score = overlap / len(question_words) if question_words else 0
        
        return relevance_score
    
    def llm_reasoning_with_kg(self, question, kg_subgraph):
        """ä½¿ç”¨LLMè¿›è¡ŒçŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†"""
        kg_context = "\n".join([f"{t[0]} -> {t[1]} -> {t[2]}" for t in kg_subgraph])
        
        prompt = f"""
        Question: {question}
        
        Knowledge Graph Context:
        {kg_context}
        
        Based on the structured medical knowledge above, provide step-by-step reasoning to answer the question.
        Focus on the relationships and pathways shown in the knowledge graph.
        """
        
        try:
            response = self.llm([HumanMessage(content=prompt)])
            return response.content
        except Exception as e:
            logger.error(f"Error in LLM reasoning: {e}")
            return "Unable to generate reasoning based on knowledge graph."

# ========================= ä¼˜åŒ–å¤šè·³æ¨ç† =========================

class PathRanker:
    def __init__(self):
        self.medical_relation_weights = {
            'causes': 3.0,
            'treats': 2.8,
            'prevents': 2.5,
            'symptom_of': 2.2,
            'diagnoses': 2.0,
            'associated_with': 1.8,
            'located_in': 1.5,
            'part_of': 1.2,
            'related_to': 1.0
        }
    
    def rank_by_quality(self, paths):
        """æ ¹æ®è´¨é‡å¯¹è·¯å¾„è¿›è¡Œæ’åº"""
        if not ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
            return paths
            
        scored_paths = []
        
        for path in paths:
            quality_score = self._calculate_path_quality(path)
            scored_paths.append((path, quality_score))
        
        scored_paths.sort(key=lambda x: x[1], reverse=True)
        return [path for path, score in scored_paths]
    
    def _calculate_path_quality(self, path):
        """è®¡ç®—è·¯å¾„è´¨é‡åˆ†æ•°"""
        if not path:
            return 0
        
        relation_score = 0
        for step in path:
            if len(step) >= 2:
                relation = step[1].lower()
                for key, weight in self.medical_relation_weights.items():
                    if key in relation:
                        relation_score += weight
                        break
                else:
                    relation_score += 0.5
        
        length_penalty = len(path) * 0.1
        quality_score = relation_score - length_penalty
        
        return quality_score

class OptimizedMultiHopReasoning:
    def __init__(self, kg, path_ranker=None):
        self.kg = kg
        self.path_ranker = path_ranker or PathRanker()
        self.reasoning_cache = {}
    
    def intelligent_path_selection(self, start_entities, target_entities, max_hops=3):
        """æ™ºèƒ½è·¯å¾„é€‰æ‹©"""
        if not ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
            return self._basic_path_selection(start_entities, target_entities, max_hops)
            
        weighted_paths = self.calculate_medical_relevance_weights(
            start_entities, target_entities
        )
        
        pruned_paths = self.dynamic_pruning(weighted_paths, max_hops)
        quality_ranked_paths = self.path_ranker.rank_by_quality(pruned_paths)
        
        return quality_ranked_paths
    
    def _basic_path_selection(self, start_entities, target_entities, max_hops):
        """åŸºç¡€ç‰ˆæœ¬çš„è·¯å¾„é€‰æ‹©"""
        basic_paths = []
        for start_entity in start_entities:
            for target_entity in target_entities:
                paths = self._find_connecting_paths(start_entity, target_entity)
                basic_paths.extend(paths[:3])
        return basic_paths
    
    def calculate_medical_relevance_weights(self, start_entities, target_entities):
        """è®¡ç®—åŸºäºåŒ»å­¦çŸ¥è¯†çš„è·¯å¾„æƒé‡"""
        weighted_paths = []
        
        for start_entity in start_entities:
            for target_entity in target_entities:
                cache_key = f"{start_entity}-{target_entity}"
                
                if cache_key in self.reasoning_cache:
                    weighted_paths.extend(self.reasoning_cache[cache_key])
                    continue
                
                paths = self._find_connecting_paths(start_entity, target_entity)
                
                for path in paths:
                    weight = self._calculate_medical_relevance(path)
                    weighted_paths.append((path, weight))
                
                self.reasoning_cache[cache_key] = [(path, weight) for path, weight in weighted_paths[-len(paths):]]
        
        return weighted_paths
    
    def dynamic_pruning(self, weighted_paths, max_hops):
        """åŠ¨æ€å‰ªæç­–ç•¥"""
        pruned_paths = []
        
        weighted_paths.sort(key=lambda x: x[1], reverse=True)
        
        for path, weight in weighted_paths:
            if len(path) <= max_hops:
                if weight > 0.5:
                    pruned_paths.append(path)
            
            if len(pruned_paths) >= 20:
                break
        
        return pruned_paths
    
    def _find_connecting_paths(self, start_entity, target_entity):
        """æŸ¥æ‰¾è¿æ¥è·¯å¾„"""
        paths = []
        
        for triple in self.kg:
            if len(triple) >= 3:
                if triple[0] == start_entity and triple[2] == target_entity:
                    paths.append([triple])
        
        intermediate_entities = set()
        for triple in self.kg:
            if len(triple) >= 3 and triple[0] == start_entity:
                intermediate_entities.add(triple[2])
        
        for intermediate in intermediate_entities:
            for triple in self.kg:
                if len(triple) >= 3 and triple[0] == intermediate and triple[2] == target_entity:
                    first_hop = next((t for t in self.kg if len(t) >= 3 and t[0] == start_entity and t[2] == intermediate), None)
                    if first_hop:
                        paths.append([first_hop, triple])
        
        return paths[:10]
    
    def _calculate_medical_relevance(self, path):
        """è®¡ç®—åŒ»å­¦ç›¸å…³æ€§"""
        relevance_score = 0
        
        for step in path:
            if len(step) >= 3:
                entity_score = self._get_entity_medical_score(step[0]) + self._get_entity_medical_score(step[2])
                relation_score = self._get_relation_medical_score(step[1])
                relevance_score += entity_score + relation_score
        
        return relevance_score / len(path) if path else 0
    
    def _get_entity_medical_score(self, entity):
        """è·å–å®ä½“çš„åŒ»å­¦ç›¸å…³æ€§åˆ†æ•°"""
        medical_keywords = ['disease', 'symptom', 'treatment', 'medication', 'diagnosis', 'therapy']
        entity_lower = entity.lower()
        
        score = 0
        for keyword in medical_keywords:
            if keyword in entity_lower:
                score += 1
        
        return score
    
    def _get_relation_medical_score(self, relation):
        """è·å–å…³ç³»çš„åŒ»å­¦ç›¸å…³æ€§åˆ†æ•°"""
        relation_weights = {
            'causes': 3.0, 'treats': 2.8, 'prevents': 2.5,
            'symptom_of': 2.2, 'diagnoses': 2.0, 'associated_with': 1.8
        }
        
        relation_lower = relation.lower()
        for key, weight in relation_weights.items():
            if key in relation_lower:
                return weight
        
        return 1.0

# ========================= UMLS APIé›†æˆ =========================

class UMLS_API:
    def __init__(self, api_key, version="current"):
        """åˆå§‹åŒ–UMLS APIå®¢æˆ·ç«¯"""
        self.api_key = api_key
        self.version = version
        self.search_url = f"https://uts-ws.nlm.nih.gov/rest/search/{version}"
        self.content_url = f"https://uts-ws.nlm.nih.gov/rest/content/{version}"
        
        self.session = requests.Session()
        self.session.timeout = 30
        
        self.cache = {}
        self.cache_size = 10000
        self.failed_cuis = set()
        
        if ABLATION_CONFIG['USE_UMLS_NORMALIZATION'] or ABLATION_CONFIG['USE_ADAPTIVE_UMLS']:
            try:
                self._test_connection()
                logger.info("UMLS API connection successful")
            except Exception as e:
                logger.warning(f"UMLS API connection failed: {e}. Operating in offline mode.")
        else:
            logger.info("ğŸ”¬ UMLS API disabled in ablation study")
    
    def _test_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        try:
            params = {
                "string": "pain",
                "apiKey": self.api_key,
                "pageNumber": 1,
                "pageSize": 1
            }
            response = self.session.get(self.search_url, params=params, timeout=5)
            response.raise_for_status()
            data = response.json()
            if 'result' not in data:
                raise Exception("Invalid API response format")
        except Exception as e:
            raise Exception(f"API connection test failed: {e}")
    
    def search_concepts(self, search_string, search_type="words", page_size=25):
        """æœç´¢UMLSæ¦‚å¿µ"""
        if not (ABLATION_CONFIG['USE_UMLS_NORMALIZATION'] or ABLATION_CONFIG['USE_ADAPTIVE_UMLS']):
            return None
            
        cache_key = f"search_{search_string}_{page_size}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            params = {
                "string": search_string,
                "apiKey": self.api_key,
                "pageNumber": 1,
                "pageSize": page_size
            }
            
            response = self.session.get(self.search_url, params=params)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            data = response.json()
            result = data.get("result", {})
            
            if len(self.cache) < self.cache_size:
                self.cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Error searching concepts for '{search_string}': {e}")
            return None
    
    def get_concept_details(self, cui):
        """è·å–æ¦‚å¿µè¯¦ç»†ä¿¡æ¯"""
        if not (ABLATION_CONFIG['USE_UMLS_NORMALIZATION'] or ABLATION_CONFIG['USE_ADAPTIVE_UMLS']):
            return None
            
        cache_key = f"details_{cui}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            url = f"{self.content_url}/CUI/{cui}"
            params = {"apiKey": self.api_key}
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            response.encoding = "utf-8"
            
            data = response.json()
            result = data.get("result", {})
            
            if len(self.cache) < self.cache_size:
                self.cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Error getting details for CUI {cui}: {e}")
            return None
    
    def get_concept_atoms(self, cui):
        """è·å–æ¦‚å¿µçš„åŸå­ä¿¡æ¯"""
        if not (ABLATION_CONFIG['USE_UMLS_NORMALIZATION'] or ABLATION_CONFIG['USE_ADAPTIVE_UMLS']):
            return None
            
        cache_key = f"atoms_{cui}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            url = f"{self.content_url}/CUI/{cui}/atoms"
            params = {"apiKey": self.api_key, "pageSize": 100}
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            response.encoding = "utf-8"
            
            data = response.json()
            result = data.get("result", [])
            
            if len(self.cache) < self.cache_size:
                self.cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Error getting atoms for CUI {cui}: {e}")
            return None
    
    def get_concept_relations(self, cui):
        """è·å–æ¦‚å¿µå…³ç³»"""
        if not (ABLATION_CONFIG['USE_UMLS_NORMALIZATION'] or ABLATION_CONFIG['USE_ADAPTIVE_UMLS']):
            return []
            
        cache_key = f"relations_{cui}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        if cui in self.failed_cuis:
            return []
        
        all_relations = []
        
        try:
            for page in range(1, 6):
                url = f"{self.content_url}/CUI/{cui}/relations"
                params = {
                    "apiKey": self.api_key,
                    "pageNumber": page,
                    "pageSize": 100
                }
                
                response = self.session.get(url, params=params)
                response.raise_for_status()
                response.encoding = "utf-8"
                
                data = response.json()
                page_relations = data.get("result", [])
                
                if not page_relations:
                    break
                    
                all_relations.extend(page_relations)
            
            if len(self.cache) < self.cache_size:
                self.cache[cache_key] = all_relations
            
            return all_relations
            
        except requests.exceptions.HTTPError as e:
            if "404" in str(e):
                self.failed_cuis.add(cui)
                logger.warning(f"CUI {cui} not found (404), adding to failed cache")
            else:
                logger.error(f"HTTP error getting relations for CUI {cui}: {e}")
            return []
        except Exception as e:
            logger.error(f"Error getting relations for CUI {cui}: {e}")
            return []

class UMLSNormalizer:
    def __init__(self, api_key):
        """åˆå§‹åŒ–UMLSæ ‡å‡†åŒ–å™¨"""
        # åˆ›å»ºUMLS APIå®ä¾‹ï¼Œç”¨äºä¸UMLSæ•°æ®åº“äº¤äº’
        self.umls_api = UMLS_API(api_key)
        
        # æœ¬åœ°ç¼“å­˜å­—å…¸ï¼Œå­˜å‚¨å·²æŸ¥è¯¢è¿‡çš„æœ¯è¯­ç»“æœï¼Œé¿å…é‡å¤APIè°ƒç”¨
        self.local_cache = {}
        
        # è¯­ä¹‰ç±»å‹ç¼“å­˜ï¼Œå­˜å‚¨æ¦‚å¿µçš„è¯­ä¹‰ç±»å‹ä¿¡æ¯
        self.semantic_type_cache = {}
        
        # å¼•ç”¨å…¨å±€çš„å±‚æ¬¡åŒ–çŸ¥è¯†å›¾è°±æ¡†æ¶å®ä¾‹
        # ç”¨äºè·å–æ¦‚å¿µçš„å±‚æ¬¡ç»“æ„ä¿¡æ¯
        self.hierarchical_kg = hierarchical_kg_framework
        
        # åˆ›å»ºå¢å¼ºå®ä½“é“¾æ¥å™¨å®ä¾‹ï¼Œç”¨äºå¤šç­–ç•¥å®ä½“é“¾æ¥
        self.enhanced_entity_linking = EnhancedEntityLinking()
        
        # åˆ›å»ºè‡ªé€‚åº”UMLSé€‰æ‹©å™¨å®ä¾‹ï¼Œæ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç›¸å…³çŸ¥è¯†
        self.adaptive_umls_selector = AdaptiveUMLSSelector(self.umls_api)
        
        # å®šä¹‰è¯­ä¹‰ç±»å‹çš„ä¼˜å…ˆçº§æƒé‡å­—å…¸
        # UMLSä¸­ä¸åŒè¯­ä¹‰ç±»å‹çš„é‡è¦æ€§æ’åºï¼Œæ•°å€¼è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜
        self.semantic_type_priority = {
            'T047': 10,  # Disease or Syndrome - ç–¾ç—…æˆ–ç»¼åˆå¾ï¼Œæœ€é«˜ä¼˜å…ˆçº§
            'T184': 9,   # Sign or Symptom - ä½“å¾æˆ–ç—‡çŠ¶
            'T061': 8,   # Therapeutic or Preventive Procedure - æ²»ç–—æˆ–é¢„é˜²ç¨‹åº
            'T121': 7,   # Pharmacologic Substance - è¯ç†ç‰©è´¨
            'T023': 6,   # Body Part, Organ, or Organ Component - èº«ä½“éƒ¨ä½ã€å™¨å®˜æˆ–ç»„ä»¶
            'T037': 5,   # Injury or Poisoning - ä¼¤å®³æˆ–ä¸­æ¯’
            'T046': 4,   # Pathologic Function - ç—…ç†åŠŸèƒ½
            'T033': 3,   # Finding - å‘ç°
            'T170': 2,   # Intellectual Product - æ™ºåŠ›äº§å“
            'T169': 1    # Functional Concept - åŠŸèƒ½æ¦‚å¿µï¼Œæœ€ä½ä¼˜å…ˆçº§
        }
    
    def _get_best_cui_for_term(self, term):
        """ä¸ºç»™å®šæœ¯è¯­è·å–æœ€ä½³CUI"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†UMLSæ ‡å‡†åŒ–åŠŸèƒ½ï¼ˆæ¶ˆèå®éªŒæ§åˆ¶ï¼‰
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return None
            
        # é¦–å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤APIè°ƒç”¨
        if term in self.local_cache:
            return self.local_cache[term]
        
        try:
            # è°ƒç”¨UMLS APIæœç´¢ä¸termç›¸å…³çš„æ¦‚å¿µ
            search_results = self.umls_api.search_concepts(term)
            
            # æ£€æŸ¥APIè¿”å›ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if not search_results or 'results' not in search_results:
                return None
            
            # è·å–æœç´¢ç»“æœåˆ—è¡¨
            results = search_results['results']
            if not results:
                return None
            
            # åˆå§‹åŒ–æœ€ä½³åŒ¹é…å˜é‡
            best_cui = None      # æœ€ä½³åŒ¹é…çš„CUI
            best_score = 0       # æœ€é«˜åŒ¹é…åˆ†æ•°
            
            # éå†æ‰€æœ‰æœç´¢ç»“æœï¼Œæ‰¾åˆ°æœ€ä½³åŒ¹é…
            for result in results:
                cui = result['ui']        # è·å–æ¦‚å¿µçš„å”¯ä¸€æ ‡è¯†ç¬¦(CUI)
                name = result['name']     # è·å–æ¦‚å¿µçš„åç§°
                
                # è®¡ç®—å½“å‰ç»“æœä¸åŸæœ¯è¯­çš„åŒ¹é…åˆ†æ•°
                score = self._calculate_match_score(term, name, result)
                
                # å¦‚æœå½“å‰åˆ†æ•°æ›´é«˜ï¼Œæ›´æ–°æœ€ä½³åŒ¹é…
                if score > best_score:
                    best_score = score
                    best_cui = cui
            
            # å°†ç»“æœç¼“å­˜åˆ°æœ¬åœ°ï¼Œé¿å…é‡å¤æŸ¥è¯¢
            self.local_cache[term] = best_cui
            return best_cui
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯æ—¥å¿—å¹¶è¿”å›None
            logger.error(f"Error getting CUI for term '{term}': {e}")
            return None
    
    def _calculate_match_score(self, original_term, concept_name, result):
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        score = 0  # åˆå§‹åŒ–åˆ†æ•°
        
        # å®Œå…¨åŒ¹é…æ£€æŸ¥ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        if original_term.lower() == concept_name.lower():
            score += 100  # å®Œå…¨åŒ¹é…ç»™äºˆæœ€é«˜åˆ†æ•°
        # åŸæœ¯è¯­åŒ…å«åœ¨æ¦‚å¿µåç§°ä¸­
        elif original_term.lower() in concept_name.lower():
            score += 50   # éƒ¨åˆ†åŒ…å«ç»™äºˆä¸­ç­‰åˆ†æ•°
        # æ¦‚å¿µåç§°åŒ…å«åœ¨åŸæœ¯è¯­ä¸­
        elif concept_name.lower() in original_term.lower():
            score += 30   # åå‘åŒ…å«ç»™äºˆè¾ƒä½åˆ†æ•°
        
        # è¯æ±‡é‡å åº¦è®¡ç®—
        # å°†æœ¯è¯­å’Œæ¦‚å¿µåç§°åˆ†åˆ«æ‹†åˆ†ä¸ºå•è¯é›†åˆ
        original_words = set(original_term.lower().split())
        concept_words = set(concept_name.lower().split())
        # è®¡ç®—äº¤é›†ï¼ˆå…±åŒå•è¯æ•°é‡ï¼‰
        overlap = len(original_words & concept_words)
        # æ¯ä¸ªé‡å å•è¯è´¡çŒ®10åˆ†
        score += overlap * 10
        
        # è¯æ ¹åŒ¹é…æ£€æŸ¥
        if self._has_root_match(original_term, concept_name):
            score += 20  # è¯æ ¹åŒ¹é…é¢å¤–åŠ åˆ†
        
        return score
    
    def _has_root_match(self, term1, term2):
        """æ£€æŸ¥è¯æ ¹åŒ¹é…"""
        # å®šä¹‰å¸¸è§è‹±è¯­åç¼€åˆ—è¡¨
        suffixes = ['s', 'es', 'ing', 'ed', 'er', 'est', 'ly']
        
        def get_root(word):
            """æå–å•è¯è¯æ ¹çš„å†…éƒ¨å‡½æ•°"""
            # éå†åç¼€åˆ—è¡¨
            for suffix in suffixes:
                # å¦‚æœå•è¯ä»¥æŸä¸ªåç¼€ç»“å°¾
                if word.endswith(suffix):
                    # å»é™¤åç¼€è¿”å›è¯æ ¹
                    return word[:-len(suffix)]
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„åç¼€ï¼Œè¿”å›åŸå•è¯
            return word
        
        # è·å–ä¸¤ä¸ªæœ¯è¯­çš„è¯æ ¹ï¼ˆè½¬ä¸ºå°å†™ï¼‰
        root1 = get_root(term1.lower())
        root2 = get_root(term2.lower())
        
        # æ£€æŸ¥è¯æ ¹æ˜¯å¦ç›¸ç­‰æˆ–äº’ç›¸åŒ…å«
        return root1 == root2 or root1 in root2 or root2 in root1
    
    def get_concept_synonyms(self, cui):
        """è·å–æ¦‚å¿µçš„åŒä¹‰è¯"""
        # æ£€æŸ¥UMLSæ ‡å‡†åŒ–åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return []
            
        try:
            # è°ƒç”¨APIè·å–æ¦‚å¿µçš„åŸå­ä¿¡æ¯ï¼ˆatomsåŒ…å«æ‰€æœ‰åŒä¹‰è¡¨è¾¾ï¼‰
            atoms_result = self.umls_api.get_concept_atoms(cui)
            
            # æ£€æŸ¥APIè¿”å›ç»“æœ
            if not atoms_result:
                return []
            
            # åˆå§‹åŒ–åŒä¹‰è¯åˆ—è¡¨
            synonyms = []
            # éå†æ‰€æœ‰åŸå­è®°å½•
            for atom in atoms_result:
                # è·å–åŸå­çš„åç§°
                name = atom.get('name', '')
                # å¦‚æœåç§°å­˜åœ¨ä¸”ä¸é‡å¤ï¼Œæ·»åŠ åˆ°åŒä¹‰è¯åˆ—è¡¨
                if name and name not in synonyms:
                    synonyms.append(name)
            
            return synonyms
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
            logger.error(f"Error getting synonyms for CUI {cui}: {e}")
            return []
    
    def get_concept_relations(self, cui):
        """è·å–æ¦‚å¿µå…³ç³»"""
        # æ£€æŸ¥UMLSæ ‡å‡†åŒ–åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return []
            
        try:
            # è°ƒç”¨APIè·å–æ¦‚å¿µçš„æ‰€æœ‰å…³ç³»
            relations_result = self.umls_api.get_concept_relations(cui)
            
            # æ£€æŸ¥APIè¿”å›ç»“æœ
            if not relations_result:
                return []
            
            # åˆå§‹åŒ–å…³ç³»åˆ—è¡¨
            relations = []
            # éå†æ‰€æœ‰å…³ç³»è®°å½•
            for relation in relations_result:
                # æå–å…³ç³»çš„å„ä¸ªç»„ä»¶
                rel_type = relation.get('relationLabel', '')     # å…³ç³»ç±»å‹æ ‡ç­¾
                related_cui = relation.get('relatedId', '')      # ç›¸å…³æ¦‚å¿µçš„CUI
                related_name = relation.get('relatedIdName', '') # ç›¸å…³æ¦‚å¿µçš„åç§°
                
                # å¦‚æœå…³ç³»ç±»å‹å’Œç›¸å…³CUIéƒ½å­˜åœ¨
                if rel_type and related_cui:
                    # æ„å»ºå…³ç³»å­—å…¸å¹¶æ·»åŠ åˆ°åˆ—è¡¨
                    relations.append({
                        'relation_type': rel_type,
                        'related_cui': related_cui,
                        'related_name': related_name
                    })
            
            return relations
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
            logger.error(f"Error getting relations for CUI {cui}: {e}")
            return []
    
    def normalize_medical_terms(self, entities):
        """å°†åŒ»å­¦æœ¯è¯­æ ‡å‡†åŒ–ä¸ºUMLSæ¦‚å¿µ"""
        # æ£€æŸ¥UMLSæ ‡å‡†åŒ–åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return entities  # å¦‚æœç¦ç”¨ï¼Œç›´æ¥è¿”å›åŸå®ä½“åˆ—è¡¨
            
        # åˆå§‹åŒ–æ ‡å‡†åŒ–åçš„å®ä½“åˆ—è¡¨
        normalized_entities = []
        
        # éå†æ¯ä¸ªè¾“å…¥å®ä½“
        for entity in entities:
            try:
                # ä¸ºå½“å‰å®ä½“è·å–æœ€ä½³åŒ¹é…çš„CUI
                cui = self._get_best_cui_for_term(entity)
                
                # å¦‚æœæ‰¾åˆ°äº†CUI
                if cui:
                    # è·å–è¯¥CUIå¯¹åº”çš„æ¦‚å¿µè¯¦ç»†ä¿¡æ¯
                    concept_details = self.umls_api.get_concept_details(cui)
                    
                    # å¦‚æœæˆåŠŸè·å–æ¦‚å¿µè¯¦æƒ…
                    if concept_details:
                        # æå–é¦–é€‰åç§°ï¼ˆUMLSæ ‡å‡†åç§°ï¼‰
                        preferred_name = concept_details.get('name', entity)
                        # æ·»åŠ æ ‡å‡†åç§°åˆ°ç»“æœåˆ—è¡¨
                        normalized_entities.append(preferred_name)
                        # è®°å½•æ ‡å‡†åŒ–è¿‡ç¨‹çš„è°ƒè¯•ä¿¡æ¯
                        logger.debug(f"æ ‡å‡†åŒ–: {entity} -> {preferred_name} (CUI: {cui})")
                    else:
                        # å¦‚æœè·å–è¯¦æƒ…å¤±è´¥ï¼Œä¿ç•™åŸå®ä½“
                        normalized_entities.append(entity)
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°CUIï¼Œä¿ç•™åŸå®ä½“
                    normalized_entities.append(entity)
                    
            except Exception as e:
                # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶ä¿ç•™åŸå®ä½“
                logger.error(f"Error normalizing entity '{entity}': {e}")
                normalized_entities.append(entity)
        
        return normalized_entities
    
    def get_semantic_variants(self, entity):
        """è·å–å®ä½“çš„è¯­ä¹‰å˜ä½“"""
        # æ£€æŸ¥UMLSæ ‡å‡†åŒ–åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return [entity]  # ç¦ç”¨æ—¶è¿”å›åŸå®ä½“
            
        try:
            # è·å–å®ä½“å¯¹åº”çš„CUI
            cui = self._get_best_cui_for_term(entity)
            if not cui:
                return [entity]  # æ²¡æ‰¾åˆ°CUIï¼Œè¿”å›åŸå®ä½“
            
            # è·å–åŒä¹‰è¯åˆ—è¡¨
            synonyms = self.get_concept_synonyms(cui)
            # è·å–å…³ç³»åˆ—è¡¨
            relations = self.get_concept_relations(cui)
            related_terms = []  # ç›¸å…³æœ¯è¯­åˆ—è¡¨
            
            # ä»å…³ç³»ä¸­æå–ç‰¹å®šç±»å‹çš„ç›¸å…³æœ¯è¯­
            for relation in relations:
                # åªé€‰æ‹©ç‰¹å®šå…³ç³»ç±»å‹çš„ç›¸å…³æœ¯è¯­
                if relation['relation_type'] in ['SY', 'PT', 'equivalent_to']:
                    related_terms.append(relation['related_name'])
            
            # åˆå¹¶åŸå®ä½“ã€åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­
            variants = [entity] + synonyms + related_terms
            
            # å»é‡å¤„ç†
            unique_variants = []  # å”¯ä¸€å˜ä½“åˆ—è¡¨
            seen = set()          # å·²è§è¿‡çš„æœ¯è¯­é›†åˆï¼ˆå°å†™ï¼‰
            
            # éå†æ‰€æœ‰å˜ä½“è¿›è¡Œå»é‡
            for variant in variants:
                # æ£€æŸ¥å˜ä½“æ˜¯å¦æœ‰æ•ˆä¸”æœªé‡å¤
                if variant and variant.lower() not in seen and len(variant) > 2:
                    seen.add(variant.lower())      # è®°å½•å°å†™ç‰ˆæœ¬
                    unique_variants.append(variant)  # æ·»åŠ åŸå§‹å¤§å°å†™ç‰ˆæœ¬
            
            # æœ€å¤šè¿”å›10ä¸ªå˜ä½“ï¼Œé¿å…ç»“æœè¿‡å¤š
            return unique_variants[:10]
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›åŸå®ä½“
            logger.error(f"Error getting semantic variants for '{entity}': {e}")
            return [entity]
    
    def get_concept_hierarchy(self, entity):
        """è·å–æ¦‚å¿µå±‚æ¬¡ç»“æ„"""
        # æ£€æŸ¥UMLSæ ‡å‡†åŒ–åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            return {}  # ç¦ç”¨æ—¶è¿”å›ç©ºå­—å…¸
            
        try:
            # è·å–å®ä½“å¯¹åº”çš„CUI
            cui = self._get_best_cui_for_term(entity)
            if not cui:
                return {}  # æ²¡æ‰¾åˆ°CUIï¼Œè¿”å›ç©ºå­—å…¸
            
            # è·å–æ¦‚å¿µçš„æ‰€æœ‰å…³ç³»
            relations = self.get_concept_relations(cui)
            
            # åˆå§‹åŒ–å±‚æ¬¡ç»“æ„å­—å…¸
            hierarchy = {
                'broader': [],   # ä¸Šä½æ¦‚å¿µï¼ˆæ›´å®½æ³›çš„æ¦‚å¿µï¼‰
                'narrower': [],  # ä¸‹ä½æ¦‚å¿µï¼ˆæ›´å…·ä½“çš„æ¦‚å¿µï¼‰
                'related': []    # ç›¸å…³æ¦‚å¿µï¼ˆåŒçº§æ¦‚å¿µï¼‰
            }
            
            # éå†æ‰€æœ‰å…³ç³»ï¼ŒæŒ‰ç±»å‹åˆ†ç±»
            for relation in relations:
                rel_type = relation['relation_type']      # å…³ç³»ç±»å‹
                related_name = relation['related_name']   # ç›¸å…³æ¦‚å¿µåç§°
                
                # æ ¹æ®å…³ç³»ç±»å‹åˆ†ç±»åˆ°ç›¸åº”ç±»åˆ«
                if rel_type in ['RB', 'inverse_isa', 'parent']:
                    # ä¸Šä½å…³ç³»ï¼šå½“å‰æ¦‚å¿µæ˜¯ç›¸å…³æ¦‚å¿µçš„å­ç±»
                    hierarchy['broader'].append(related_name)
                elif rel_type in ['RN', 'isa', 'child']:
                    # ä¸‹ä½å…³ç³»ï¼šå½“å‰æ¦‚å¿µæ˜¯ç›¸å…³æ¦‚å¿µçš„çˆ¶ç±»
                    hierarchy['narrower'].append(related_name)
                elif rel_type in ['RT', 'related_to']:
                    # ç›¸å…³å…³ç³»ï¼šæ¦‚å¿µä¹‹é—´å­˜åœ¨å…³è”ä½†éå±‚æ¬¡å…³ç³»
                    hierarchy['related'].append(related_name)
            
            return hierarchy
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºå­—å…¸
            logger.error(f"Error getting concept hierarchy for '{entity}': {e}")
            return {}
    
    def enhanced_entity_linking_method(self, entities, context, question_types):
        """å¢å¼ºçš„å®ä½“é“¾æ¥"""
        # æ£€æŸ¥å¤šç­–ç•¥é“¾æ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
            return {}  # ç¦ç”¨æ—¶è¿”å›ç©ºå­—å…¸
            
        try:
            # æ„å»ºUMLSçŸ¥è¯†å›¾è°±
            umls_kg = []
            # ä¸ºæ¯ä¸ªå®ä½“æœç´¢UMLSæ¦‚å¿µ
            for entity in entities:
                concepts = self.umls_api.search_concepts(entity)
                # å¦‚æœæœç´¢æˆåŠŸä¸”æœ‰ç»“æœ
                if concepts and 'results' in concepts:
                    # æå–å‰5ä¸ªæ¦‚å¿µçš„åç§°åŠ å…¥çŸ¥è¯†å›¾è°±
                    umls_kg.extend([concept['name'] for concept in concepts['results'][:5]])
            
            # è°ƒç”¨å¢å¼ºå®ä½“é“¾æ¥å™¨è¿›è¡Œå¤šç­–ç•¥é“¾æ¥
            linking_results = self.enhanced_entity_linking.multi_strategy_linking(
                entities,      # å¾…é“¾æ¥çš„å®ä½“åˆ—è¡¨
                context,       # ä¸Šä¸‹æ–‡ä¿¡æ¯
                umls_kg        # UMLSçŸ¥è¯†å›¾è°±
            )
            
            return linking_results
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºå­—å…¸
            logger.error(f"Error in enhanced entity linking: {e}")
            return {}
    
    def adaptive_knowledge_selection(self, question_types, entities):
        """è‡ªé€‚åº”çŸ¥è¯†é€‰æ‹©"""
        # æ£€æŸ¥è‡ªé€‚åº”UMLSåŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not ABLATION_CONFIG['USE_ADAPTIVE_UMLS']:
            return []  # ç¦ç”¨æ—¶è¿”å›ç©ºåˆ—è¡¨
            
        try:
            # åˆå§‹åŒ–é€‰ä¸­çš„çŸ¥è¯†åˆ—è¡¨
            selected_knowledge = []
            
            # éå†æ‰€æœ‰é—®é¢˜ç±»å‹
            for question_type in question_types:
                # ä¸ºæ¯ç§é—®é¢˜ç±»å‹é€‰æ‹©ç›¸å…³çš„UMLSçŸ¥è¯†
                knowledge = self.adaptive_umls_selector.select_relevant_umls_knowledge(
                    question_type,  # é—®é¢˜ç±»å‹ï¼ˆå¦‚'treatment', 'diagnosis'ï¼‰
                    entities        # ç›¸å…³å®ä½“åˆ—è¡¨
                )
                # å°†é€‰ä¸­çš„çŸ¥è¯†æ‰©å±•åˆ°æ€»åˆ—è¡¨ä¸­
                selected_knowledge.extend(knowledge)
            
            return selected_knowledge
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
            logger.error(f"Error in adaptive knowledge selection: {e}")
            return []

# ========================= åŒ»å­¦æ¨ç†è§„åˆ™æ¨¡å— =========================

class MedicalReasoningRules:
    def __init__(self, umls_normalizer=None):
        """åˆå§‹åŒ–åŒ»å­¦æ¨ç†è§„åˆ™"""
        self.umls_normalizer = umls_normalizer
        self.kg_guided_reasoning = None
        
        self.rules = {
            'transitivity': {
                'causes': ['causes', 'leads_to', 'results_in', 'induces'],
                'treats': ['treats', 'alleviates', 'improves', 'cures'],
                'part_of': ['part_of', 'located_in', 'component_of'],
                'precedes': ['precedes', 'before', 'prior_to'],
                'prevents': ['prevents', 'reduces_risk_of', 'protects_against']
            },
            'inverse_relations': {
                'causes': 'caused_by',
                'treats': 'treated_by',
                'part_of': 'contains',
                'precedes': 'follows',
                'prevents': 'prevented_by'
            },
            'semantic_implications': {
                'symptom_of': 'has_symptom',
                'risk_factor_for': 'has_risk_factor',
                'complication_of': 'has_complication'
            },
            'medical_hierarchies': {
                'disease_subtype': 'is_type_of',
                'anatomical_part': 'part_of_anatomy',
                'drug_class': 'belongs_to_class'
            }
        }
        
        self.confidence_weights = {
            'direct': 1.0,
            'transitive_1hop': 0.8,
            'transitive_2hop': 0.6,
            'inverse': 0.9,
            'semantic': 0.7,
            'hierarchical': 0.75
        }
    
    def initialize_kg_guided_reasoning(self, kg, llm):
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±å¼•å¯¼æ¨ç†"""
        if ABLATION_CONFIG['USE_KG_GUIDED_REASONING']:
            self.kg_guided_reasoning = KGGuidedReasoningEngine(kg, llm)
        else:
            logger.info("ğŸ”¬ KG-guided reasoning disabled in ablation study")
    
    def apply_reasoning_rules(self, knowledge_triples, max_hops=2):
        """
        åº”ç”¨åŒ»å­¦æ¨ç†è§„åˆ™æ‰©å±•çŸ¥è¯†
        
        è¿™ä¸ªæ–¹æ³•é€šè¿‡åº”ç”¨å¤šç§é€»è¾‘æ¨ç†è§„åˆ™æ¥æ‰©å±•åŸå§‹çš„åŒ»å­¦çŸ¥è¯†ä¸‰å…ƒç»„ï¼Œ
        ä»æœ‰é™çš„äº‹å®ä¸­æ¨å¯¼å‡ºæ›´å¤šéšå«çš„åŒ»å­¦çŸ¥è¯†å…³ç³»
        
        å‚æ•°:
            knowledge_triples (list): åŸå§‹çŸ¥è¯†ä¸‰å…ƒç»„åˆ—è¡¨
                                    æ ¼å¼: [['entity1', 'relation', 'entity2'], ...]
            max_hops (int): ä¼ é€’æ¨ç†çš„æœ€å¤§è·³æ•°ï¼Œé»˜è®¤ä¸º2
        
        è¿”å›:
            list: æ‰©å±•åçš„å»é‡çŸ¥è¯†ä¸‰å…ƒç»„åˆ—è¡¨
        """
        
        # ==================== ç¬¬1æ­¥ï¼šæ¶ˆèå®éªŒé…ç½®æ£€æŸ¥ ====================
        if not ABLATION_CONFIG['USE_REASONING_RULES']:
            logger.info("ğŸ”¬ Medical reasoning rules disabled in ablation study")
            return knowledge_triples
            # å¦‚æœæ¨ç†è§„åˆ™åŠŸèƒ½è¢«ç¦ç”¨ï¼ˆæ¶ˆèå®éªŒæ§åˆ¶ï¼‰ï¼Œç›´æ¥è¿”å›åŸå§‹ä¸‰å…ƒç»„
            # è¿™å…è®¸ç ”ç©¶äººå‘˜æ¯”è¾ƒæœ‰æ— æ¨ç†è§„åˆ™çš„ç³»ç»Ÿæ€§èƒ½å·®å¼‚
            # æ¶ˆèå®éªŒæ˜¯AIç ”ç©¶ä¸­å¸¸ç”¨çš„æ–¹æ³•ï¼Œç”¨äºéªŒè¯å„æ¨¡å—çš„è´¡çŒ®
        
        # ==================== ç¬¬2æ­¥ï¼šåˆå§‹åŒ–æ•°æ®ç»“æ„ ====================
        expanded_triples = knowledge_triples.copy()
        # åˆ›å»ºåŸå§‹ä¸‰å…ƒç»„çš„å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        # åç»­æ‰€æœ‰æ¨ç†å¾—åˆ°çš„æ–°ä¸‰å…ƒç»„éƒ½ä¼šæ·»åŠ åˆ°è¿™ä¸ªåˆ—è¡¨ä¸­
        
        reasoning_log = []
        # åˆå§‹åŒ–æ¨ç†æ—¥å¿—ï¼Œè®°å½•æ¯ç§æ¨ç†ç±»å‹ç”Ÿæˆçš„ä¸‰å…ƒç»„æ•°é‡
        # ç”¨äºæ€§èƒ½ç›‘æ§å’Œè°ƒè¯•åˆ†æ
        
        # ==================== ç¬¬3æ­¥ï¼šä¼ é€’æ€§æ¨ç† ====================
        transitive_triples = self._apply_transitivity(knowledge_triples, max_hops)
        """
        ä¼ é€’æ€§æ¨ç†ç¤ºä¾‹ï¼š
        åŸå§‹äº‹å®:
        - ['é˜¿å°”èŒ¨æµ·é»˜ç—…', 'causes', 'è®°å¿†ä¸§å¤±']
        - ['è®°å¿†ä¸§å¤±', 'leads_to', 'è®¤çŸ¥éšœç¢']
        
        æ¨ç†ç»“æœ:
        - ['é˜¿å°”èŒ¨æµ·é»˜ç—…', 'transitively_causes', 'è®¤çŸ¥éšœç¢']
        
        åŒ»å­¦æ„ä¹‰: å¦‚æœAå¯¼è‡´Bï¼ŒBå¯¼è‡´Cï¼Œé‚£ä¹ˆAé—´æ¥å¯¼è‡´C
        è¿™åœ¨åŒ»å­¦ä¸­å¾ˆå¸¸è§ï¼Œç–¾ç—…â†’ç—‡çŠ¶â†’åŠŸèƒ½éšœç¢çš„é“¾æ¡
        """
        
        expanded_triples.extend(transitive_triples)
        # å°†ä¼ é€’æ€§æ¨ç†çš„ç»“æœæ·»åŠ åˆ°æ‰©å±•ä¸‰å…ƒç»„åˆ—è¡¨
        reasoning_log.extend([('transitivity', len(transitive_triples))])
        # è®°å½•ä¼ é€’æ€§æ¨ç†ç”Ÿæˆçš„ä¸‰å…ƒç»„æ•°é‡
        
        # ==================== ç¬¬4æ­¥ï¼šé€†å…³ç³»æ¨ç† ====================
        inverse_triples = self._apply_inverse_relations(knowledge_triples)
        """
        é€†å…³ç³»æ¨ç†ç¤ºä¾‹ï¼š
        åŸå§‹äº‹å®:
        - ['é˜¿å¸åŒ¹æ—', 'treats', 'å¤´ç—›']
        
        æ¨ç†ç»“æœ:
        - ['å¤´ç—›', 'treated_by', 'é˜¿å¸åŒ¹æ—']
        
        åŒ»å­¦æ„ä¹‰: è®¸å¤šåŒ»å­¦å…³ç³»æ˜¯åŒå‘çš„
        å¦‚æœè¯ç‰©Aæ²»ç–—ç–¾ç—…Bï¼Œé‚£ä¹ˆç–¾ç—…Bè¢«è¯ç‰©Aæ²»ç–—
        è¿™å¢åŠ äº†çŸ¥è¯†å›¾è°±çš„è¿é€šæ€§å’ŒæŸ¥è¯¢çµæ´»æ€§
        """
        
        expanded_triples.extend(inverse_triples)
        reasoning_log.extend([('inverse', len(inverse_triples))])
        
        # ==================== ç¬¬5æ­¥ï¼šè¯­ä¹‰è•´æ¶µæ¨ç† ====================
        semantic_triples = self._apply_semantic_implications(knowledge_triples)
        """
        è¯­ä¹‰è•´æ¶µæ¨ç†ç¤ºä¾‹ï¼š
        åŸå§‹äº‹å®:
        - ['ç³–å°¿ç—…', 'symptom_of', 'é«˜è¡€ç³–']
        
        æ¨ç†ç»“æœ:
        - ['é«˜è¡€ç³–', 'has_symptom', 'ç³–å°¿ç—…']
        
        åŒ»å­¦æ„ä¹‰: æŸäº›å…³ç³»åœ¨è¯­ä¹‰ä¸Šäº’ç›¸è•´æ¶µ
        å¦‚æœXæ˜¯Yçš„ç—‡çŠ¶ï¼Œé‚£ä¹ˆYå…·æœ‰ç—‡çŠ¶X
        è¿™åŸºäºåŒ»å­¦æœ¯è¯­çš„è¯­ä¹‰ç»“æ„è¿›è¡Œæ¨ç†
        """
        
        expanded_triples.extend(semantic_triples)
        reasoning_log.extend([('semantic', len(semantic_triples))])
        
        # ==================== ç¬¬6æ­¥ï¼šå±‚æ¬¡åŒ–æ¨ç† ====================
        hierarchical_triples = self._apply_hierarchical_reasoning(knowledge_triples)
        """
        å±‚æ¬¡åŒ–æ¨ç†ç¤ºä¾‹ï¼š
        åŸå§‹äº‹å®:
        - ['å¿ƒè‚Œæ¢—æ­»', 'is_type_of', 'å¿ƒè„ç—…']
        - UMLSå±‚æ¬¡ç»“æ„æ˜¾ç¤º: å¿ƒè‚Œæ¢—æ­» â†’ å† å¿ƒç—… â†’ å¿ƒè„ç—…
        
        æ¨ç†ç»“æœ:
        - ['å¿ƒè‚Œæ¢—æ­»', 'is_subtype_of', 'å† å¿ƒç—…']
        - ['å† å¿ƒç—…', 'is_subtype_of', 'å¿ƒè„ç—…']
        
        åŒ»å­¦æ„ä¹‰: åˆ©ç”¨UMLSç­‰åŒ»å­¦æœ¬ä½“çš„å±‚æ¬¡ç»“æ„
        æ¨å¯¼ç–¾ç—…ã€è§£å‰–ç»“æ„ã€è¯ç‰©ç­‰çš„ä¸Šä¸‹ä½å…³ç³»
        è¿™ä¸°å¯Œäº†æ¦‚å¿µé—´çš„åˆ†ç±»å­¦å…³ç³»
        """
        
        expanded_triples.extend(hierarchical_triples)
        reasoning_log.extend([('hierarchical', len(hierarchical_triples))])
        
        # ==================== ç¬¬7æ­¥ï¼šå»é‡å¤„ç† ====================
        unique_triples = self._deduplicate_triples(expanded_triples)
        """
        å»é‡å¤„ç†çš„å¿…è¦æ€§ï¼š
        
        é—®é¢˜: ä¸åŒæ¨ç†è§„åˆ™å¯èƒ½ç”Ÿæˆç›¸åŒçš„ä¸‰å…ƒç»„
        ä¾‹å¦‚:
        - ä¼ é€’æ€§æ¨ç†: ['ç–¾ç—…A', 'causes', 'ç—‡çŠ¶B']
        - é€†å…³ç³»æ¨ç†: ['ç—‡çŠ¶B', 'caused_by', 'ç–¾ç—…A'] â†’ ['ç–¾ç—…A', 'causes', 'ç—‡çŠ¶B']
        
        è§£å†³æ–¹æ¡ˆ: æ ‡å‡†åŒ–å¹¶å»é‡
        - è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        - ä½¿ç”¨é›†åˆæ•°æ®ç»“æ„å¿«é€Ÿå»é‡
        - ä¿ç•™åŸå§‹æ ¼å¼çš„ä¸‰å…ƒç»„
        """
        
        # ==================== ç¬¬8æ­¥ï¼šæ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§ ====================
        logger.info(f"æ¨ç†æ‰©å±•: {reasoning_log}")
        logger.info(f"åŸå§‹ä¸‰å…ƒç»„: {len(knowledge_triples)}, æ‰©å±•å: {len(unique_triples)}")
        """
        æ—¥å¿—è¾“å‡ºç¤ºä¾‹:
        æ¨ç†æ‰©å±•: [('transitivity', 5), ('inverse', 12), ('semantic', 8), ('hierarchical', 15)]
        åŸå§‹ä¸‰å…ƒç»„: 25, æ‰©å±•å: 58
        
        ä¿¡æ¯è§£è¯»:
        - ä¼ é€’æ€§æ¨ç†ç”Ÿæˆäº†5ä¸ªæ–°ä¸‰å…ƒç»„
        - é€†å…³ç³»æ¨ç†ç”Ÿæˆäº†12ä¸ªæ–°ä¸‰å…ƒç»„
        - è¯­ä¹‰è•´æ¶µæ¨ç†ç”Ÿæˆäº†8ä¸ªæ–°ä¸‰å…ƒç»„
        - å±‚æ¬¡åŒ–æ¨ç†ç”Ÿæˆäº†15ä¸ªæ–°ä¸‰å…ƒç»„
        - æ€»å…±ä»25ä¸ªåŸå§‹ä¸‰å…ƒç»„æ‰©å±•åˆ°58ä¸ªï¼ˆå»é‡åï¼‰
        
        æ€§èƒ½æŒ‡æ ‡:
        - æ‰©å±•å€æ•°: 58/25 = 2.32å€
        - å„æ¨ç†ç±»å‹çš„è´¡çŒ®æ¯”ä¾‹å¯ç”¨äºä¼˜åŒ–
        """
        
        return unique_triples
        # è¿”å›æ‰©å±•å’Œå»é‡åçš„çŸ¥è¯†ä¸‰å…ƒç»„

    
    def _apply_transitivity(self, triples, max_hops):
        """åº”ç”¨ä¼ é€’æ€§æ¨ç†"""
        transitive_triples = []
        
        relation_graph = {}
        for triple in triples:
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                if head not in relation_graph:
                    relation_graph[head] = []
                relation_graph[head].append((relation, tail))
        
        for rule_type, relation_variants in self.rules['transitivity'].items():
            transitive_triples.extend(
                self._find_transitive_paths(relation_graph, relation_variants, max_hops)
            )
        
        return transitive_triples
    
    def _find_transitive_paths(self, graph, relation_variants, max_hops):
        """æŸ¥æ‰¾ä¼ é€’æ€§è·¯å¾„"""
        paths = []
        
        for start_entity in graph:
            for hop in range(1, max_hops + 1):
                paths.extend(
                    self._dfs_transitive_search(graph, start_entity, relation_variants, hop, [])
                )
        
        return paths
    
    def _dfs_transitive_search(self, graph, current_entity, target_relations, remaining_hops, path):
        """æ·±åº¦ä¼˜å…ˆæœç´¢ä¼ é€’æ€§è·¯å¾„"""
        if remaining_hops == 0:
            return []
        
        results = []
        
        if current_entity in graph:
            for relation, next_entity in graph[current_entity]:
                if any(target_rel in relation.lower() for target_rel in target_relations):
                    new_path = path + [(current_entity, relation, next_entity)]
                    
                    if remaining_hops == 1:
                        if len(new_path) >= 2:
                            start = new_path[0][0]
                            end = new_path[-1][2]
                            inferred_relation = f"transitively_{target_relations[0]}"
                            results.append([start, inferred_relation, end])
                    else:
                        results.extend(
                            self._dfs_transitive_search(
                                graph, next_entity, target_relations, 
                                remaining_hops - 1, new_path
                            )
                        )
        
        return results
    
    def _apply_inverse_relations(self, triples):
        """åº”ç”¨é€†å…³ç³»æ¨ç†"""
        inverse_triples = []
        
        for triple in triples:
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                for forward_rel, inverse_rel in self.rules['inverse_relations'].items():
                    if forward_rel in relation.lower():
                        inverse_triples.append([tail, inverse_rel, head])
        
        return inverse_triples
    
    def _apply_semantic_implications(self, triples):
        """åº”ç”¨è¯­ä¹‰è•´æ¶µæ¨ç†"""
        semantic_triples = []
        
        for triple in triples:
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                for source_rel, target_rel in self.rules['semantic_implications'].items():
                    if source_rel in relation.lower():
                        semantic_triples.append([tail, target_rel, head])
        
        return semantic_triples
    
    def _apply_hierarchical_reasoning(self, triples):
        """åº”ç”¨å±‚æ¬¡æ¨ç†"""
        hierarchical_triples = []
        
        if not self.umls_normalizer:
            return hierarchical_triples
        
        entities = set()
        for triple in triples:
            if len(triple) >= 3:
                entities.add(triple[0])
                entities.add(triple[2])
        
        for entity in entities:
            try:
                hierarchy = self.umls_normalizer.get_concept_hierarchy(entity)
                
                for broader_concept in hierarchy.get('broader', []):
                    hierarchical_triples.append([entity, 'is_subtype_of', broader_concept])
                
                for narrower_concept in hierarchy.get('narrower', []):
                    hierarchical_triples.append([narrower_concept, 'is_subtype_of', entity])
                
            except Exception as e:
                logger.error(f"Error in hierarchical reasoning for {entity}: {e}")
        
        return hierarchical_triples
    
    def _deduplicate_triples(self, triples):
        """å»é‡ä¸‰å…ƒç»„"""
        seen = set()
        unique_triples = []
        
        for triple in triples:
            if len(triple) >= 3:
                triple_key = (triple[0].lower(), triple[1].lower(), triple[2].lower())
                if triple_key not in seen:
                    seen.add(triple_key)
                    unique_triples.append(triple)
        
        return unique_triples

# ========================= å¤šè·³æ¨ç†æ¨¡å— =========================

class MultiHopReasoning:
    def __init__(self, max_hops=3, umls_normalizer=None):
        """åˆå§‹åŒ–å¤šè·³æ¨ç†å™¨"""
        self.max_hops = max_hops
        self.umls_normalizer = umls_normalizer
        self.reasoning_chains = []
        self.evidence_weights = {
            'direct': 1.0,
            'one_hop': 0.8,
            'two_hop': 0.6,
            'three_hop': 0.4
        }
        
        self.optimized_multi_hop = OptimizedMultiHopReasoning(kg=[], path_ranker=PathRanker())
    
    def perform_multi_hop_reasoning(self, question, kg_subgraph):
        """æ‰§è¡Œå¤šè·³æ¨ç†"""
        if not ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
            return self._basic_multi_hop_reasoning(question, kg_subgraph)
            
        self.optimized_multi_hop.kg = kg_subgraph
        
        question_entities = self._extract_question_entities(question)
        
        if self.umls_normalizer and ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            normalized_entities = self.umls_normalizer.normalize_medical_terms(question_entities)
            question_entities.extend(normalized_entities)
        
        if len(question_entities) >= 2:
            start_entities = question_entities[:1]
            target_entities = question_entities[1:]
            
            intelligent_paths = self.optimized_multi_hop.intelligent_path_selection(
                start_entities, target_entities, self.max_hops
            )
            
            reasoning_chains = []
            for path in intelligent_paths[:5]:
                chain = self._build_reasoning_chain_from_path(path, kg_subgraph)
                if chain:
                    reasoning_chains.append(chain)
        else:
            reasoning_chains = []
            for entity in question_entities:
                chain = self._build_reasoning_chain(entity, kg_subgraph, self.max_hops)
                if chain:
                    reasoning_chains.append(chain)
        
        final_answer = self._fuse_reasoning_chains(reasoning_chains, question)
        return final_answer
    
    def _basic_multi_hop_reasoning(self, question, kg_subgraph):
        """åŸºç¡€ç‰ˆæœ¬çš„å¤šè·³æ¨ç†"""
        logger.info("ğŸ”¬ Using basic multi-hop reasoning (optimized version disabled)")
        
        question_entities = self._extract_question_entities(question)
        
        if len(question_entities) >= 2:
            reasoning_summary = f"Basic reasoning: Found entities {question_entities[:2]} in knowledge graph."
            
            direct_connections = []
            for triple in kg_subgraph:
                if len(triple) >= 3:
                    if (triple[0] in question_entities and triple[2] in question_entities) or \
                       (triple[2] in question_entities and triple[0] in question_entities):
                        direct_connections.append(f"{triple[0]} -> {triple[1]} -> {triple[2]}")
            
            if direct_connections:
                reasoning_summary += f" Found {len(direct_connections)} direct connections."
            
            return reasoning_summary
        else:
            return "Insufficient entities for multi-hop reasoning."
    
    def _build_reasoning_chain_from_path(self, path, kg_subgraph):
        """ä»è·¯å¾„æ„å»ºæ¨ç†é“¾"""
        chain = {
            'path': path,
            'confidence': self._calculate_path_confidence(path),
            'reasoning_steps': []
        }
        
        for i, step in enumerate(path):
            if len(step) >= 3:
                reasoning_step = {
                    'step': i + 1,
                    'from': step[0],
                    'relation': step[1],
                    'to': step[2],
                    'explanation': f"Based on medical knowledge, {step[0]} {step[1]} {step[2]}"
                }
                chain['reasoning_steps'].append(reasoning_step)
        
        return chain
    
    def _calculate_path_confidence(self, path):
        """è®¡ç®—è·¯å¾„ç½®ä¿¡åº¦"""
        if not path:
            return 0.0
        
        total_confidence = 1.0
        for step in path:
            if len(step) >= 2:
                relation_weight = self._calculate_relation_weight(step[1])
                total_confidence *= relation_weight
        
        length_penalty = 0.9 ** len(path)
        return total_confidence * length_penalty
    
    def _extract_question_entities(self, question):
        """ä»é—®é¢˜ä¸­æå–å®ä½“"""
        entities = []
        
        medical_terms = [
            'alzheimer', 'dementia', 'brain', 'memory', 'cognitive',
            'treatment', 'medication', 'symptom', 'diagnosis', 'disease',
            'protein', 'amyloid', 'tau', 'hippocampus', 'cortex'
        ]
        
        question_lower = question.lower()
        for term in medical_terms:
            if term in question_lower:
                entities.append(term)
        
        words = question.split()
        for word in words:
            if word[0].isupper() and len(word) > 3:
                entities.append(word)
        
        return list(set(entities))
    
    def _build_reasoning_chain(self, start_entity, kg_subgraph, max_hops):
        """æ„å»ºä»èµ·å§‹å®ä½“å¼€å§‹çš„æ¨ç†é“¾"""
        chain = {
            'start_entity': start_entity,
            'paths': [],
            'confidence': 0.0
        }
        
        graph = self._build_graph_from_subgraph(kg_subgraph)
        
        for hop in range(1, max_hops + 1):
            hop_paths = self._find_paths_at_hop(graph, start_entity, hop)
            chain['paths'].extend(hop_paths)
        
        chain['confidence'] = self._calculate_chain_confidence(chain['paths'])
        
        return chain
    
    def _build_graph_from_subgraph(self, kg_subgraph):
        """ä»å­å›¾æ„å»ºå›¾ç»“æ„"""
        graph = {}
        
        for triple in kg_subgraph:
            if len(triple) >= 3:
                head, relation, tail = triple[0], triple[1], triple[2]
                
                if head not in graph:
                    graph[head] = []
                graph[head].append({
                    'relation': relation,
                    'target': tail,
                    'weight': self._calculate_relation_weight(relation)
                })
        
        return graph
    
    def _find_paths_at_hop(self, graph, start_entity, target_hop):
        """æŸ¥æ‰¾æŒ‡å®šè·³æ•°çš„è·¯å¾„"""
        def dfs_path_search(current_entity, current_hop, path, visited):
            if current_hop == target_hop:
                return [path]
            
            if current_entity not in graph or current_entity in visited:
                return []
            
            visited.add(current_entity)
            paths = []
            
            for edge in graph[current_entity]:
                new_path = path + [(current_entity, edge['relation'], edge['target'])]
                paths.extend(
                    dfs_path_search(edge['target'], current_hop + 1, new_path, visited.copy())
                )
            
            return paths
        
        return dfs_path_search(start_entity, 0, [], set())
    
    def _calculate_relation_weight(self, relation):
        """è®¡ç®—å…³ç³»æƒé‡"""
        relation_lower = relation.lower().replace('_', ' ')
        
        weights = {
            'causes': 3.0, 'treats': 2.8, 'prevents': 2.5,
            'associated_with': 2.2, 'diagnoses': 2.0,
            'symptom_of': 1.8, 'risk_factor': 1.6,
            'interacts_with': 1.4, 'located_in': 1.2,
            'part_of': 1.0, 'related_to': 0.8
        }
        
        for key, weight in weights.items():
            if key in relation_lower:
                return weight
        
        return 1.0
    
    def _calculate_chain_confidence(self, paths):
        """è®¡ç®—æ¨ç†é“¾çš„ç½®ä¿¡åº¦"""
        if not paths:
            return 0.0
        
        total_confidence = 0.0
        for path in paths:
            path_confidence = 1.0
            hop_weight = self.evidence_weights.get(f"{len(path)}_hop", 0.2)
            
            for step in path:
                relation_weight = self._calculate_relation_weight(step[1])
                path_confidence *= relation_weight
            
            path_confidence *= hop_weight
            total_confidence += path_confidence
        
        return min(total_confidence / len(paths), 1.0)
    
    def _fuse_reasoning_chains(self, reasoning_chains, question):
        """èåˆæ¨ç†ç»“æœ"""
        if not reasoning_chains:
            return "Unable to find sufficient reasoning paths."
        
        reasoning_chains.sort(key=lambda x: x['confidence'], reverse=True)
        
        answer_components = []
        total_confidence = 0.0
        
        for chain in reasoning_chains[:3]:
            if chain['confidence'] > 0.1:
                chain_summary = self._summarize_chain(chain)
                answer_components.append(chain_summary)
                total_confidence += chain['confidence']
        
        if answer_components:
            final_answer = f"Based on multi-hop reasoning (confidence: {total_confidence:.2f}):\n"
            final_answer += "\n".join(answer_components)
            return final_answer
        else:
            return "Insufficient evidence for multi-hop reasoning."
    
    def _summarize_chain(self, chain):
        """æ€»ç»“æ¨ç†é“¾"""
        summary = f"From {chain['start_entity']}:"
        
        best_paths = sorted(chain['paths'], 
                           key=lambda p: self._calculate_path_score(p), 
                           reverse=True)[:2]
        
        for i, path in enumerate(best_paths):
            path_str = " -> ".join([f"{step[0]} ({step[1]}) {step[2]}" for step in path])
            summary += f"\nPath {i+1}: {path_str}"
        
        return summary
    
    def _calculate_path_score(self, path):
        """è®¡ç®—è·¯å¾„å¾—åˆ†"""
        score = 1.0
        for step in path:
            score *= self._calculate_relation_weight(step[1])
        return score / len(path)

# ========================= åŒ»å­¦é¢†åŸŸçŸ¥è¯†åº“ =========================

MEDICAL_ABBREVIATIONS = {
    'AD': 'Alzheimer Disease',
    'AIDS': 'Acquired Immunodeficiency Syndrome',
    'BP': 'Blood Pressure',
    'CAD': 'Coronary Artery Disease',
    'CHF': 'Congestive Heart Failure',
    'COPD': 'Chronic Obstructive Pulmonary Disease',
    'CVA': 'Cerebrovascular Accident',
    'DM': 'Diabetes Mellitus',
    'HTN': 'Hypertension',
    'MI': 'Myocardial Infarction',
    'MS': 'Multiple Sclerosis',
    'PD': "Parkinson's Disease",
    'PTSD': 'Post Traumatic Stress Disorder',
    'RA': 'Rheumatoid Arthritis',
    'TB': 'Tuberculosis',
    'UTI': 'Urinary Tract Infection',
    'HIV': 'Human Immunodeficiency Virus',
    'CT': 'Computed Tomography',
    'MRI': 'Magnetic Resonance Imaging',
    'EEG': 'Electroencephalogram',
    'ECG': 'Electrocardiogram',
    'CSF': 'Cerebrospinal Fluid',
    'MCI': 'Mild Cognitive Impairment',
    'NFT': 'Neurofibrillary Tangles',
    'APP': 'Amyloid Precursor Protein',
    'APOE': 'Apolipoprotein E',
    'AChE': 'Acetylcholinesterase',
    'MMSE': 'Mini Mental State Examination'
}

MEDICAL_SYNONYMS = {
    'alzheimer': ['alzheimer disease', 'dementia', 'alzheimers', 'ad', 'alzheimer\'s disease'],
    'heart attack': ['myocardial infarction', 'mi', 'cardiac arrest'],
    'stroke': ['cerebrovascular accident', 'cva', 'brain attack'],
    'high blood pressure': ['hypertension', 'htn', 'elevated bp'],
    'diabetes': ['diabetes mellitus', 'dm', 'diabetic'],
    'cancer': ['carcinoma', 'tumor', 'malignancy', 'neoplasm'],
    'infection': ['infectious disease', 'sepsis', 'inflammation'],
    'treatment': ['therapy', 'medication', 'drug', 'medicine'],
    'symptom': ['sign', 'manifestation', 'presentation'],
    'diagnosis': ['diagnostic', 'identification', 'detection'],
    'cognitive decline': ['cognitive impairment', 'dementia', 'memory loss'],
    'memory problems': ['memory deficit', 'memory loss', 'amnesia'],
    'confusion': ['disorientation', 'bewilderment', 'perplexity'],
    'brain imaging': ['neuroimaging', 'brain scan', 'ct scan', 'mri scan']
}

RELATION_IMPORTANCE_WEIGHTS = {
    'cause': 3.0, 'causes': 3.0, 'caused_by': 3.0,
    'treat': 2.8, 'treats': 2.8, 'treatment': 2.8,
    'prevent': 2.5, 'prevents': 2.5, 'prevention': 2.5,
    'associate': 2.2, 'associates': 2.2, 'associated_with': 2.2,
    'diagnose': 2.0, 'diagnosis': 2.0, 'diagnostic': 2.0,
    'symptom': 1.8, 'symptoms': 1.8, 'has_symptom': 1.8,
    'risk_factor': 1.6, 'risk': 1.6,
    'interact': 1.4, 'interacts': 1.4, 'interaction': 1.4,
    'located_in': 1.2, 'location': 1.2,
    'part_of': 1.0, 'is_a': 1.0,
    'related': 0.8, 'similar': 0.6
}

QUESTION_TYPE_KEYWORDS = {
    'definition': ['what is', 'define', 'definition', 'meaning'],
    'causation': ['cause', 'causes', 'reason', 'why', 'due to', 'because'],
    'treatment': ['treat', 'treatment', 'therapy', 'cure', 'medication', 'drug'],
    'symptom': ['symptom', 'sign', 'present', 'manifestation'],
    'diagnosis': ['diagnose', 'diagnosis', 'test', 'examination'],
    'prevention': ['prevent', 'prevention', 'avoid', 'reduce risk'],
    'exception': ['except', 'not', 'exclude', 'excluding', 'other than']
}

NEGATION_WORDS = ['not', 'except', 'excluding', 'other than', 'rather than', 'instead of', 'exclude']

dataset2processor = {
    'medmcqa': medmcqaZeroshotsProcessor,
    'medqa':medqaZeroshotsProcessor,
    'mmlu': mmluZeroshotsProcessor,
    'qa4mre':qa4mreZeroshotsProcessor
}
datasets = ['medqa', 'medmcqa', 'mmlu', 'qa4mre']



# ========================= æ€§èƒ½ä¼˜åŒ–å‡½æ•° =========================

def cleanup_resources(sample_count):
    """æ€§èƒ½ä¼˜åŒ–ï¼šå®šæœŸæ¸…ç†ç³»ç»Ÿèµ„æº"""
    try:
        collected = gc.collect()
        
        if hasattr(umls_normalizer, 'umls_api') and hasattr(umls_normalizer.umls_api, 'cache'):
            cache_size_before = len(umls_normalizer.umls_api.cache)
            if cache_size_before > MAX_CACHE_SIZE:
                cache_items = list(umls_normalizer.umls_api.cache.items())
                umls_normalizer.umls_api.cache = dict(cache_items[-KEEP_CACHE_SIZE:])
                logger.info(f"ğŸ§¹ Cleaned UMLS cache: {cache_size_before} â†’ {len(umls_normalizer.umls_api.cache)}")
        
        if hasattr(umls_normalizer, 'local_cache'):
            local_cache_size_before = len(umls_normalizer.local_cache)
            if local_cache_size_before > MAX_CACHE_SIZE:
                cache_items = list(umls_normalizer.local_cache.items())
                umls_normalizer.local_cache = dict(cache_items[-KEEP_CACHE_SIZE:])
                logger.info(f"ğŸ§¹ Cleaned local cache: {local_cache_size_before} â†’ {len(umls_normalizer.local_cache)}")
        
        if hasattr(umls_normalizer, 'umls_api') and hasattr(umls_normalizer.umls_api, 'failed_cuis'):
            failed_cuis_size_before = len(umls_normalizer.umls_api.failed_cuis)
            if failed_cuis_size_before > MAX_FAILED_CUIS:
                umls_normalizer.umls_api.failed_cuis.clear()
                logger.info(f"ğŸ§¹ Cleaned failed CUI cache: {failed_cuis_size_before} â†’ 0")
        
        if hasattr(multi_hop_reasoner, 'optimized_multi_hop') and hasattr(multi_hop_reasoner.optimized_multi_hop, 'reasoning_cache'):
            reasoning_cache_size_before = len(multi_hop_reasoner.optimized_multi_hop.reasoning_cache)
            if reasoning_cache_size_before > 500:
                multi_hop_reasoner.optimized_multi_hop.reasoning_cache.clear()
                logger.info(f"ğŸ§¹ Cleaned reasoning cache: {reasoning_cache_size_before} â†’ 0")
        
        logger.info(f"âœ… Resource cleanup completed at sample {sample_count} (collected {collected} objects)")
        
    except Exception as e:
        logger.error(f"âŒ Error during resource cleanup: {e}")

# ========================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =========================

def expand_medical_abbreviations(text):
    """æ‰©å±•åŒ»å­¦ç¼©å†™è¯"""
    expanded_text = text
    for abbr, full_form in MEDICAL_ABBREVIATIONS.items():
        pattern = r'\b' + re.escape(abbr) + r'\b'
        expanded_text = re.sub(pattern, full_form, expanded_text, flags=re.IGNORECASE)
    return expanded_text

def get_medical_synonyms(entity):
    """
    è·å–åŒ»å­¦æœ¯è¯­çš„åŒä¹‰è¯
    
    å‚æ•°:
        entity (str): è¾“å…¥çš„åŒ»å­¦æœ¯è¯­
    
    è¿”å›:
        list: åŒ…å«åŸæœ¯è¯­åŠå…¶æ‰€æœ‰åŒä¹‰è¯çš„åˆ—è¡¨
    """
    
    # ç¬¬1æ­¥ï¼šå°†è¾“å…¥çš„åŒ»å­¦æœ¯è¯­è½¬æ¢ä¸ºå°å†™ï¼Œä¾¿äºåç»­çš„å­—ç¬¦ä¸²åŒ¹é…
    # è¿™æ ·å¯ä»¥é¿å…å¤§å°å†™æ•æ„Ÿçš„é—®é¢˜
    entity_lower = entity.lower()
    
    # ç¬¬2æ­¥ï¼šåˆå§‹åŒ–åŒä¹‰è¯åˆ—è¡¨ï¼Œå°†åŸå§‹æœ¯è¯­ä½œä¸ºç¬¬ä¸€ä¸ªå…ƒç´ 
    # ç¡®ä¿å³ä½¿æ²¡æ‰¾åˆ°å…¶ä»–åŒä¹‰è¯ï¼Œä¹Ÿä¼šè¿”å›åŸæœ¯è¯­
    synonyms = [entity]
    
    # ç¬¬3æ­¥ï¼šç¬¬ä¸€æ¬¡UMLSï¼ˆç»Ÿä¸€åŒ»å­¦è¯­è¨€ç³»ç»Ÿï¼‰è§„èŒƒåŒ–å¤„ç†
    # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å¯ç”¨äº†UMLSè§„èŒƒåŒ–åŠŸèƒ½
    if ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
        try:
            # è°ƒç”¨UMLSè§„èŒƒåŒ–å™¨è·å–è¯­ä¹‰å˜ä½“
            # è¯­ä¹‰å˜ä½“æ˜¯æŒ‡æ„æ€ç›¸åŒä½†è¡¨è¾¾æ–¹å¼ä¸åŒçš„æœ¯è¯­
            umls_variants = umls_normalizer.get_semantic_variants(entity)
            
            # å°†è·å–åˆ°çš„UMLSå˜ä½“æ·»åŠ åˆ°åŒä¹‰è¯åˆ—è¡¨ä¸­
            synonyms.extend(umls_variants)
            
            # è®°å½•è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ‰¾åˆ°çš„UMLSå˜ä½“
            logger.debug(f"UMLS variants for '{entity}': {umls_variants}")
            
        except Exception as e:
            # å¦‚æœUMLSå¤„ç†å‡ºç°å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯ä½†ä¸ä¸­æ–­ç¨‹åºæ‰§è¡Œ
            logger.error(f"Error getting UMLS variants for '{entity}': {e}")
    
    # ç¬¬4æ­¥ï¼šä»é¢„å®šä¹‰çš„åŒ»å­¦åŒä¹‰è¯è¯å…¸ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹
    # éå†è¯å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹
    for key, synonym_list in MEDICAL_SYNONYMS.items():
        # æ£€æŸ¥ä¸¤ç§åŒ¹é…æƒ…å†µï¼š
        # 1. è¯å…¸çš„é”®åŒ…å«åœ¨è¾“å…¥æœ¯è¯­ä¸­ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        # 2. è¾“å…¥æœ¯è¯­åŒ…å«åœ¨è¯å…¸çš„åŒä¹‰è¯åˆ—è¡¨ä¸­ï¼ˆå®Œå…¨åŒ¹é…ï¼‰
        if key in entity_lower or entity_lower in synonym_list:
            # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œå°†å¯¹åº”çš„åŒä¹‰è¯åˆ—è¡¨æ·»åŠ åˆ°ç»“æœä¸­
            synonyms.extend(synonym_list)
    
    # ç¬¬5æ­¥ï¼šç¬¬äºŒæ¬¡UMLSè§„èŒƒåŒ–å¤„ç†
    # å¯¹å·²æ”¶é›†çš„æ‰€æœ‰åŒä¹‰è¯è¿›è¡Œè¿›ä¸€æ­¥çš„è§„èŒƒåŒ–
    if ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
        try:
            # å¯¹å½“å‰æ”¶é›†åˆ°çš„æ‰€æœ‰åŒä¹‰è¯è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
            # è¿™å¯èƒ½ä¼šç”Ÿæˆæ›´å¤šçš„æ ‡å‡†åŒ–å½¢å¼
            normalized_synonyms = umls_normalizer.normalize_medical_terms(synonyms)
            
            # å°†è§„èŒƒåŒ–åçš„æœ¯è¯­æ·»åŠ åˆ°åŒä¹‰è¯åˆ—è¡¨ä¸­
            synonyms.extend(normalized_synonyms)
            
        except Exception as e:
            # å¦‚æœè§„èŒƒåŒ–å¤„ç†å‡ºç°å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
            logger.error(f"Error normalizing synonyms for '{entity}': {e}")
    
    # ç¬¬6æ­¥ï¼šè¿”å›æœ€ç»ˆç»“æœ
    # ä½¿ç”¨set()å»é™¤é‡å¤é¡¹ï¼Œç„¶åè½¬æ¢å›list
    # è¿™ç¡®ä¿æ¯ä¸ªåŒä¹‰è¯åªå‡ºç°ä¸€æ¬¡
    return list(set(synonyms))

def has_negation(question):
    """æ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«å¦å®šè¯"""
    question_lower = question.lower()
    return any(neg_word in question_lower for neg_word in NEGATION_WORDS)

def calculate_relation_weight(relation_type):
    """è®¡ç®—å…³ç³»é‡è¦æ€§æƒé‡"""
    relation_lower = relation_type.lower().replace('_', ' ')
    
    if relation_lower in RELATION_IMPORTANCE_WEIGHTS:
        return RELATION_IMPORTANCE_WEIGHTS[relation_lower]
    
    for key, weight in RELATION_IMPORTANCE_WEIGHTS.items():
        if key in relation_lower or relation_lower in key:
            return weight
    
    return 1.0

def calculate_knowledge_quality_score(knowledge_items):
    """è®¡ç®—çŸ¥è¯†è´¨é‡åˆ†æ•°"""
    if not knowledge_items:
        return 0.0
    
    quality_scores = []
    
    for item in knowledge_items:
        score = 1.0
        
        if isinstance(item, list) and len(item) >= 3:
            entity, relation, objects = item[0], item[1], item[2]
            
            if len(entity) > 3:
                score += 0.5
            
            relation_weight = calculate_relation_weight(relation)
            score += relation_weight * 0.3
            
            object_count = len(objects.split(',')) if ',' in objects else 1
            score += min(object_count * 0.1, 1.0)
        
        quality_scores.append(score)
    
    return np.mean(quality_scores)

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢NumPyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def retry_on_failure(max_retries=MAX_RETRIES, wait_time=RETRY_WAIT_TIME):
    """æ·»åŠ é‡è¯•æœºåˆ¶çš„è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for retry in range(max_retries):
                try:
                    result = func(*args, **kwargs)
                    if result is not None and result != "":
                        return result
                    else:
                        logger.warning(f"Function {func.__name__} returned empty result, retrying {retry + 1}/{max_retries}...")
                        sleep(wait_time)
                except Exception as e:
                    logger.error(f"Function {func.__name__} failed attempt {retry + 1}/{max_retries}: {e}")
                    sleep(wait_time)
            
            logger.error(f"Function {func.__name__} exhausted all retries, returning empty string")
            return ""
        return wrapper
    return decorator

def validate_knowledge_triple(head, relation, tail):
    """éªŒè¯çŸ¥è¯†ä¸‰å…ƒç»„çš„è´¨é‡"""
    if pd.isna(head) or pd.isna(relation) or pd.isna(tail):
        return False
    
    head = str(head).strip() if head is not None else ""
    relation = str(relation).strip() if relation is not None else ""
    tail = str(tail).strip() if tail is not None else ""
    
    if not head or not relation or not tail:
        return False
    
    if len(head) < 2 or len(tail) < 2:
        return False
    
    noise_patterns = ['http', 'www', '@', '#', '___', '...', 'nan', 'none']
    for pattern in noise_patterns:
        if pattern in head.lower() or pattern in tail.lower():
            return False
    
    return True

def basic_entity_matching(question_kg, entity_embeddings, keyword_embeddings, question_text=""):
    """åŸºç¡€ç‰ˆæœ¬çš„å®ä½“åŒ¹é…ï¼Œç”¨äºæ¶ˆèå®éªŒ"""
    match_kg = []
    entity_embeddings_emb = pd.DataFrame(entity_embeddings["embeddings"])
    entity_confidence_scores = []
    
    # ä½¿ç”¨ç»Ÿä¸€é˜ˆå€¼é…ç½®
    keyword_match_threshold = THRESHOLDS.get_threshold('semantic_matching', 'keyword_matching')
    similarity_threshold = THRESHOLDS.get_threshold('entity_matching', 'basic_similarity')
    
    for kg_entity in question_kg:
        try:
            if kg_entity in keyword_embeddings["keywords"]:
                keyword_index = keyword_embeddings["keywords"].index(kg_entity)
            else:
                best_match_idx = None
                best_similarity = 0
                for idx, keyword in enumerate(keyword_embeddings["keywords"]):
                    if kg_entity.lower() in keyword.lower():
                        # åŸæ¥: similarity = 0.8
                        # ç°åœ¨: ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
                        similarity = keyword_match_threshold  
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_match_idx = idx
                
                if best_match_idx is None:
                    continue
                keyword_index = best_match_idx
            
            kg_entity_emb = np.array(keyword_embeddings["embeddings"][keyword_index])

            kg_entity_emb_norm = kg_entity_emb / np.linalg.norm(kg_entity_emb)
            entity_embeddings_norm = entity_embeddings_emb.values / np.linalg.norm(entity_embeddings_emb.values, axis=1, keepdims=True)
            
            cos_similarities = np.dot(entity_embeddings_norm, kg_entity_emb_norm)
            
            best_idx = np.argmax(cos_similarities)
            similarity_score = cos_similarities[best_idx]
            
            # åŸæ¥: if similarity_score >= 0.6:
            # ç°åœ¨: ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
            if similarity_score >= similarity_threshold:
                candidate_entity = entity_embeddings["entities"][best_idx]
                if candidate_entity not in match_kg:
                    match_kg.append(candidate_entity)
                    entity_confidence_scores.append(float(similarity_score))
                    logger.debug(f"Basic matched: {kg_entity} -> {candidate_entity} (score: {similarity_score:.3f})")
                
        except Exception as e:
            logger.error(f"Error in basic entity matching for {kg_entity}: {e}")
            continue
    
    return match_kg, entity_confidence_scores

def debug_entity_matching_progress(step_info, question_kg, question_text, question_types=None, 
                                 expanded_entities=None, match_kg=None, confidence_scores=None,
                                 extra_info=None):
    """ä¸“é—¨ç”¨äºå®ä½“åŒ¹é…çš„è°ƒè¯•æ‰“å°ï¼Œå¸¦æ­¥éª¤æç¤º"""
    print(f"\n{'='*80}")
    print(f"å®ä½“åŒ¹é…çš„è°ƒè¯•æ‰“å°:ğŸ” {step_info}")
    print(f"{'='*80}")
    print(f"ğŸ“ é—®é¢˜æ–‡æœ¬: {question_text}")
    print(f"ğŸ”¤ åŸå§‹å®ä½“: {question_kg}")
    
    # å¯é€‰ä¿¡æ¯ï¼Œæ ¹æ®ä¼ å…¥å‚æ•°å†³å®šæ˜¯å¦æ˜¾ç¤º
    if question_types:
        print(f"ğŸ·ï¸  é—®é¢˜ç±»å‹: {question_types}")
    
    if expanded_entities:
        print(f"ğŸ“ˆ æ‰©å±•å®ä½“: {expanded_entities[:10]}..." if len(expanded_entities) > 10 else f"ğŸ“ˆ æ‰©å±•å®ä½“: {expanded_entities}")
    
    if match_kg is not None:
        print(f"âœ… åŒ¹é…ç»“æœ: {match_kg}")
        print(f"ğŸ“Š åŒ¹é…å®ä½“æ•°é‡: {len(match_kg)}")
    
    if confidence_scores:
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {[f'{score:.3f}' for score in confidence_scores]}")
        print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")
    
    if extra_info:
        print(f"â„¹ï¸  é¢å¤–ä¿¡æ¯: {extra_info}")
    
    print(f"{'='*80}")

def enhanced_entity_matching(question_kg, entity_embeddings, keyword_embeddings, question_text=""):
    """å¢å¼ºçš„å®ä½“åŒ¹é…ï¼Œé›†æˆçœŸå®UMLS APIå’Œæ–°ä¼˜åŒ–"""
    
    # ===== ç¬¬1æ­¥ï¼šæ£€æŸ¥æ¶ˆèå®éªŒé…ç½®ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½ =====
    if not any([
        ABLATION_CONFIG['USE_HIERARCHICAL_KG'],        # æ˜¯å¦ä½¿ç”¨å±‚æ¬¡åŒ–çŸ¥è¯†å›¾è°±
        ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING'], # æ˜¯å¦ä½¿ç”¨å¤šç­–ç•¥é“¾æ¥
        ABLATION_CONFIG['USE_ADAPTIVE_UMLS'],          # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”UMLS
        ABLATION_CONFIG['USE_UMLS_NORMALIZATION'],     # æ˜¯å¦ä½¿ç”¨UMLSæ ‡å‡†åŒ–
        ABLATION_CONFIG['USE_REASONING_RULES']         # æ˜¯å¦ä½¿ç”¨æ¨ç†è§„åˆ™
    ]):
        # å¦‚æœæ‰€æœ‰å¢å¼ºåŠŸèƒ½éƒ½å…³é—­ï¼Œåˆ™ä½¿ç”¨åŸºç¡€ç‰ˆæœ¬
        logger.info("ğŸ”¬ Using basic entity matching (all enhancements disabled)")
        return basic_entity_matching(question_kg, entity_embeddings, keyword_embeddings, question_text)
    
    # ===== ç¬¬2æ­¥ï¼šåˆå§‹åŒ–å˜é‡ =====
    match_kg = []                                      # å­˜å‚¨åŒ¹é…åˆ°çš„çŸ¥è¯†å›¾è°±å®ä½“
    entity_embeddings_emb = pd.DataFrame(entity_embeddings["embeddings"])  # è½¬æ¢ä¸ºDataFrameä¾¿äºè®¡ç®—
    entity_confidence_scores = []                      # å­˜å‚¨æ¯ä¸ªåŒ¹é…çš„ç½®ä¿¡åº¦åˆ†æ•°
    
    # æ‰“å°ä¿¡æ¯
    debug_entity_matching_progress(
        "1.åˆå§‹åŒ–", 
        question_kg, question_text, match_kg
    )

    # ===== ç¬¬3æ­¥ï¼šé—®é¢˜ç±»å‹è¯†åˆ« =====
    question_types = semantic_question_classifier.identify_question_type(question_text)
    # ä¾‹å¦‚ï¼šquestion_text="What causes Alzheimer's?" â†’ question_types=['causation']

    # æ‰“å°ä¿¡æ¯
    debug_entity_matching_progress(
        "2.é—®é¢˜ç±»å‹è¯†åˆ«", 
        question_kg, question_text, 
        question_types=question_types, 
        match_kg=match_kg
    )
    
    # ===== ç¬¬4æ­¥ï¼šå®ä½“æ‰©å±• - åŒ»å­¦ç¼©å†™è¯å¤„ç† =====
    expanded_entities = []
    for kg_entity in question_kg:
        # æ‰©å±•åŒ»å­¦ç¼©å†™è¯ï¼ˆå¦‚ AD â†’ Alzheimer Diseaseï¼‰
        expanded_entity = expand_medical_abbreviations(kg_entity)
        expanded_entities.append(expanded_entity)
        # ä¾‹å¦‚ï¼škg_entity="AD" â†’ expanded_entity="Alzheimer Disease"

        # æ‰“å°ä¿¡æ¯
        debug_entity_matching_progress(
            "3.å®ä½“æ‰©å±•ä¸­çš„æ‰©å±•åŒ»å­¦ç¼©å†™è¯", 
            question_kg, question_text, 
            question_types=question_types, 
            expanded_entities=expanded_entities, 
            match_kg=match_kg,
            extra_info=f"åŒ¹é…çš„å®ä½“ä¸ºï¼š{kg_entity}ï¼Œæ‰©å±•çš„å®ä½“ä¸º{expanded_entity}"
        )
        
        # å¦‚æœå¯ç”¨UMLSæ ‡å‡†åŒ–ï¼Œè·å–åŒä¹‰è¯
        if ABLATION_CONFIG['USE_UMLS_NORMALIZATION']:
            synonyms = get_medical_synonyms(kg_entity)
            expanded_entities.extend(synonyms)
            # ä¾‹å¦‚ï¼škg_entity="alzheimer" â†’ synonyms=["dementia", "alzheimer disease", "ad"]

            # æ‰“å°ä¿¡æ¯
            debug_entity_matching_progress(
                "4.UMLSæ ‡å‡†åŒ–ï¼Œè·å–åŒä¹‰è¯", 
                question_kg, question_text, 
                question_types=question_types, 
                expanded_entities=expanded_entities, 
                match_kg=match_kg,
                extra_info=f"åŒ¹é…çš„å®ä½“ä¸ºï¼š{kg_entity}ï¼Œè·å–çš„åŒä¹‰è¯ä¸º{synonyms}"
            )
    
    # ===== ç¬¬5æ­¥ï¼šå¤šç­–ç•¥å®ä½“é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰ =====
    if ABLATION_CONFIG['USE_MULTI_STRATEGY_LINKING']:
        try:
            # ä½¿ç”¨è¯­ä¹‰åŒ¹é… + ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¢å¼ºé“¾æ¥
            enhanced_links = umls_normalizer.enhanced_entity_linking_method(
                expanded_entities, question_text, question_types
            )
            
            # ç­›é€‰é«˜ç½®ä¿¡åº¦çš„é“¾æ¥ç»“æœ
            for entity, link_info in enhanced_links.items():
                if link_info.get('final_score', 0) > 0.6:  # ç½®ä¿¡åº¦é˜ˆå€¼ä¸º0.6
                    expanded_entities.append(entity)

            # æ‰“å°ä¿¡æ¯
            debug_entity_matching_progress(
                "5.å¤šç­–ç•¥å®ä½“é“¾æ¥", 
                question_kg, question_text, 
                question_types=question_types, 
                expanded_entities=expanded_entities, 
                match_kg=match_kg,
                extra_info=f"é“¾æ¥ç»“æœ:{enhanced_links}"
            )
                    
        except Exception as e:
            logger.error(f"Error in enhanced entity linking: {e}")
    
    # ===== ç¬¬6æ­¥ï¼šè‡ªé€‚åº”UMLSçŸ¥è¯†é€‰æ‹©ï¼ˆå¦‚æœå¯ç”¨ï¼‰ =====
    if ABLATION_CONFIG['USE_ADAPTIVE_UMLS']:
        try:
            # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç›¸å…³çš„UMLSçŸ¥è¯†
            adaptive_knowledge = umls_normalizer.adaptive_knowledge_selection(
                question_types, expanded_entities
            )
            
            # ä»è‡ªé€‚åº”çŸ¥è¯†ä¸­æå–ç›¸å…³å®ä½“åç§°
            for knowledge_item in adaptive_knowledge:
                if isinstance(knowledge_item, dict):
                    related_name = knowledge_item.get('related_name', '')
                    if related_name:
                        expanded_entities.append(related_name)
                        # ä¾‹å¦‚ï¼šä»UMLSå…³ç³»ä¸­æå–åˆ° "cognitive_impairment"

            # æ‰“å°ä¿¡æ¯
            debug_entity_matching_progress(
                "6.è‡ªé€‚åº”UMLSçŸ¥è¯†é€‰æ‹©", 
                question_kg, question_text, 
                question_types=question_types, 
                expanded_entities=expanded_entities, 
                match_kg=match_kg,
                extra_info=f"æ ¹æ®é—®é¢˜ç±»å‹ï¼Œé€‰æ‹©åˆ°çš„UMLSçŸ¥è¯†:{adaptive_knowledge}"
            )
                            
        except Exception as e:
            logger.error(f"Error in adaptive knowledge selection: {e}")
    
    # ===== ç¬¬7æ­¥ï¼šåŸºäºæ¨ç†è§„åˆ™çš„å®ä½“æ‰©å±•ï¼ˆå¦‚æœå¯ç”¨ï¼‰ =====
    if ABLATION_CONFIG['USE_REASONING_RULES']:
        try:
            # åˆ›å»ºä¸´æ—¶ä¸‰å…ƒç»„ç”¨äºæ¨ç†
            temp_triples = [[entity, 'mentions', 'question'] for entity in expanded_entities]
            # åº”ç”¨åŒ»å­¦æ¨ç†è§„åˆ™ï¼ˆå¦‚ä¼ é€’æ€§ã€é€†å…³ç³»ç­‰ï¼‰
            reasoned_triples = medical_reasoning_rules.apply_reasoning_rules(temp_triples)
            
            # ä»æ¨ç†ç»“æœä¸­æå–æ–°çš„å®ä½“
            for triple in reasoned_triples:
                if len(triple) >= 3:
                    expanded_entities.extend([triple[0], triple[2]])  # æ·»åŠ å¤´å®ä½“å’Œå°¾å®ä½“

            # æ‰“å°ä¿¡æ¯
            debug_entity_matching_progress(
                "7.åŸºäºæ¨ç†è§„åˆ™çš„å®ä½“æ‰©å±•", 
                question_kg, question_text, 
                question_types=question_types, 
                expanded_entities=expanded_entities, 
                match_kg=match_kg
            )

        except Exception as e:
            logger.error(f"Error in reasoning-based entity expansion: {e}")

    # ===== ç¬¬8æ­¥ï¼šå»é‡å¤„ç† =====
    seen = set()                    # ç”¨äºè®°å½•å·²è§è¿‡çš„å®ä½“ï¼ˆå°å†™ï¼‰
    unique_entities = []            # å­˜å‚¨å»é‡åçš„å”¯ä¸€å®ä½“
    for entity in expanded_entities:
        if entity.lower() not in seen:
            seen.add(entity.lower())
            unique_entities.append(entity)

    # æ‰“å°ä¿¡æ¯
    debug_entity_matching_progress(
        "8.å»é‡å¤„ç†", 
        question_kg, question_text, 
        question_types=question_types, 
        expanded_entities=expanded_entities, 
        match_kg=match_kg
    )
    
    # æ‰“å°æ‰©å±•ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logger.info(f"Original entities: {question_kg}")
    logger.info(f"Expanded entities (with optimizations): {unique_entities[:10]}...")
    # ä¾‹å¦‚ï¼šOriginal: ["alzheimer"] â†’ Expanded: ["Alzheimer Disease", "dementia", "cognitive_impairment", ...]
    
    # ===== ç¬¬9æ­¥ï¼šåŠ¨æ€é˜ˆå€¼è°ƒæ•´ =====
    is_negation = has_negation(question_text)  # æ£€æŸ¥æ˜¯å¦æœ‰å¦å®šè¯
    if 'exception' in question_types or is_negation:
        # å¯¹äºå¦å®š/ä¾‹å¤–é—®é¢˜ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼
        base_threshold = THRESHOLDS.get_threshold('entity_matching', 'min_similarity')
        similarity_threshold = THRESHOLDS.adjust_for_negation(base_threshold)
        # ä¾‹å¦‚ï¼šbase_threshold=0.6 â†’ similarity_threshold=0.6*0.8=0.48
    else:
        # æ™®é€šé—®é¢˜ä½¿ç”¨æ ‡å‡†é˜ˆå€¼
        similarity_threshold = THRESHOLDS.get_threshold('entity_matching', 'enhanced_similarity')
        # ä¾‹å¦‚ï¼šsimilarity_threshold=0.6

    # æ‰“å°ä¿¡æ¯
    debug_entity_matching_progress(
        "9.åŠ¨æ€é˜ˆå€¼è°ƒæ•´", 
        question_kg, question_text, 
        question_types=question_types, 
        expanded_entities=expanded_entities, 
        match_kg=match_kg
    )
    
    # ===== ç¬¬10æ­¥ï¼šå‘é‡åŒ¹é…è¿‡ç¨‹ =====
    for kg_entity in unique_entities:
        try:
            # å°è¯•ç›´æ¥åœ¨å…³é”®è¯åµŒå…¥ä¸­æ‰¾åˆ°å®ä½“
            if kg_entity in keyword_embeddings["keywords"]:
                keyword_index = keyword_embeddings["keywords"].index(kg_entity)
            else:
                # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                best_match_idx = None
                best_similarity = 0
                for idx, keyword in enumerate(keyword_embeddings["keywords"]):
                    # æ£€æŸ¥å®ä½“æ˜¯å¦åŒ…å«åœ¨å…³é”®è¯ä¸­ï¼Œæˆ–å…³é”®è¯åŒ…å«åœ¨å®ä½“ä¸­
                    if kg_entity.lower() in keyword.lower() or keyword.lower() in kg_entity.lower():
                        # è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆäº¤é›†/å¹¶é›†ï¼‰
                        similarity = len(set(kg_entity.lower().split()) & set(keyword.lower().split())) / len(set(kg_entity.lower().split()) | set(keyword.lower().split()))
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_match_idx = idx
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…æˆ–ç›¸ä¼¼åº¦å¤ªä½ï¼Œè·³è¿‡è¿™ä¸ªå®ä½“
                if best_match_idx is None or best_similarity < 0.3:
                    continue
                keyword_index = best_match_idx
            
            # ===== ç¬¬11æ­¥ï¼šå‘é‡ç›¸ä¼¼åº¦è®¡ç®— =====
            # è·å–å®ä½“çš„åµŒå…¥å‘é‡
            kg_entity_emb = np.array(keyword_embeddings["embeddings"][keyword_index])

            # å‘é‡æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–åˆ°å•ä½é•¿åº¦ï¼‰
            kg_entity_emb_norm = kg_entity_emb / np.linalg.norm(kg_entity_emb)
            entity_embeddings_norm = entity_embeddings_emb.values / np.linalg.norm(entity_embeddings_emb.values, axis=1, keepdims=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cos_similarities = np.dot(entity_embeddings_norm, kg_entity_emb_norm)
            
            # ===== ç¬¬12æ­¥ï¼šTop-Kå€™é€‰é€‰æ‹© =====
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼Œå–å‰5ä¸ªå€™é€‰
            top_indices = np.argsort(cos_similarities)[::-1]
            
            best_match_found = False
            # éå†å‰5ä¸ªæœ€ç›¸ä¼¼çš„å€™é€‰å®ä½“
            for idx in top_indices[:5]:
                similarity_score = cos_similarities[idx]           # ç›¸ä¼¼åº¦åˆ†æ•°
                candidate_entity = entity_embeddings["entities"][idx]  # å€™é€‰å®ä½“åç§°
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼è¦æ±‚ä¸”æœªé‡å¤
                if (similarity_score >= similarity_threshold and 
                    candidate_entity not in match_kg):
                    match_kg.append(candidate_entity)              # æ·»åŠ åˆ°åŒ¹é…åˆ—è¡¨
                    entity_confidence_scores.append(float(similarity_score))  # è®°å½•ç½®ä¿¡åº¦
                    best_match_found = True
                    logger.debug(f"Matched: {kg_entity} -> {candidate_entity} (score: {similarity_score:.3f})")
                    break  # æ‰¾åˆ°ä¸€ä¸ªé«˜è´¨é‡åŒ¹é…å°±åœæ­¢
            
            # å¦‚æœæ²¡æ‰¾åˆ°é«˜ç½®ä¿¡åº¦åŒ¹é…ï¼Œè®°å½•è­¦å‘Š
            if not best_match_found:
                logger.warning(f"No high-confidence match found for entity: {kg_entity}")
                
        except (ValueError, IndexError):
            # å®ä½“ä¸åœ¨å…³é”®è¯åµŒå…¥ä¸­
            logger.error(f"Entity {kg_entity} not found in keyword embeddings")
            continue
        except Exception as e:
            # å…¶ä»–å¤„ç†é”™è¯¯
            logger.error(f"Error processing entity {kg_entity}: {e}")
            continue

    # æ‰“å°ä¿¡æ¯
    debug_entity_matching_progress(
        "10.å‘é‡åŒ¹é…", 
        question_kg, question_text, 
        question_types=question_types, 
        expanded_entities=expanded_entities, 
        match_kg=match_kg,
        extra_info=f"ç½®ä¿¡åº¦ï¼š{entity_confidence_scores}"
    )
    
    # ===== ç¬¬13æ­¥ï¼šç»“æœç»Ÿè®¡å’Œè¿”å› =====
    if entity_confidence_scores:
        avg_confidence = np.mean(entity_confidence_scores)
        logger.info(f"Entity matching average confidence: {avg_confidence:.3f}")
    
    return match_kg, entity_confidence_scores  # è¿”å›åŒ¹é…çš„å®ä½“åˆ—è¡¨å’Œç½®ä¿¡åº¦åˆ†æ•°

def enhanced_find_shortest_path(start_entity_name, end_entity_name, candidate_list, question_types=[]):
    """å¢å¼ºçš„è·¯å¾„æŸ¥æ‰¾ï¼Œå¸¦æœ‰åŒ»å­¦çŸ¥è¯†æƒé‡"""
    global exist_entity
    paths_with_scores = []
    
    with driver.session() as session:
        try:
            result = session.run(
                "MATCH (start_entity:Entity{name:$start_entity_name}), (end_entity:Entity{name:$end_entity_name}) "
                "MATCH p = allShortestPaths((start_entity)-[*..5]->(end_entity)) "
                "RETURN p LIMIT 15",
                start_entity_name=start_entity_name,
                end_entity_name=end_entity_name
            )
            
            paths = []
            short_path = 0
            
            for record in result:
                path = record["p"]
                entities = []
                relations = []
                path_quality_score = 0
                
                for i in range(len(path.nodes)):
                    node = path.nodes[i]
                    entity_name = node["name"]
                    entities.append(entity_name)
                    
                    if i < len(path.relationships):
                        relationship = path.relationships[i]
                        relation_type = relationship.type
                        relations.append(relation_type)
                        
                        if any([ABLATION_CONFIG['USE_HIERARCHICAL_KG'], 
                               ABLATION_CONFIG['USE_REASONING_RULES'],
                               ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']]):
                            relation_weight = calculate_relation_weight(relation_type)
                            path_quality_score += relation_weight
                            
                            if question_types:
                                if 'treatment' in question_types and 'treat' in relation_type.lower():
                                    path_quality_score += 1.0
                                elif 'causation' in question_types and 'cause' in relation_type.lower():
                                    path_quality_score += 1.0
                                elif 'symptom' in question_types and 'symptom' in relation_type.lower():
                                    path_quality_score += 1.0
                        else:
                            path_quality_score += 1.0
               
                path_str = ""
                for i in range(len(entities)):
                    entities[i] = entities[i].replace("_"," ")
                    
                    if entities[i] in candidate_list:
                        short_path = 1
                        exist_entity = entities[i]
                        path_quality_score += 3
                        
                    path_str += entities[i]
                    if i < len(relations):
                        relations[i] = relations[i].replace("_"," ")
                        path_str += "->" + relations[i] + "->"
                
                path_length = len(relations)
                length_penalty = path_length * 0.1 if ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP'] else 0
                final_score = path_quality_score - length_penalty
                
                paths_with_scores.append((path_str, final_score))
                
                if short_path == 1:
                    if ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
                        paths_with_scores.sort(key=lambda x: x[1], reverse=True)
                    paths = [path[0] for path in paths_with_scores[:5]]
                    break
            
            if not paths and paths_with_scores:
                if ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
                    paths_with_scores.sort(key=lambda x: x[1], reverse=True)
                paths = [path[0] for path in paths_with_scores[:5]]
                exist_entity = {}
            
        except Exception as e:
            logger.error(f"Error in path finding: {e}")
            return [], {}
            
        try:
            return paths, exist_entity
        except:
            return paths, {}

def find_shortest_path(start_entity_name, end_entity_name, candidate_list, question_types=[]):
    """åŸå§‹å‡½æ•°ï¼Œä½¿ç”¨å¢å¼ºå®ç°"""
    return enhanced_find_shortest_path(start_entity_name, end_entity_name, candidate_list, question_types)

def combine_lists(*lists):
    combinations = list(itertools.product(*lists))
    results = []
    for combination in combinations:
        new_combination = []
        for sublist in combination:
            if isinstance(sublist, list):
                new_combination += sublist
            else:
                new_combination.append(sublist)
        results.append(new_combination)
    return results

def enhanced_get_entity_neighbors(entity_name: str, disease_flag, question_types=[]) -> Tuple[List[List[str]], List[str]]:
    """å¢å¼ºçš„é‚»å±…æå–ï¼Œå¸¦æœ‰é—®é¢˜ç±»å‹æ„ŸçŸ¥è¿‡æ»¤"""
    disease = []
    neighbor_list = []
    
    if any([ABLATION_CONFIG['USE_ADAPTIVE_UMLS'], ABLATION_CONFIG['USE_REASONING_RULES']]):
        limit = 25 if any(q_type in ['treatment', 'causation'] for q_type in question_types) else 20
    else:
        limit = 10
    
    query = f"""
    MATCH (e:Entity)-[r]->(n)
    WHERE e.name = $entity_name
    RETURN type(r) AS relationship_type,
           collect(n.name) AS neighbor_entities
    ORDER BY size(collect(n.name)) DESC
    LIMIT {limit}
    """
    
    try:
        result = session.run(query, entity_name=entity_name)
        relation_quality_scores = {}
        
        for record in result:
            rel_type = record["relationship_type"]
            
            if disease_flag == 1 and rel_type == 'has_symptom':
                continue

            neighbors = record["neighbor_entities"]
            
            if ABLATION_CONFIG['USE_REASONING_RULES']:
                quality_score = calculate_relation_weight(rel_type)
                
                if question_types:
                    if 'treatment' in question_types and 'treat' in rel_type.lower():
                        quality_score += 1.0
                    elif 'causation' in question_types and 'cause' in rel_type.lower():
                        quality_score += 1.0
                    elif 'symptom' in question_types and 'symptom' in rel_type.lower():
                        quality_score += 1.0
            else:
                quality_score = 1.0
            
            if "disease" in rel_type.replace("_"," ").lower():
                disease.extend(neighbors)
                quality_score += 1.0
                
            filtered_neighbors = []
            for neighbor in neighbors:
                if validate_knowledge_triple(entity_name, rel_type, neighbor):
                    filtered_neighbors.append(neighbor)
            
            if filtered_neighbors:
                neighbor_entry = [entity_name.replace("_"," "), rel_type.replace("_"," "), 
                                ','.join([x.replace("_"," ") for x in filtered_neighbors])]
                neighbor_list.append(neighbor_entry)
                relation_quality_scores[len(neighbor_list)-1] = quality_score
        
        if relation_quality_scores and ABLATION_CONFIG['USE_REASONING_RULES']:
            sorted_indices = sorted(relation_quality_scores.keys(), 
                                  key=lambda k: relation_quality_scores[k], reverse=True)
            neighbor_list = [neighbor_list[i] for i in sorted_indices]
    
    except Exception as e:
        logger.error(f"Error getting entity neighbors: {e}")
    
    return neighbor_list, disease

def get_entity_neighbors(entity_name: str, disease_flag, question_types=[]) -> List[List[str]]:
    """åŸå§‹å‡½æ•°ç­¾åä¿æŒä¸å˜"""
    neighbor_list, disease = enhanced_get_entity_neighbors(entity_name, disease_flag, question_types)
    return neighbor_list, disease

@retry_on_failure()
def prompt_path_finding(path_input):
    """åŸå§‹è·¯å¾„æŸ¥æ‰¾æç¤ºæ¨¡æ¿"""
    template = """
    There are some knowledge graph path. They follow entity->relationship->entity format.
    \n\n
    {Path}
    \n\n
    Use the knowledge graph information. Try to convert them to natural language, respectively. Use single quotation marks for entity name and relation name. And name them as Path-based Evidence 1, Path-based Evidence 2,...\n\n

    Output:
    """

    prompt = PromptTemplate(
        template = template,
        input_variables = ["Path"]
    )

    system_message_prompt = SystemMessagePromptTemplate(prompt = prompt)
    system_message_prompt.format(Path = path_input)

    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])
    chat_prompt_with_values = chat_prompt.format_prompt(Path = path_input,\
                                                        text={})

    response = chat(chat_prompt_with_values.to_messages())
    if response.content is not None:
        return response.content
    else:
        return ""

@retry_on_failure()
def prompt_neighbor(neighbor):
    """åŸå§‹é‚»å±…æç¤ºæ¨¡æ¿"""
    template = """
    There are some knowledge graph. They follow entity->relationship->entity list format.
    \n\n
    {neighbor}
    \n\n
    Use the knowledge graph information. Try to convert them to natural language, respectively. Use single quotation marks for entity name and relation name. And name them as Neighbor-based Evidence 1, Neighbor-based Evidence 2,...\n\n

    Output:
    """

    prompt = PromptTemplate(
        template = template,
        input_variables = ["neighbor"]
    )

    system_message_prompt = SystemMessagePromptTemplate(prompt = prompt)
    system_message_prompt.format(neighbor = neighbor)

    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])
    chat_prompt_with_values = chat_prompt.format_prompt(neighbor = neighbor,\
                                                        text={})

    response = chat(chat_prompt_with_values.to_messages())
    if response.content is not None:
        return response.content
    else:
        return ""

@retry_on_failure()
def self_knowledge_retrieval(graph, question):
    """åŸå§‹çŸ¥è¯†æ£€ç´¢æç¤ºæ¨¡æ¿"""
    template = """
    There is a question and some knowledge graph. The knowledge graphs follow entity->relationship->entity list format.
    \n\n
    ##Graph: {graph}
    \n\n
    ##Question: {question}
    \n\n
    Please filter noisy knowledge from this knowledge graph that useless or irrelevant to the give question. Output the filtered knowledges in the same format as the input knowledge graph.\n\n

    Filtered Knowledge:
    """

    prompt = PromptTemplate(
        template = template,
        input_variables = ["graph", "question"]
    )

    system_message_prompt = SystemMessagePromptTemplate(prompt = prompt)
    system_message_prompt.format(graph = graph, question=question)

    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])
    chat_prompt_with_values = chat_prompt.format_prompt(graph = graph, question=question,\
                                                        text={})

    response = chat(chat_prompt_with_values.to_messages())
    if response.content is not None:
        return response.content
    else:
        return ""

def enhanced_self_knowledge_retrieval_reranking(graph, question):
    """å¢å¼ºçš„çŸ¥è¯†é‡æ’åºï¼Œå¸¦æœ‰åŒ»å­¦çŸ¥è¯†æ„ŸçŸ¥å’Œå¤šè·³æ¨ç†"""
    
    if not any([ABLATION_CONFIG['USE_REASONING_RULES'], 
               ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP'],
               ABLATION_CONFIG['USE_KG_GUIDED_REASONING']]):
        logger.info("ğŸ”¬ Using basic knowledge retrieval reranking")
        return self_knowledge_retrieval(graph, question)
    
    question_types = semantic_question_classifier.identify_question_type(question)
    has_neg = has_negation(question)
    
    if ABLATION_CONFIG['USE_OPTIMIZED_MULTIHOP']:
        try:
            graph_triples = []
            for line in graph.split('\n'):
                if '->' in line:
                    parts = line.split('->')
                    if len(parts) >= 3:
                        head = parts[0].strip()
                        relation = parts[1].strip()
                        tail = '->'.join(parts[2:]).strip()
                        graph_triples.append([head, relation, tail])
            
            if graph_triples:
                reasoned_result = multi_hop_reasoner.perform_multi_hop_reasoning(question, graph_triples)
                logger.debug(f"Multi-hop reasoning result: {reasoned_result[:200]}...")
        
        except Exception as e:
            logger.error(f"Error in multi-hop reasoning during reranking: {e}")
    
    if 'exception' in question_types or has_neg:
        focus_instruction = "Focus on triples that help identify what is NOT associated with the question topic or what should be excluded."
    elif 'treatment' in question_types:
        focus_instruction = "Focus on triples related to treatments, therapies, medications, and therapeutic interventions."
    elif 'causation' in question_types:
        focus_instruction = "Focus on triples showing causal relationships, risk factors, and etiological connections."
    elif 'symptom' in question_types:
        focus_instruction = "Focus on triples describing symptoms, signs, manifestations, and clinical presentations."
    else:
        focus_instruction = "Focus on triples that directly relate to the question entities and provide meaningful medical relationships."
    
    template = f"""
    There is a question and some knowledge graph. The knowledge graphs follow entity->relationship->entity list format.
    \n\n
    ##Graph: {{graph}}
    \n\n
    ##Question: {{question}}
    \n\n
    Please rerank the knowledge graph and output at most 5 important and relevant triples for solving the given question. {focus_instruction} Output the reranked knowledge in the following format:
    Reranked Triple1: xxx â€”â€”> xxx
    Reranked Triple2: xxx â€”â€”> xxx
    Reranked Triple3: xxx â€”â€”> xxx
    Reranked Triple4: xxx â€”â€”> xxx
    Reranked Triple5: xxx â€”â€”> xxx

    Answer:
    """

    prompt = PromptTemplate(
        template = template,
        input_variables = ["graph", "question"]
    )

    system_message_prompt = SystemMessagePromptTemplate(prompt = prompt)
    system_message_prompt.format(graph = graph, question=question)

    human_template = "{text}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])
    chat_prompt_with_values = chat_prompt.format_prompt(graph = graph, question=question,\
                                                        text={})

    for attempt in range(3):
        try:
            response = chat(chat_prompt_with_values.to_messages())
            if response.content is not None and len(response.content.strip()) > 10:
                return response.content
            else:
                logger.warning(f"Reranking attempt {attempt + 1} returned insufficient content")
                sleep(5)
        except Exception as e:
            logger.error(f"Reranking attempt {attempt + 1} failed: {e}")
            sleep(5)
    
    logger.error("All reranking attempts failed, returning original graph")
    return graph

@retry_on_failure()
def self_knowledge_retrieval_reranking(graph, question):
    """åŸå§‹å‡½æ•°ï¼Œä½¿ç”¨å¢å¼ºå®ç°"""
    return enhanced_self_knowledge_retrieval_reranking(graph, question)

def enhanced_is_unable_to_answer(response):
    """å¢å¼ºçš„å“åº”è´¨é‡éªŒè¯"""
    if not response or len(response.strip()) < 5:
        return True
    
    failure_patterns = [
        "i don't know", "cannot answer", "insufficient information",
        "unable to determine", "not enough context", "cannot provide"
    ]
    
    response_lower = response.lower()
    for pattern in failure_patterns:
        if pattern in response_lower:
            return True
    
    try:
        analysis = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": response}
            ],
            max_tokens=1,
            temperature=0.0,
            n=1,
            stop=None,
            presence_penalty=0.0,
            frequency_penalty=0.0
        )
        score = analysis.choices[0].message.content.strip().replace("'", "").replace(".", "")
        if not score.isdigit():   
            return True
        threshold = 0.6
        if float(score) > threshold:
            return False
        else:
            return True
    except:
        return False

def is_unable_to_answer(response):
    """åŸå§‹å‡½æ•°ï¼Œä½¿ç”¨å¢å¼ºå®ç°"""
    return enhanced_is_unable_to_answer(response)

def enhanced_final_answer(question_text, response_of_KG_list_path, response_of_KG_neighbor):
    """
    å¢å¼ºçš„æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå‡½æ•°
    
    åŠŸèƒ½ï¼šæ•´åˆçŸ¥è¯†å›¾è°±è·¯å¾„ä¿¡æ¯å’Œé‚»å±…ä¿¡æ¯ï¼Œä½¿ç”¨å¤šç­–ç•¥æ¨ç†ç”ŸæˆåŒ»ç–—é—®ç­”çš„æœ€ç»ˆç­”æ¡ˆ
    
    å‚æ•°ï¼š
    - question_text: åŸå§‹åŒ»ç–—é—®é¢˜æ–‡æœ¬
    - response_of_KG_list_path: çŸ¥è¯†å›¾è°±è·¯å¾„æ¨ç†çš„ç»“æœæ–‡æœ¬
    - response_of_KG_neighbor: çŸ¥è¯†å›¾è°±é‚»å±…å®ä½“çš„ç»“æœæ–‡æœ¬
    
    è¿”å›ï¼š
    - str: æœ€ç»ˆçš„ç­”æ¡ˆé€‰é¡¹ï¼ˆå¦‚ "A", "B", "C", "D", "E"ï¼‰
    """
    
    # ========== ç¬¬1æ­¥ï¼šæ¶ˆèå®éªŒé…ç½®æ£€æŸ¥ ==========
    if not ABLATION_CONFIG['USE_ENHANCED_ANSWER_GEN']:
        # æ£€æŸ¥å…¨å±€é…ç½®ï¼Œå¦‚æœç¦ç”¨äº†å¢å¼ºç­”æ¡ˆç”ŸæˆåŠŸèƒ½
        logger.info("ğŸ”¬ Using basic final answer generation")
        # è®°å½•æ—¥å¿—ï¼Œè¯´æ˜ä½¿ç”¨åŸºç¡€ç‰ˆæœ¬
        return basic_final_answer(question_text, response_of_KG_list_path, response_of_KG_neighbor)
        # ç›´æ¥è°ƒç”¨åŸºç¡€ç‰ˆæœ¬çš„ç­”æ¡ˆç”Ÿæˆå‡½æ•°å¹¶è¿”å›
    
    # ========== ç¬¬2æ­¥ï¼šè¾“å…¥æ•°æ®é¢„å¤„ç† ==========
    if response_of_KG_list_path == []:
        # å¦‚æœè·¯å¾„æ¨ç†ç»“æœæ˜¯ç©ºåˆ—è¡¨
        response_of_KG_list_path = ''
        # å°†å…¶è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œç»Ÿä¸€æ•°æ®ç±»å‹
    if response_of_KG_neighbor == []:
        # å¦‚æœé‚»å±…æ¨ç†ç»“æœæ˜¯ç©ºåˆ—è¡¨
        response_of_KG_neighbor = ''
        # å°†å…¶è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œç»Ÿä¸€æ•°æ®ç±»å‹
    
    # ========== ç¬¬3æ­¥ï¼šé—®é¢˜ç±»å‹è¯†åˆ«å’Œå¦å®šè¯å¤„ç† ==========
    question_types = semantic_question_classifier.identify_question_type(question_text)
    # ä½¿ç”¨è¯­ä¹‰é—®é¢˜åˆ†ç±»å™¨è¯†åˆ«é—®é¢˜ç±»å‹
    # ä¾‹å¦‚ï¼š['causation']è¡¨ç¤ºå› æœå…³ç³»é—®é¢˜ï¼Œ['treatment']è¡¨ç¤ºæ²»ç–—é—®é¢˜ç­‰
    
    has_neg = has_negation(question_text)
    # æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«å¦å®šè¯ï¼ˆå¦‚"not", "except", "excluding"ç­‰ï¼‰
    # è¿”å›å¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºå­˜åœ¨å¦å®šï¼ŒFalseè¡¨ç¤ºä¸å­˜åœ¨
    
    # ========== ç¬¬4æ­¥ï¼šçŸ¥è¯†å›¾è°±å¼•å¯¼æ¨ç†ï¼ˆKG-guided reasoningï¼‰==========
    try:
        kg_subgraph = []
        # åˆå§‹åŒ–çŸ¥è¯†å­å›¾åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ä¸‰å…ƒç»„[å¤´å®ä½“, å…³ç³», å°¾å®ä½“]
        
        # å¤„ç†è·¯å¾„æ¨ç†ç»“æœ
        if response_of_KG_list_path:
            # å¦‚æœè·¯å¾„æ¨ç†ç»“æœä¸ä¸ºç©º
            path_lines = response_of_KG_list_path.split('\n')
            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²æˆå¤šè¡Œ
            for line in path_lines:
                # éå†æ¯ä¸€è¡Œ
                if '->' in line:
                    # å¦‚æœè¯¥è¡ŒåŒ…å«ç®­å¤´åˆ†éš”ç¬¦ï¼ˆè¡¨ç¤ºå®ä½“->å…³ç³»->å®ä½“çš„æ ¼å¼ï¼‰
                    parts = line.split('->')
                    # æŒ‰ç®­å¤´åˆ†å‰²æˆä¸åŒéƒ¨åˆ†
                    if len(parts) >= 3:
                        # å¦‚æœåˆ†å‰²åè‡³å°‘æœ‰3ä¸ªéƒ¨åˆ†ï¼ˆå¤´å®ä½“ã€å…³ç³»ã€å°¾å®ä½“ï¼‰
                        kg_subgraph.append([parts[0].strip(), parts[1].strip(), parts[2].strip()])
                        # å»é™¤ç©ºç™½å­—ç¬¦å¹¶æ·»åŠ åˆ°çŸ¥è¯†å­å›¾ä¸­
        
        # å¤„ç†é‚»å±…æ¨ç†ç»“æœ
        if response_of_KG_neighbor:
            # å¦‚æœé‚»å±…æ¨ç†ç»“æœä¸ä¸ºç©º
            neighbor_lines = response_of_KG_neighbor.split('\n')
            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²æˆå¤šè¡Œ
            for line in neighbor_lines:
                # éå†æ¯ä¸€è¡Œ
                if '->' in line:
                    # å¦‚æœè¯¥è¡ŒåŒ…å«ç®­å¤´åˆ†éš”ç¬¦
                    parts = line.split('->')
                    # æŒ‰ç®­å¤´åˆ†å‰²
                    if len(parts) >= 3:
                        # å¦‚æœåˆ†å‰²åè‡³å°‘æœ‰3ä¸ªéƒ¨åˆ†
                        kg_subgraph.append([parts[0].strip(), parts[1].strip(), parts[2].strip()])
                        # æ·»åŠ åˆ°çŸ¥è¯†å­å›¾ä¸­
        
        # æ‰§è¡ŒçŸ¥è¯†å›¾è°±å¼•å¯¼æ¨ç†
        if kg_subgraph and medical_reasoning_rules.kg_guided_reasoning:
            # å¦‚æœçŸ¥è¯†å­å›¾ä¸ä¸ºç©ºä¸”KGå¼•å¯¼æ¨ç†æ¨¡å—å¯ç”¨
            kg_guided_result = medical_reasoning_rules.kg_guided_reasoning.kg_guided_reasoning(
                question_text, kg_subgraph
            )
            # è°ƒç”¨KGå¼•å¯¼æ¨ç†ï¼Œä¼ å…¥é—®é¢˜æ–‡æœ¬å’ŒçŸ¥è¯†å­å›¾
            logger.debug(f"KG-guided reasoning result: {kg_guided_result[:200]}...")
            # è®°å½•æ¨ç†ç»“æœçš„å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
        
    except Exception as e:
        # å¦‚æœåœ¨KGå¼•å¯¼æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸
        logger.error(f"Error in KG-guided reasoning: {e}")
        # è®°å½•é”™è¯¯æ—¥å¿—ï¼Œä½†ä¸ä¸­æ–­ç¨‹åºæ‰§è¡Œ
    
    # ========== ç¬¬5æ­¥ï¼šæ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´æ¨ç†æŒ‡ä»¤ ==========
    if has_neg or 'exception' in question_types:
        # å¦‚æœé—®é¢˜åŒ…å«å¦å®šè¯æˆ–è€…é—®é¢˜ç±»å‹æ˜¯ä¾‹å¤–ç±»å‹
        reasoning_instruction = "Pay special attention to negation words and identify what should be EXCLUDED or what is NOT associated with the topic."
        # è®¾ç½®å¦å®šæ¨ç†æŒ‡ä»¤ï¼Œæé†’æ¨¡å‹æ³¨æ„å¦å®šè¯å’Œæ’é™¤é€»è¾‘
    else:
        # å¦‚æœæ˜¯æ­£å¸¸çš„è‚¯å®šé—®é¢˜
        reasoning_instruction = "Focus on positive associations and direct relationships."
        # è®¾ç½®æ­£é¢æ¨ç†æŒ‡ä»¤ï¼Œå…³æ³¨æ­£å‘å…³è”å’Œç›´æ¥å…³ç³»
    
    # ========== ç¬¬6æ­¥ï¼šæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ç”Ÿæˆ ==========
    messages = [
        # æ„å»ºå¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹
        SystemMessage(content="You are an excellent AI assistant specialized in medical question answering with access to UMLS standardized medical knowledge and hierarchical reasoning capabilities"),
        # ç³»ç»Ÿæ¶ˆæ¯ï¼šå®šä¹‰AIåŠ©æ‰‹çš„è§’è‰²å’Œèƒ½åŠ›
        HumanMessage(content=f'Question: {question_text}'),
        # ç”¨æˆ·æ¶ˆæ¯ï¼šæä¾›åŸå§‹é—®é¢˜
        AIMessage(content=f"You have some medical knowledge information in the following:\n\n" + 
                 f'###Path-based Evidence: {response_of_KG_list_path}\n\n' + 
                 f'###Neighbor-based Evidence: {response_of_KG_neighbor}'),
        # AIæ¶ˆæ¯ï¼šæä¾›çŸ¥è¯†å›¾è°±è¯æ®ï¼ŒåŒ…æ‹¬è·¯å¾„è¯æ®å’Œé‚»å±…è¯æ®
        HumanMessage(content=f"Answer: Let's think step by step using hierarchical medical reasoning. {reasoning_instruction} ")
        # ç”¨æˆ·æ¶ˆæ¯ï¼šè¯·æ±‚é€æ­¥æ¨ç†ï¼Œå¹¶æä¾›é’ˆå¯¹é—®é¢˜ç±»å‹çš„ç‰¹å®šæŒ‡ä»¤
    ]
    
    output_CoT = ""
    # åˆå§‹åŒ–æ€ç»´é“¾è¾“å‡ºå˜é‡
    for retry in range(3):
        # æœ€å¤šå°è¯•3æ¬¡ç”Ÿæˆæ€ç»´é“¾
        try:
            result_CoT = chat(messages)
            # è°ƒç”¨èŠå¤©æ¨¡å‹ç”Ÿæˆæ€ç»´é“¾æ¨ç†è¿‡ç¨‹
            if result_CoT.content is not None and len(result_CoT.content.strip()) > 10:
                # å¦‚æœç”Ÿæˆçš„å†…å®¹ä¸ä¸ºç©ºä¸”é•¿åº¦å¤§äº10ä¸ªå­—ç¬¦
                output_CoT = result_CoT.content
                # ä¿å­˜æ€ç»´é“¾å†…å®¹
                break
                # é€€å‡ºé‡è¯•å¾ªç¯
            else:
                logger.warning(f"CoT generation attempt {retry + 1} returned insufficient content")
                # è®°å½•è­¦å‘Šï¼šå†…å®¹ä¸è¶³
                sleep(5)
                # ç­‰å¾…5ç§’åé‡è¯•
        except Exception as e:
            # å¦‚æœç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸
            logger.error(f"CoT generation attempt {retry + 1} failed: {e}")
            # è®°å½•é”™è¯¯æ—¥å¿—
            sleep(5)
            # ç­‰å¾…5ç§’åé‡è¯•
    
    if not output_CoT:
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ€ç»´é“¾ä¸ºç©º
        logger.warning("CoT generation failed, using default reasoning")
        # è®°å½•è­¦å‘Šæ—¥å¿—
        output_CoT = f"Based on the provided medical knowledge, I need to analyze the evidence carefully."
        # ä½¿ç”¨é»˜è®¤çš„æ¨ç†æ–‡æœ¬
    
    # ========== ç¬¬7æ­¥ï¼šå¤šæ¬¡ç­”æ¡ˆç”Ÿæˆå’ŒæŠ•ç¥¨æœºåˆ¶ ==========
    answers = []
    # åˆå§‹åŒ–ç­”æ¡ˆåˆ—è¡¨ï¼Œç”¨äºæ”¶é›†å¤šæ¬¡ç”Ÿæˆçš„ç­”æ¡ˆ
    for attempt in range(3):
        # å°è¯•3æ¬¡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        try:
            final_prompts = [
                # å®šä¹‰ä¸åŒçš„æç¤ºè¯ï¼Œæ¯æ¬¡å°è¯•ä½¿ç”¨ä¸åŒçš„æç¤º
                "The final answer (output the letter option) is:",
                "Based on the hierarchical analysis above, the correct answer is:",
                "Therefore, using multi-strategy reasoning, the answer choice is:"
            ]
            
            messages = [
                # æ„å»ºæœ€ç»ˆç­”æ¡ˆç”Ÿæˆçš„å¯¹è¯æ¶ˆæ¯
                SystemMessage(content="You are an excellent AI assistant specialized in medical question answering with access to UMLS standardized medical knowledge and hierarchical reasoning capabilities"),
                # ç³»ç»Ÿæ¶ˆæ¯ï¼šå®šä¹‰AIåŠ©æ‰‹è§’è‰²
                HumanMessage(content=f'Question: {question_text}'),
                # ç”¨æˆ·æ¶ˆæ¯ï¼šåŸå§‹é—®é¢˜
                AIMessage(content=f"Medical knowledge:\n\n" + 
                         f'###Path-based Evidence: {response_of_KG_list_path}\n\n' + 
                         f'###Neighbor-based Evidence: {response_of_KG_neighbor}'),
                # AIæ¶ˆæ¯ï¼šåŒ»ç–—çŸ¥è¯†è¯æ®
                AIMessage(content=f"Analysis: {output_CoT}"),
                # AIæ¶ˆæ¯ï¼šå‰é¢ç”Ÿæˆçš„æ€ç»´é“¾åˆ†æ
                AIMessage(content=final_prompts[attempt % len(final_prompts)])
                # AIæ¶ˆæ¯ï¼šä½¿ç”¨å¾ªç¯æ–¹å¼é€‰æ‹©ä¸åŒçš„æœ€ç»ˆæç¤ºè¯
            ]
            
            result = chat(messages)
            # è°ƒç”¨èŠå¤©æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if result.content is not None and len(result.content.strip()) > 0:
                # å¦‚æœç”Ÿæˆçš„å†…å®¹ä¸ä¸ºç©º
                answer_match = re.search(r'\b([A-E])\b', result.content)
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢A-Eçš„é€‰é¡¹å­—æ¯
                if answer_match:
                    # å¦‚æœæ‰¾åˆ°äº†åŒ¹é…çš„é€‰é¡¹å­—æ¯
                    answers.append(answer_match.group(1))
                    # å°†é€‰é¡¹å­—æ¯æ·»åŠ åˆ°ç­”æ¡ˆåˆ—è¡¨
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†çš„é€‰é¡¹å­—æ¯
                    answers.append(result.content.strip()[:10])
                    # å–å‰10ä¸ªå­—ç¬¦ä½œä¸ºç­”æ¡ˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                    
        except Exception as e:
            # å¦‚æœç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸
            logger.error(f"Final answer attempt {attempt + 1} failed: {e}")
            # è®°å½•é”™è¯¯æ—¥å¿—
            sleep(3)
            # ç­‰å¾…3ç§’åé‡è¯•
    
    # ========== ç¬¬8æ­¥ï¼šæŠ•ç¥¨é€‰æ‹©æœ€ç»ˆç­”æ¡ˆ ==========
    if answers:
        # å¦‚æœæˆåŠŸç”Ÿæˆäº†è‡³å°‘ä¸€ä¸ªç­”æ¡ˆ
        answer_counts = Counter(answers)
        # ä½¿ç”¨Counterç»Ÿè®¡æ¯ä¸ªç­”æ¡ˆå‡ºç°çš„æ¬¡æ•°
        most_common_answer = answer_counts.most_common(1)[0][0]
        # è·å–å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç­”æ¡ˆï¼ˆæŠ•ç¥¨æœºåˆ¶ï¼‰
        
        logger.info(f"Voting results: {dict(answer_counts)}, Selected: {most_common_answer}")
        # è®°å½•æŠ•ç¥¨ç»“æœå’Œé€‰æ‹©çš„ç­”æ¡ˆ
        return most_common_answer
        # è¿”å›è·å¾—æœ€å¤šç¥¨æ•°çš„ç­”æ¡ˆ
    
    # ========== ç¬¬9æ­¥ï¼šå¼‚å¸¸å¤„ç† ==========
    logger.error("All final answer attempts failed")
    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè®°å½•é”™è¯¯æ—¥å¿—
    return "A"
    # è¿”å›é»˜è®¤ç­”æ¡ˆ"A"ä½œä¸ºå…œåº•æ–¹æ¡ˆ

def basic_final_answer(question_text, response_of_KG_list_path, response_of_KG_neighbor):
    """åŸºç¡€ç‰ˆæœ¬çš„æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ"""
    if response_of_KG_list_path == []:
        response_of_KG_list_path = ''
    if response_of_KG_neighbor == []:
        response_of_KG_neighbor = ''
    
    messages = [
        SystemMessage(content="You are a medical AI assistant."),
        HumanMessage(content=f'Question: {question_text}'),
        AIMessage(content=f"Knowledge:\n{response_of_KG_list_path}\n{response_of_KG_neighbor}"),
        HumanMessage(content="Answer: The final answer is:")
    ]
    
    try:
        result = chat(messages)
        answer_match = re.search(r'\b([A-E])\b', result.content)
        return answer_match.group(1) if answer_match else "A"
    except:
        return "A"

def final_answer(str, response_of_KG_list_path, response_of_KG_neighbor):
    """åŸå§‹å‡½æ•°ç­¾åä¿æŒä¸å˜"""
    return enhanced_final_answer(str, response_of_KG_list_path, response_of_KG_neighbor)

def load_and_clean_triples(file_path):
    """ä»CSVæ–‡ä»¶åŠ è½½å’Œæ¸…ç†çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„"""
    logger.info("Loading knowledge graph triples...")
    
    df = pd.read_csv(file_path, sep='\t', header=None, names=['head', 'relation', 'tail'])
    df_clean = df.dropna().copy()
    
    df_clean.loc[:, 'head'] = df_clean['head'].astype(str).str.strip()
    df_clean.loc[:, 'relation'] = df_clean['relation'].astype(str).str.strip()
    df_clean.loc[:, 'tail'] = df_clean['tail'].astype(str).str.strip()
    
    df_clean = df_clean[(df_clean['head'] != '') & 
                       (df_clean['relation'] != '') & 
                       (df_clean['tail'] != '')]
    
    logger.info(f"Loaded {len(df)} total triples, {len(df_clean)} valid triples after cleaning")
    
    return df_clean

def check_database_populated(session):
    """æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰æ•°æ®"""
    try:
        result = session.run("MATCH (n) RETURN count(n) as node_count")
        node_count = result.single()["node_count"]
        return node_count > 0
    except:
        return False

import inspect
from functools import wraps

def simple_print_progress(idx, item, step_name, **kwargs):
    """ç®€å•ç›´æ¥çš„è¿›åº¦æ‰“å°ï¼Œä½¿ç”¨æ˜¾å¼å‚æ•°ä¼ é€’"""
    print(f"\n{'='*80}")
    print(f"\n{step_name}")
    print(f"\n--- Question {idx+1} Progress ---")
    
    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
    input_text = kwargs.get('input_text', [])
    question_types = kwargs.get('question_types', [])
    question_kg = kwargs.get('question_kg', [])
    match_kg = kwargs.get('match_kg', [])
    confidence_scores = kwargs.get('confidence_scores', [])
    result_path_list = kwargs.get('result_path_list', [])
    neighbor_list = kwargs.get('neighbor_list', [])
    response_of_KG_list_path = kwargs.get('response_of_KG_list_path', '')
    response_of_KG_neighbor = kwargs.get('response_of_KG_neighbor', '')
    output_all = kwargs.get('output_all', '')
    ret_parsed = kwargs.get('ret_parsed', {})
    
    print(f"Question: {input_text[0] if input_text else 'æœªå®šä¹‰'}")
    print(f"Types: {question_types}")
    print(f"Original Entities: {question_kg}")
    
    if match_kg:
        print(f"Matched Entities: {match_kg[:5]}...")
        if confidence_scores:
            print(f"Avg Confidence: {np.mean(confidence_scores):.3f}")
    else:
        print("Matched Entities: å°šæœªå¤„ç†")
    
    if result_path_list:
        print(f"Found Paths: {len(result_path_list)}")
    else:
        print("Paths: å°šæœªæŸ¥æ‰¾")
    
    if neighbor_list:
        print(f"Neighbors: {len(neighbor_list)} relations")
    else:
        print("Neighbors: å°šæœªè·å–")
    
    if response_of_KG_list_path:
        print(f"Path Response Length: {len(response_of_KG_list_path)}")
    else:
        print("Path Response: å°šæœªç”Ÿæˆ")
    
    if response_of_KG_neighbor:
        print(f"Neighbor Response Length: {len(response_of_KG_neighbor)}")
    else:
        print("Neighbor Response: å°šæœªç”Ÿæˆ")
    
    if output_all:
        print(f"Final Answer: {output_all}")
        predicted = ret_parsed.get('prediction', 'æœªçŸ¥') if ret_parsed else 'æœªçŸ¥'
        correct = item.get('answer', 'æœªçŸ¥') if item else 'æœªçŸ¥'
        print(f"Predicted: {predicted}, Correct: {correct}")
    else:
        print("Final Answer: å°šæœªç”Ÿæˆ")
    
    print("-" * 50)
    print(f"{'='*80}")

if __name__ == "__main__":
    # æ‰“å°å½“å‰é…ç½®
    # THRESHOLDS.print_config()
    
    # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # THRESHOLDS.set_threshold('entity_matching', 'basic_similarity', 0.65)
    
    # é…ç½®ç¬¬ä¸‰æ–¹API
    openai.api_key = "sk-P4hNAfoKF4JLckjCuE99XbaN4bZIORZDPllgpwh6PnYWv4cj"
    openai.api_base = "https://aiyjg.lol/v1"
    
    os.environ['OPENAI_API_KEY'] = openai.api_key

    # 1. æ„å»ºneo4jçŸ¥è¯†å›¾è°±æ•°æ®é›†
    uri = "bolt://localhost:7688"
    username = "neo4j"
    password = "Cyber@511"

    driver = GraphDatabase.driver(uri, auth=(username, password))
    session = driver.session()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¯¼å…¥æ•°æ®
    force_reload = os.getenv('FORCE_RELOAD_DB', 'false').lower() == 'true'
    
    if force_reload or not check_database_populated(session):
        logger.info("Loading knowledge graph data...")
        logger.info("Cleaning existing knowledge graph...")
        session.run("MATCH (n) DETACH DELETE n")

        df_clean = load_and_clean_triples('./Alzheimers/train_s2s.txt')

        batch_size = 1000
        valid_triples = 0
        batch_queries = []
        batch_params = []
        
        logger.info("Starting batch insertion of knowledge graph triples...")
        
        for index, row in tqdm(df_clean.iterrows(), desc="Building knowledge graph"):
            head_name = row['head']
            tail_name = row['tail']
            relation_name = row['relation']
            
            if not validate_knowledge_triple(head_name, relation_name, tail_name):
                continue

            query = (
                "MERGE (h:Entity { name: $head_name }) "
                "MERGE (t:Entity { name: $tail_name }) "
                "MERGE (h)-[r:`" + relation_name + "`]->(t)"
            )
            
            batch_queries.append(query)
            batch_params.append({
                'head_name': head_name,
                'tail_name': tail_name,
                'relation_name': relation_name
            })
            
            if len(batch_queries) >= batch_size:
                try:
                    with driver.session() as batch_session:
                        tx = batch_session.begin_transaction()
                        for q, params in zip(batch_queries, batch_params):
                            tx.run(q, **params)
                        tx.commit()
                    valid_triples += len(batch_queries)
                    logger.debug(f"Successfully inserted batch of {len(batch_queries)} triples")
                except Exception as e:
                    logger.error(f"Failed to insert batch: {e}")
                    for q, params in zip(batch_queries, batch_params):
                        try:
                            session.run(q, **params)
                            valid_triples += 1
                        except Exception as single_e:
                            logger.warning(f"Failed to insert single triple: {params['head_name']} -> {params['relation_name']} -> {params['tail_name']}, Error: {single_e}")
                
                batch_queries = []
                batch_params = []
        
        if batch_queries:
            try:
                with driver.session() as batch_session:
                    tx = batch_session.begin_transaction()
                    for q, params in zip(batch_queries, batch_params):
                        tx.run(q, **params)
                    tx.commit()
                valid_triples += len(batch_queries)
                logger.debug(f"Successfully inserted final batch of {len(batch_queries)} triples")
            except Exception as e:
                logger.error(f"Failed to insert final batch: {e}")
                for q, params in zip(batch_queries, batch_params):
                    try:
                        session.run(q, **params)
                        valid_triples += 1
                    except Exception as single_e:
                        logger.warning(f"Failed to insert single triple: {params['head_name']} -> {params['relation_name']} -> {params['tail_name']}, Error: {single_e}")

        logger.info(f"Successfully inserted {valid_triples} valid triples using batch processing")

    else:
        logger.info("âœ… Database already populated, skipping data import")
        logger.info("ğŸ’¡ To force reload, set environment variable: FORCE_RELOAD_DB=true")
        
        # ä»éœ€è¦åŠ è½½flat_kg_triplesç”¨äºå±‚æ¬¡åŒ–å›¾è°±æ„å»º
        df_clean = load_and_clean_triples('./Alzheimers/train_s2s.txt')


    OPENAI_API_KEY = openai.api_key
    chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo', temperature=0.7)

    logger.info("Loading embeddings...")
    with open('./Alzheimers/entity_embeddings.pkl','rb') as f1:
        entity_embeddings = pickle.load(f1)
        
    with open('./Alzheimers/keyword_embeddings.pkl','rb') as f2:
        keyword_embeddings = pickle.load(f2)

    # æ–°çš„ä»£ç ï¼Œæ”¹è¿›å±‚æ¬¡åŒ–å›¾è°±çš„å…³é”®è¯åŒ¹é…ä¸ºè¯­ä¹‰åŒ¹é…
    hierarchical_kg_framework = OptimizedHierarchicalKGFramework(
        entity_embeddings=entity_embeddings,
        keyword_embeddings=keyword_embeddings,
        use_semantic_matching=True    # å¯ç”¨è¯­ä¹‰åŒ¹é…
        # similarity_threshold=0.7       # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        # similarity_thresholdç°åœ¨ä»THRESHOLDSè‡ªåŠ¨è·å–
    )

    logger.info("Building hierarchical knowledge graph structure...")
    flat_kg_triples = []
    for _, row in df_clean.iterrows():
        flat_kg_triples.append([row['head'], row['relation'], row['tail']])
    
    hierarchical_kg_framework.build_hierarchical_structure(flat_kg_triples)

    # åœ¨åŠ è½½embeddingsä¹‹åæ·»åŠ 
    logger.info("Initializing semantic question type classifier...")
    semantic_question_classifier = SemanticQuestionTypeClassifier(
        model_name='sentence-transformers/all-mpnet-base-v2',
        # similarity_threshold=0.4
        # similarity_thresholdç°åœ¨ä»THRESHOLDSè‡ªåŠ¨è·å–
    )

    # ========================= åˆå§‹åŒ–å¢å¼ºæ¨¡å— =========================
    umls_api_key = "7cce913d-29bf-459f-aa9a-2ba57d6efccf"
    umls_normalizer = UMLSNormalizer(umls_api_key)
    medical_reasoning_rules = MedicalReasoningRules(umls_normalizer)
    multi_hop_reasoner = MultiHopReasoning(max_hops=3, umls_normalizer=umls_normalizer)


    medical_reasoning_rules.initialize_kg_guided_reasoning(flat_kg_triples, chat)

    # åªå¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®é›†
    # datasets = ['medqa', 'medmcqa', 'mmlu', 'qa4mre']
    datasets = ['medqa']  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®é›†

    for dataset in datasets:
        logger.info(f"Processing dataset: {dataset}")
        processor = dataset2processor[dataset]()

        data = processor.load_dataset()

        # âœ… åªå–å‰Nä¸ªé—®é¢˜
        data = data[:1]

        # âœ… æ–°å¢ï¼šæå–æ‰€æœ‰é—®é¢˜è¿›è¡Œæ‰¹é‡å¤„ç†
        all_questions = []
        for item in data:
            input_text = processor.generate_prompt(item)
            all_questions.append(input_text)
        
        # âœ… æ‰¹é‡å¤„ç†æ‰€æœ‰é—®é¢˜ç±»å‹è¯†åˆ« â†’ åªæœ‰1ä¸ªè¿›åº¦æ¡
        logger.info(f"Batch processing {len(all_questions)} questions for semantic classification...")
        all_question_types = semantic_question_classifier.batch_identify_question_types(all_questions)

        acc, total_num = 0, 0
        generated_data = []

        # âœ… ä¿®æ”¹ï¼šä½¿ç”¨é¢„è®¡ç®—çš„é—®é¢˜ç±»å‹
        for idx, item in enumerate(tqdm(data, desc=f"Processing {dataset}")):
            
            if total_num > 0 and total_num % CLEANUP_FREQUENCY == 0:
                cleanup_resources(total_num)

            # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šç«‹å³åˆå§‹åŒ–æ‰€æœ‰å˜é‡ï¼ˆåœ¨ä»»ä½•ä½¿ç”¨ä¹‹å‰ï¼‰
            match_kg = []
            confidence_scores = []
            result_path_list = []
            neighbor_list = []
            response_of_KG_list_path = ""
            response_of_KG_neighbor = ""
            output_all = ""
            ret_parsed = {}
            path_sampled = ""
            neighbor_input_sampled = ""

            input_text = [all_questions[idx]]  # ä½¿ç”¨é¢„è®¡ç®—çš„é—®é¢˜æ–‡æœ¬
            entity_list = item['entity'].split('\n')
            question_kg = []
            
            for entity in entity_list:
                try:
                    entity = entity.split('.')[1].strip()
                    question_kg.append(entity)
                except:
                    continue


            # âœ… ä½¿ç”¨é¢„è®¡ç®—çš„é—®é¢˜ç±»å‹ï¼Œä¸å†é‡æ–°è®¡ç®—
            question_types = all_question_types[idx]
            logger.info(f"Question types identified: {question_types}")

            # å†…å®¹è¾“å‡º
            simple_print_progress(idx, item, "ç¬¬1æ­¥å†…å®¹æ‰“å°ï¼Œä½¿ç”¨ä¸è®¡ç®—çš„é—®é¢˜ç±»å‹", 
                         input_text=input_text,
                         question_types=question_types,
                         question_kg=question_kg,
                         match_kg=match_kg,  # ç©ºåˆ—è¡¨
                         confidence_scores=confidence_scores,  # ç©ºåˆ—è¡¨
                         result_path_list=result_path_list,  # ç©ºåˆ—è¡¨
                         neighbor_list=neighbor_list,  # ç©ºåˆ—è¡¨
                         response_of_KG_list_path=response_of_KG_list_path,  # ç©ºå­—ç¬¦ä¸²
                         response_of_KG_neighbor=response_of_KG_neighbor,  # ç©ºå­—ç¬¦ä¸²
                         output_all=output_all,  # ç©ºå­—ç¬¦ä¸²
                         ret_parsed=ret_parsed)  # ç©ºå­—å…¸

            match_kg, confidence_scores = enhanced_entity_matching(
                question_kg, entity_embeddings, keyword_embeddings, input_text[0])

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬2æ­¥å†…å®¹æ‰“å°ï¼Œå®ä½“åŒ¹é…",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,  # ç°åœ¨æœ‰å†…å®¹
                             confidence_scores=confidence_scores,  # ç°åœ¨æœ‰å†…å®¹
                             result_path_list=result_path_list,  # ä»ç„¶ç©º
                             neighbor_list=neighbor_list,  # ä»ç„¶ç©º
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)
            

            if len(match_kg) < 2:
                logger.warning(f"Insufficient entities matched for question: {input_text[0][:100]}...")
                match_kg.extend(question_kg[:2])

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬3æ­¥å†…å®¹æ‰“å°ï¼ŒçŸ¥è¯†å›¾è°±è·¯å¾„æŸ¥æ‰¾ä¹‹å‰",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,  # ç°åœ¨æœ‰å†…å®¹
                             confidence_scores=confidence_scores,  # ç°åœ¨æœ‰å†…å®¹
                             result_path_list=result_path_list,  # ä»ç„¶ç©º
                             neighbor_list=neighbor_list,  # ä»ç„¶ç©º
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)

            # 4. å¢å¼ºçš„neo4jçŸ¥è¯†å›¾è°±è·¯å¾„æŸ¥æ‰¾ï¼ŒæŸ¥æ‰¾åˆå§‹å®ä½“å’Œå€™é€‰å®ä½“ä¹‹é—´çš„æœ€ä¼˜è·¯å¾„
            if len(match_kg) > 1:
                start_entity = match_kg[0]
                candidate_entity = match_kg[1:]
                
                result_path_list = []
                while True:
                    flag = 0
                    paths_list = []
                    
                    while candidate_entity:
                        end_entity = candidate_entity[0]
                        candidate_entity.remove(end_entity)
                        
                        paths, exist_entity = find_shortest_path(start_entity, end_entity, candidate_entity, question_types)
                        path_list = []
                        
                        if paths == [''] or paths == []:
                            flag = 1
                            if not candidate_entity:
                                flag = 0
                                break
                            start_entity = candidate_entity[0]
                            candidate_entity.remove(start_entity)
                            break
                        else:
                            for p in paths:
                                path_list.append(p.split('->'))
                            if path_list:
                                paths_list.append(path_list)
                        
                        if exist_entity != {}:
                            try:
                                candidate_entity.remove(exist_entity)
                            except:
                                continue
                        start_entity = end_entity
                    
                    result_path = combine_lists(*paths_list)
                    
                    if result_path:
                        result_path_list.extend(result_path)
                    if flag == 1:
                        continue
                    else:
                        break
                        
                start_tmp = []
                for path_new in result_path_list:
                    if path_new == []:
                        continue
                    if path_new[0] not in start_tmp:
                        start_tmp.append(path_new[0])
                
                if len(start_tmp) == 0:
                    result_path = {}
                    single_path = {}
                else:
                    if len(start_tmp) == 1:
                        result_path = result_path_list[:5]
                    else:
                        result_path = []
                                                                
                        if len(start_tmp) >= 5:
                            for path_new in result_path_list:
                                if path_new == []:
                                    continue
                                if path_new[0] in start_tmp:
                                    result_path.append(path_new)
                                    start_tmp.remove(path_new[0])
                                if len(result_path) == 5:
                                    break
                        else:
                            count = 5 // len(start_tmp)
                            remind = 5 % len(start_tmp)
                            count_tmp = 0
                            for path_new in result_path_list:
                                if len(result_path) < 5:
                                    if path_new == []:
                                        continue
                                    if path_new[0] in start_tmp:
                                        if count_tmp < count:
                                            result_path.append(path_new)
                                            count_tmp += 1
                                        else:
                                            start_tmp.remove(path_new[0])
                                            count_tmp = 0
                                            if path_new[0] in start_tmp:
                                                result_path.append(path_new)
                                                count_tmp += 1

                                        if len(start_tmp) == 1:
                                            count = count + remind
                                else:
                                    break

                    try:
                        single_path = result_path_list[0]
                    except:
                        single_path = result_path_list
                    
            else:
                result_path = {}
                single_path = {}            

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬4æ­¥å†…å®¹æ‰“å°ï¼Œå¢å¼ºçš„neo4jçŸ¥è¯†å›¾è°±è·¯å¾„æŸ¥æ‰¾",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,
                             confidence_scores=confidence_scores,
                             result_path_list=result_path_list,
                             neighbor_list=neighbor_list,
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)
            
            # 5. å¢å¼ºçš„neo4jçŸ¥è¯†å›¾è°±é‚»å±…å®ä½“,è·å–é‚»å±…ä¿¡æ¯
            neighbor_list = []
            neighbor_list_disease = []
            
            for match_entity in match_kg:
                disease_flag = 0
                neighbors, disease = get_entity_neighbors(match_entity, disease_flag, question_types)
                neighbor_list.extend(neighbors)

                try:
                    hierarchical_context = hierarchical_kg_framework.get_hierarchical_context(
                        match_entity, context_type='all'
                    )
                    
                    for context_type, context_items in hierarchical_context.items():
                        for context_item in context_items[:3]:
                            if isinstance(context_item, dict):
                                entity_name = context_item.get('entity', '')
                                relation_name = context_item.get('relation', '')
                                if entity_name and relation_name:
                                    neighbor_entry = [
                                        match_entity.replace("_", " "),
                                        f"hierarchical_{relation_name}".replace("_", " "),
                                        entity_name.replace("_", " ")
                                    ]
                                    neighbor_list.append(neighbor_entry)
                                    
                except Exception as e:
                    logger.error(f"Error getting hierarchical context for {match_entity}: {e}")

                while disease:
                    new_disease = []
                    for disease_tmp in disease:
                        if disease_tmp in match_kg:
                            new_disease.append(disease_tmp)

                    if new_disease:
                        for disease_entity in new_disease:
                            disease_flag = 1
                            neighbors, disease = get_entity_neighbors(disease_entity, disease_flag, question_types)
                            neighbor_list_disease.extend(neighbors)
                    else:
                        for disease_entity in disease:
                            disease_flag = 1
                            neighbors, disease = get_entity_neighbors(disease_entity, disease_flag, question_types)
                            neighbor_list_disease.extend(neighbors)
                    
                    if len(neighbor_list_disease) > 10:
                        break
            
            if len(neighbor_list) <= 5:
                neighbor_list.extend(neighbor_list_disease)

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬5æ­¥å†…å®¹æ‰“å°ï¼Œå¢å¼ºçš„neo4jçŸ¥è¯†å›¾è°±é‚»å±…å®ä½“",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,
                             confidence_scores=confidence_scores,
                             result_path_list=result_path_list,
                             neighbor_list=neighbor_list,
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)

            # 6. å¢å¼ºçš„çŸ¥è¯†å›¾è°±è·¯å¾„åŸºç¡€æç¤ºç”Ÿæˆï¼Œå°†è·¯å¾„è½¬æ¢æˆè‡ªç„¶è¯­è¨€æç¤º
            if len(match_kg) > 1:
                response_of_KG_list_path = []
                if result_path == {}:
                    response_of_KG_list_path = []
                    path_sampled = []
                else:
                    result_new_path = []
                    for total_path_i in result_path:
                        path_input = "->".join(total_path_i)
                        result_new_path.append(path_input)
                    
                    path = "\n".join(result_new_path)
                    path_sampled = self_knowledge_retrieval_reranking(path, input_text[0])
                    response_of_KG_list_path = prompt_path_finding(path_sampled)
                    
                    # é‡è¯•å±è”½ä½ç½®
                    if is_unable_to_answer(response_of_KG_list_path):
                        logger.warning("Path finding response validation failed, retrying...")
                        response_of_KG_list_path = prompt_path_finding(path_sampled)
                    # ç»“æŸä½ç½®
            else:
                response_of_KG_list_path = '{}'

            try:
                response_single_path = prompt_path_finding(single_path)
                if is_unable_to_answer(response_single_path):
                    response_single_path = prompt_path_finding(single_path)
            except:
                response_single_path = ""

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬6æ­¥å†…å®¹æ‰“å°ï¼Œå¢å¼ºçš„çŸ¥è¯†å›¾è°±è·¯å¾„åŸºç¡€æç¤ºç”Ÿæˆ",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,
                             confidence_scores=confidence_scores,
                             result_path_list=result_path_list,
                             neighbor_list=neighbor_list,
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)

            # 7. å¢å¼ºçš„çŸ¥è¯†å›¾è°±é‚»å±…å®ä½“åŸºç¡€æç¤ºç”Ÿæˆ
            response_of_KG_list_neighbor = []
            neighbor_new_list = []
            
            for neighbor_i in neighbor_list:
                neighbor = "->".join(neighbor_i)
                neighbor_new_list.append(neighbor)

            if len(neighbor_new_list) > 5:
                neighbor_input = "\n".join(neighbor_new_list[:5])
            else:
                neighbor_input = "\n".join(neighbor_new_list)
            
            neighbor_input_sampled = self_knowledge_retrieval_reranking(neighbor_input, input_text[0])
            response_of_KG_neighbor = prompt_neighbor(neighbor_input_sampled)
            
            # é‡è¯•å±è”½ä½ç½®
            if is_unable_to_answer(response_of_KG_neighbor):
                logger.warning("Neighbor processing response validation failed, retrying...")
                response_of_KG_neighbor = prompt_neighbor(neighbor_input_sampled)
            # ç»“æŸä½ç½®

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬7æ­¥å†…å®¹æ‰“å°ï¼Œå¢å¼ºçš„çŸ¥è¯†å›¾è°±é‚»å±…å®ä½“åŸºç¡€æç¤ºç”Ÿæˆ",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,
                             confidence_scores=confidence_scores,
                             result_path_list=result_path_list,
                             neighbor_list=neighbor_list,
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)

            # 8. å¢å¼ºçš„åŸºäºæç¤ºçš„åŒ»å­¦å¯¹è¯ç­”æ¡ˆç”Ÿæˆï¼ˆç§»é™¤äº†ç½®ä¿¡åº¦è®¡ç®—ï¼‰
            output_all = enhanced_final_answer(input_text[0], response_of_KG_list_path, response_of_KG_neighbor)
            
            # é‡è¯•å±è”½ä½ç½®
            if is_unable_to_answer(output_all):
                logger.warning("Final answer validation failed, retrying...")
                output_all = enhanced_final_answer(input_text[0], response_of_KG_list_path, response_of_KG_neighbor)
            # ç»“æŸä½ç½®

            # å†…å®¹è¾“å‡º
            if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                simple_print_progress(idx, item, "ç¬¬8æ­¥å†…å®¹æ‰“å°ï¼Œå¢å¼ºçš„åŸºäºæç¤ºçš„åŒ»å­¦å¯¹è¯ç­”æ¡ˆç”Ÿæˆï¼ˆç§»é™¤äº†ç½®ä¿¡åº¦è®¡ç®—ï¼‰",
                             input_text=input_text,
                             question_types=question_types,
                             question_kg=question_kg,
                             match_kg=match_kg,
                             confidence_scores=confidence_scores,
                             result_path_list=result_path_list,
                             neighbor_list=neighbor_list,
                             response_of_KG_list_path=response_of_KG_list_path,
                             response_of_KG_neighbor=response_of_KG_neighbor,
                             output_all=output_all,
                             ret_parsed=ret_parsed)

            ret_parsed, acc_item = processor.parse(output_all, item)
            ret_parsed['path'] = path_sampled if 'path_sampled' in locals() else ""
            ret_parsed['neighbor_input'] = neighbor_input_sampled if 'neighbor_input_sampled' in locals() else ""
            ret_parsed['response_of_KG_list_path'] = response_of_KG_list_path
            ret_parsed['response_of_KG_neighbor'] = response_of_KG_neighbor
            ret_parsed['entity_confidence_scores'] = confidence_scores if 'confidence_scores' in locals() else []
            ret_parsed['question_types'] = question_types
            
            try:
                ret_parsed['umls_normalized_entities'] = umls_normalizer.normalize_medical_terms(question_kg)
                ret_parsed['umls_semantic_variants'] = [umls_normalizer.get_semantic_variants(entity)[:3] for entity in question_kg[:3]]
                
                enhanced_links = umls_normalizer.enhanced_entity_linking_method(
                    question_kg, input_text[0], question_types
                )
                ret_parsed['enhanced_entity_links'] = enhanced_links
                
                adaptive_knowledge = umls_normalizer.adaptive_knowledge_selection(
                    question_types, question_kg
                )
                ret_parsed['adaptive_knowledge_count'] = len(adaptive_knowledge)
                
                hierarchical_contexts = {}
                for entity in question_kg[:3]:
                    hierarchical_contexts[entity] = hierarchical_kg_framework.get_hierarchical_context(
                        entity, context_type='all'
                    )
                ret_parsed['hierarchical_contexts'] = hierarchical_contexts
                
                if len(question_kg) >= 2:
                    multi_hop_paths = multi_hop_reasoner.optimized_multi_hop.intelligent_path_selection(
                        question_kg[:1], question_kg[1:2], max_hops=2
                    )
                    ret_parsed['multi_hop_paths_count'] = len(multi_hop_paths)
                else:
                    ret_parsed['multi_hop_paths_count'] = 0
                
            except Exception as e:
                logger.error(f"Error in enhanced processing: {e}")
                ret_parsed['umls_normalized_entities'] = question_kg
                ret_parsed['umls_semantic_variants'] = []
                ret_parsed['enhanced_entity_links'] = {}
                ret_parsed['adaptive_knowledge_count'] = 0
                ret_parsed['hierarchical_contexts'] = {}
                ret_parsed['multi_hop_paths_count'] = 0
            
            ret_parsed = convert_numpy_types(ret_parsed)
            
            if ret_parsed['prediction'] in processor.num2answer.values():
                acc += acc_item
                total_num += 1
            generated_data.append(ret_parsed)

        logger.info(f"Dataset: {dataset}")
        logger.info(f"Accuracy: {acc/total_num:.4f} ({acc}/{total_num})")

        # å†…å®¹è¾“å‡º
        if idx < 5:  # åªæ‰“å°å‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
            simple_print_progress(idx, item, "ç¬¬9æ­¥å†…å®¹æ‰“å°ï¼Œåç»­å¤„ç†å·¥ä½œå®Œæˆ",
                            input_text=input_text,
                            question_types=question_types,
                            question_kg=question_kg,
                            match_kg=match_kg,
                            confidence_scores=confidence_scores,
                            result_path_list=result_path_list,
                            neighbor_list=neighbor_list,
                            response_of_KG_list_path=response_of_KG_list_path,
                            response_of_KG_neighbor=response_of_KG_neighbor,
                            output_all=output_all,
                            ret_parsed=ret_parsed)

        os.makedirs('./Alzheimers/result_chatgpt_mindmap', exist_ok=True)
        
        output_filename = f"{dataset}_{CURRENT_ABLATION_CONFIG}_ablation_results.json"
        with open(os.path.join('./Alzheimers/result_chatgpt_mindmap', output_filename), 'w') as f:
            json.dump(generated_data, fp=f, indent=2)
            
        logger.info(f"Ablation results saved for dataset: {dataset}")
        
        performance_stats = {
            'ablation_config': CURRENT_ABLATION_CONFIG,
            'config_details': ABLATION_CONFIG,
            'dataset': dataset,
            'total_questions': total_num,
            'correct_answers': acc,
            'accuracy': acc/total_num if total_num > 0 else 0,
            'avg_entity_confidence': np.mean([
                np.mean(item.get('entity_confidence_scores', [0])) 
                for item in generated_data if item.get('entity_confidence_scores')
            ]) if generated_data else 0,
            'question_type_distribution': {},
            'hierarchical_context_coverage': 0,
            'multi_strategy_usage': 0
        }
        
        question_type_counts = {}
        hierarchical_coverage_count = 0
        multi_strategy_count = 0
        
        for item in generated_data:
            q_types = item.get('question_types', [])
            for q_type in q_types:
                question_type_counts[q_type] = question_type_counts.get(q_type, 0) + 1
            
            if item.get('hierarchical_contexts'):
                hierarchical_coverage_count += 1
            
            if item.get('enhanced_entity_links'):
                multi_strategy_count += 1
        
        performance_stats['question_type_distribution'] = question_type_counts
        performance_stats['hierarchical_context_coverage'] = hierarchical_coverage_count / len(generated_data) if generated_data else 0
        performance_stats['multi_strategy_usage'] = multi_strategy_count / len(generated_data) if generated_data else 0
        
        stats_filename = f"{dataset}_{CURRENT_ABLATION_CONFIG}_performance_stats.json"
        with open(os.path.join('./Alzheimers/result_chatgpt_mindmap', stats_filename), 'w') as f:
            json.dump(performance_stats, fp=f, indent=2)
            
        logger.info(f"Performance statistics saved for dataset: {dataset}")
        logger.info(f"Hierarchical context coverage: {performance_stats['hierarchical_context_coverage']:.3f}")
        logger.info(f"Multi-strategy usage: {performance_stats['multi_strategy_usage']:.3f}")

    logger.info("="*50)
    logger.info(f"ğŸ‰ Ablation study completed for configuration: {CURRENT_ABLATION_CONFIG}")
    logger.info("ğŸ“Š Ablation configuration applied:")
    for module, enabled in ABLATION_CONFIG.items():
        status = "âœ… ENABLED" if enabled else "âŒ DISABLED"
        logger.info(f"   {module}: {status}")
    logger.info("="*50)
    
    overall_stats = {
        'current_ablation_config': CURRENT_ABLATION_CONFIG,
        'ablation_configuration': ABLATION_CONFIG,
        'available_configs': list(ABLATION_CONFIGS.keys()),
        'performance_config': {
            'cleanup_frequency': CLEANUP_FREQUENCY,
            'max_cache_size': MAX_CACHE_SIZE,
            'keep_cache_size': KEEP_CACHE_SIZE,
            'max_failed_cuis': MAX_FAILED_CUIS
        },
        'datasets_processed': datasets,
        'total_datasets': len(datasets),
        'processing_timestamp': datetime.now().isoformat()
    }
    
    with open('./Alzheimers/result_chatgpt_mindmap/ablation_experiment_report.json', 'w') as f:
        json.dump(overall_stats, fp=f, indent=2)
    
    logger.info("ğŸ“ˆ Ablation experiment report saved!")
    
    driver.close()
    
    logger.info("ğŸ”Œ Database connection closed. Ablation study complete!")
    logger.info(f"ğŸ”¬ To run different ablation configurations, set ABLATION_CONFIG environment variable to one of: {list(ABLATION_CONFIGS.keys())}")

